{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ **The Complete Linear Algebra Guide for AI Mastery**\n",
    "## *From Foundations to Deep Learning Applications*\n",
    "\n",
    "---\n",
    "\n",
    "**Welcome!** Linear Algebra is the **mathematical language of AI**. This notebook is your comprehensive guide to mastering the linear algebra concepts that power modern machine learning and deep learning.\n",
    "\n",
    "### üéØ **Why Linear Algebra for AI?**\n",
    "\n",
    "| AI Concept | Linear Algebra Tool | Application |\n",
    "|------------|---------------------|-------------|\n",
    "| **Neural Networks** | Matrix multiplication | Layer transformations |\n",
    "| **Computer Vision** | Tensor operations | Image processing |\n",
    "| **NLP** | Vector embeddings | Word representations |\n",
    "| **Optimization** | Gradients, eigenvalues | Training algorithms |\n",
    "| **Dimensionality Reduction** | SVD, eigendecomposition | PCA, t-SNE |\n",
    "| **Attention Mechanisms** | Matrix operations | Transformers |\n",
    "\n",
    "### üìö **Learning Path**\n",
    "\n",
    "```\n",
    "Phase 1: Foundations (Scalars ‚Üí Vectors ‚Üí Matrices)\n",
    "    ‚Üì\n",
    "Phase 2: Matrix Operations (Multiplication ‚Üí Transpose ‚Üí Inverse)\n",
    "    ‚Üì\n",
    "Phase 3: Vector Spaces (Linear independence ‚Üí Basis ‚Üí Rank)\n",
    "    ‚Üì\n",
    "Phase 4: Advanced Topics (Eigenvalues ‚Üí SVD ‚Üí Tensors)\n",
    "    ‚Üì\n",
    "Phase 5: AI Applications (Neural nets ‚Üí Attention ‚Üí Optimization)\n",
    "```\n",
    "\n",
    "**Prerequisites:** Basic Python, High School Math  \n",
    "**Difficulty:** ‚≠ê‚≠ê Beginner to Advanced  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Linear Algebra in AI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# For animations and advanced visualizations\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üêç NumPy version:\", np.__version__)\n",
    "print(\"üìä Ready to explore Linear Algebra for AI!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 1: The Building Blocks - Scalars, Vectors, and Matrices**\n",
    "\n",
    "### üé≤ **1.1 Scalars: The Simplest Entity**\n",
    "\n",
    "A **scalar** is just a single number - no direction, no dimension, just magnitude.\n",
    "\n",
    "```\n",
    "Scalar (0-dimensional tensor)\n",
    "    ‚Üì\n",
    "    5\n",
    "    \n",
    "In AI: Learning rate, temperature parameter, loss value\n",
    "```\n",
    "\n",
    "### üéØ **1.2 Vectors: Numbers with Direction**\n",
    "\n",
    "A **vector** is an ordered list of numbers. Think of it as a point in space or an arrow from origin.\n",
    "\n",
    "```\n",
    "Vector (1-dimensional tensor)\n",
    "    ‚Üì\n",
    "    [3]\n",
    "    [1]   =   3 units right, 1 unit up\n",
    "    \n",
    "In AI: Feature vectors, word embeddings, neural network layers\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Scalars and Vectors\n",
    "print(\"=\" * 60)\n",
    "print(\"üé≤ SCALARS AND VECTORS IN NUMPY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scalar\n",
    "scalar = 5\n",
    "print(f\"\\nüî¢ Scalar: {scalar}\")\n",
    "print(f\"   Type: {type(scalar)}\")\n",
    "print(f\"   Dimensions: 0 (just a number)\")\n",
    "\n",
    "# Vector creation methods\n",
    "print(\"\\nüéØ Vector Creation:\")\n",
    "\n",
    "# Method 1: From list\n",
    "v1 = np.array([1, 2, 3])\n",
    "print(f\"\\n1. From Python list: {v1}\")\n",
    "print(f\"   Shape: {v1.shape} | Dimensions: {v1.ndim}\")\n",
    "\n",
    "# Method 2: Using arange\n",
    "v2 = np.arange(0, 10, 2)\n",
    "print(f\"\\n2. Using arange: {v2}\")\n",
    "print(f\"   Shape: {v2.shape}\")\n",
    "\n",
    "# Method 3: Using linspace\n",
    "v3 = np.linspace(0, 1, 5)\n",
    "print(f\"\\n3. Using linspace: {v3}\")\n",
    "print(f\"   Shape: {v3.shape}\")\n",
    "\n",
    "# Method 4: Random vector (common in AI for initialization)\n",
    "v4 = np.random.randn(5)  # Standard normal distribution\n",
    "print(f\"\\n4. Random (normal): {v4.round(3)}\")\n",
    "print(f\"   Mean: {v4.mean():.3f}, Std: {v4.std():.3f}\")\n",
    "\n",
    "# Visualize 2D vectors\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Create multiple vectors\n",
    "vectors = np.array([\n",
    "    [2, 1],   # v1\n",
    "    [-1, 2],  # v2\n",
    "    [1, -2],  # v3\n",
    "    [-2, -1]  # v4\n",
    "])\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "labels = ['v1 (2,1)', 'v2 (-1,2)', 'v3 (1,-2)', 'v4 (-2,-1)']\n",
    "\n",
    "# Plot each vector\n",
    "for i, (vec, color, label) in enumerate(zip(vectors, colors, labels)):\n",
    "    ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', \n",
    "              scale=1, color=color, width=0.005, label=label)\n",
    "    \n",
    "    # Add point at tip\n",
    "    ax.plot(vec[0], vec[1], 'o', color=color, markersize=8)\n",
    "    \n",
    "    # Add coordinates\n",
    "    ax.text(vec[0]+0.1, vec[1]+0.1, f'({vec[0]},{vec[1]})', fontsize=10)\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('üéØ 2D Vectors as Arrows from Origin', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Application: Word embeddings are high-dimensional vectors (e.g., 300 dimensions)\")\n",
    "print(\"   Each word = point in 300D space, similar words = close points\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß± **1.3 Matrices: 2D Arrays of Numbers**\n",
    "\n",
    "A **matrix** is a rectangular array of numbers arranged in rows and columns.\n",
    "\n",
    "```\n",
    "Matrix A (2√ó3)          Matrix B (3√ó2)\n",
    "‚îå         ‚îê            ‚îå         ‚îê\n",
    "‚îÇ 1  2  3 ‚îÇ            ‚îÇ 7  8    ‚îÇ\n",
    "‚îÇ 4  5  6 ‚îÇ            ‚îÇ 9  10   ‚îÇ\n",
    "‚îî         ‚îò            ‚îÇ 11 12   ‚îÇ\n",
    "                        ‚îî         ‚îò\n",
    "                        \n",
    "In AI: Weight matrices, image pixels, adjacency matrices, attention maps\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß± MATRICES IN NUMPY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Matrix creation methods\n",
    "print(\"\\nüõ†Ô∏è Matrix Creation Methods:\")\n",
    "\n",
    "# Method 1: From nested lists\n",
    "A = np.array([[1, 2, 3], \n",
    "              [4, 5, 6]])\n",
    "print(f\"\\n1. From nested lists (2√ó3):\\n{A}\")\n",
    "print(f\"   Shape: {A.shape}, dtype: {A.dtype}\")\n",
    "\n",
    "# Method 2: Zeros matrix (common for initialization)\n",
    "B = np.zeros((3, 4))\n",
    "print(f\"\\n2. Zeros matrix (3√ó4):\\n{B}\")\n",
    "\n",
    "# Method 3: Ones matrix\n",
    "C = np.ones((2, 2))\n",
    "print(f\"\\n3. Ones matrix (2√ó2):\\n{C}\")\n",
    "\n",
    "# Method 4: Identity matrix (crucial for linear algebra)\n",
    "I = np.eye(4)\n",
    "print(f\"\\n4. Identity matrix I‚ÇÑ (4√ó4):\\n{I}\")\n",
    "print(\"   Diagonal = 1, Off-diagonal = 0\")\n",
    "\n",
    "# Method 5: Diagonal matrix\n",
    "D = np.diag([1, 2, 3, 4])\n",
    "print(f\"\\n5. Diagonal matrix:\\n{D}\")\n",
    "\n",
    "# Method 6: Random matrix (weight initialization in neural networks)\n",
    "W = np.random.randn(3, 5) * 0.01  # Small random values (Xavier initialization style)\n",
    "print(f\"\\n6. Random matrix (3√ó5) - like neural network weights:\\n{W.round(4)}\")\n",
    "\n",
    "# Visualize matrix as heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Matrix 1: Random data\n",
    "data1 = np.random.rand(10, 10)\n",
    "im1 = axes[0].imshow(data1, cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Random Matrix (Data)', fontweight='bold')\n",
    "axes[0].set_xlabel('Column')\n",
    "axes[0].set_ylabel('Row')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Matrix 2: Identity\n",
    "im2 = axes[1].imshow(np.eye(10), cmap='Blues', aspect='auto')\n",
    "axes[1].set_title('Identity Matrix I‚ÇÅ‚ÇÄ', fontweight='bold')\n",
    "axes[1].set_xlabel('Column')\n",
    "axes[1].set_ylabel('Row')\n",
    "\n",
    "# Matrix 3: Image-like (gradient)\n",
    "data3 = np.outer(np.linspace(0, 1, 20), np.linspace(0, 1, 20))\n",
    "im3 = axes[2].imshow(data3, cmap='gray', aspect='auto')\n",
    "axes[2].set_title('Structured Matrix (Gradient)', fontweight='bold')\n",
    "axes[2].set_xlabel('Column')\n",
    "axes[2].set_ylabel('Row')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Application: A 28√ó28 MNIST image is a matrix where each entry = pixel intensity\")\n",
    "print(\"   A batch of 100 images = 3D tensor (100√ó28√ó28)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 2: Vector Operations - The Heart of Computations**\n",
    "\n",
    "### ‚ûï **2.1 Vector Addition and Scalar Multiplication**\n",
    "\n",
    "**Vector Addition:** Component-wise addition\n",
    "```\n",
    "[1]   [4]   [5]\n",
    "[2] + [5] = [7]\n",
    "[3]   [6]   [9]\n",
    "```\n",
    "\n",
    "**Scalar Multiplication:** Scaling the magnitude\n",
    "```\n",
    "    [2]   [6]\n",
    "3 √ó [1] = [3]\n",
    "    [4]   [12]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ûï VECTOR OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define vectors\n",
    "a = np.array([2, 1])\n",
    "b = np.array([1, 3])\n",
    "c = np.array([4, 2])\n",
    "\n",
    "print(f\"\\nVectors:\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = {c}\")\n",
    "\n",
    "# Addition\n",
    "print(f\"\\nüßÆ Addition: a + b = {a + b}\")\n",
    "print(f\"   NumPy: np.add(a, b) = {np.add(a, b)}\")\n",
    "\n",
    "# Subtraction\n",
    "print(f\"\\nüßÆ Subtraction: a - b = {a - b}\")\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar = 2\n",
    "print(f\"\\nüßÆ Scalar Multiplication: {scalar} √ó a = {scalar * a}\")\n",
    "print(f\"   NumPy: np.multiply(scalar, a) = {np.multiply(scalar, a)}\")\n",
    "\n",
    "# Linear combination\n",
    "print(f\"\\nüßÆ Linear Combination: 2a + 3b = {2*a + 3*b}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Original vectors\n",
    "ax.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='red', width=0.005, label='a = (2,1)')\n",
    "ax.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='blue', width=0.005, label='b = (1,3)')\n",
    "\n",
    "# Addition: a + b (parallelogram law)\n",
    "sum_vec = a + b\n",
    "ax.quiver(0, 0, sum_vec[0], sum_vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='green', width=0.008, label='a + b = (3,4)')\n",
    "\n",
    "# Show parallelogram\n",
    "ax.plot([a[0], sum_vec[0]], [a[1], sum_vec[1]], 'k--', alpha=0.3)\n",
    "ax.plot([b[0], sum_vec[0]], [b[1], sum_vec[1]], 'k--', alpha=0.3)\n",
    "\n",
    "# Scalar multiplication\n",
    "scaled = 1.5 * a\n",
    "ax.quiver(0, 0, scaled[0], scaled[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='orange', width=0.005, label='1.5 √ó a')\n",
    "\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-1, 6)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Vector Addition & Scalar Multiplication\\n(Parallelogram Law)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Geometric Interpretation:\")\n",
    "print(\"   ‚Ä¢ Addition: Place tail of b at head of a, result is from origin to end\")\n",
    "print(\"   ‚Ä¢ Scalar multiplication: Stretches (|scalar|>1) or shrinks (|scalar|<1) the vector\")\n",
    "print(\"   ‚Ä¢ Negative scalar: Reverses direction\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî≤ **2.2 Dot Product (Inner Product)**\n",
    "\n",
    "The **dot product** is the most important vector operation in AI. It measures similarity between vectors.\n",
    "\n",
    "**Formula:** $a \\cdot b = \\sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + ... + a_nb_n$\n",
    "\n",
    "**Geometric Interpretation:** $a \\cdot b = \\|a\\| \\|b\\| \\cos(\\theta)$\n",
    "\n",
    "```\n",
    "a ¬∑ b > 0: Vectors point in similar directions (acute angle)\n",
    "a ¬∑ b = 0: Vectors are perpendicular (orthogonal)\n",
    "a ¬∑ b < 0: Vectors point in opposite directions (obtuse angle)\n",
    "```\n",
    "\n",
    "**In AI:** Neural network layers compute $W \\cdot x + b$ (weights dot input + bias)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî≤ DOT PRODUCT - THE CORE OF NEURAL NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example vectors\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"\\nv1 = {v1}\")\n",
    "print(f\"v2 = {v2}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_dot = sum(x*y for x, y in zip(v1, v2))\n",
    "print(f\"\\nüßÆ Manual dot product: Œ£(v1_i √ó v2_i)\")\n",
    "print(f\"   = {v1[0]}√ó{v2[0]} + {v1[1]}√ó{v2[1]} + {v1[2]}√ó{v2[2]}\")\n",
    "print(f\"   = {v1[0]*v2[0]} + {v1[1]*v2[1]} + {v1[2]*v2[2]}\")\n",
    "print(f\"   = {manual_dot}\")\n",
    "\n",
    "# NumPy methods\n",
    "print(f\"\\nüíª NumPy methods:\")\n",
    "print(f\"   np.dot(v1, v2) = {np.dot(v1, v2)}\")\n",
    "print(f\"   v1 @ v2 (Python 3.5+) = {v1 @ v2}\")\n",
    "print(f\"   v1.dot(v2) = {v1.dot(v2)}\")\n",
    "\n",
    "# Geometric interpretation\n",
    "print(f\"\\nüìê Geometric Interpretation:\")\n",
    "norm_v1 = np.linalg.norm(v1)\n",
    "norm_v2 = np.linalg.norm(v2)\n",
    "cos_theta = np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
    "theta_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "theta_deg = np.degrees(theta_rad)\n",
    "\n",
    "print(f\"   ||v1|| = {norm_v1:.3f}\")\n",
    "print(f\"   ||v2|| = {norm_v2:.3f}\")\n",
    "print(f\"   cos(Œ∏) = {cos_theta:.3f}\")\n",
    "print(f\"   Œ∏ = {theta_deg:.2f}¬∞\")\n",
    "\n",
    "# Demonstrate different angles\n",
    "angles_demo = [\n",
    "    (np.array([1, 0]), np.array([1, 0]), \"Same direction\"),\n",
    "    (np.array([1, 0]), np.array([0, 1]), \"Perpendicular (90¬∞)\"),\n",
    "    (np.array([1, 0]), np.array([-1, 0]), \"Opposite (180¬∞)\"),\n",
    "    (np.array([1, 0]), np.array([1, 1]), \"45¬∞ angle\")\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Dot Product Examples:\")\n",
    "for a, b, desc in angles_demo:\n",
    "    dot = np.dot(a, b)\n",
    "    angle = np.degrees(np.arccos(np.clip(dot/(np.linalg.norm(a)*np.linalg.norm(b)), -1, 1)))\n",
    "    print(f\"   {desc}: {a} ¬∑ {b} = {dot:.3f} (angle: {angle:.1f}¬∞)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Different angles\n",
    "ax1 = axes[0]\n",
    "test_vectors = [\n",
    "    ([2, 0], [2, 0], '0¬∞ (same)'),\n",
    "    ([2, 0], [0, 2], '90¬∞ (perp)'),\n",
    "    ([2, 0], [-2, 0], '180¬∞ (opp)'),\n",
    "    ([2, 0], [1.5, 1.5], '45¬∞')\n",
    "]\n",
    "colors = ['green', 'blue', 'red', 'purple']\n",
    "\n",
    "for (a, b, title), color in zip(test_vectors, colors):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    ax1.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color='black', width=0.005, alpha=0.5)\n",
    "    ax1.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, width=0.005)\n",
    "    dot_prod = np.dot(a, b)\n",
    "    ax1.text(b[0], b[1]+0.3, f'{title}\\nDot={dot_prod:.1f}', \n",
    "             fontsize=9, ha='center', color=color, fontweight='bold')\n",
    "\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-1, 3)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.set_title('Dot Product = ||a|| ||b|| cos(Œ∏)', fontweight='bold')\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Neural network intuition\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "# Draw simple neural network\n",
    "# Input layer\n",
    "input_neurons = [(2, 7), (2, 5), (2, 3)]\n",
    "for i, (x, y) in enumerate(input_neurons):\n",
    "    circle = plt.Circle((x, y), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(x, y, f'x{i+1}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Output neuron\n",
    "output_neuron = (8, 5)\n",
    "circle = plt.Circle(output_neuron, 0.4, color='lightcoral', ec='black', linewidth=2)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(output_neuron[0], output_neuron[1], 'Œ£', ha='center', va='center', \n",
    "         fontsize=14, fontweight='bold')\n",
    "\n",
    "# Weights and connections\n",
    "weights = [0.5, -0.3, 0.8]\n",
    "for (x1, y1), w in zip(input_neurons, weights):\n",
    "    ax2.arrow(x1+0.3, y1, 5.4, output_neuron[1]-y1, \n",
    "              head_width=0.2, head_length=0.2, fc='gray', ec='gray', alpha=0.6)\n",
    "    mid_x = (x1 + output_neuron[0]) / 2\n",
    "    mid_y = (y1 + output_neuron[1]) / 2\n",
    "    ax2.text(mid_x, mid_y, f'w={w}', fontsize=9, color='darkred', \n",
    "             bbox=dict(boxstyle='round,pad=0.2', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Formula\n",
    "ax2.text(5, 1, 'Output = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ = w ¬∑ x\\n(Dot Product!)', \n",
    "         ha='center', fontsize=12, fontweight='bold',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "ax2.set_title('üí° Neural Network Layer = Dot Product', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Applications of Dot Product:\")\n",
    "print(\"   1. Neural network forward pass: z = W ¬∑ x + b\")\n",
    "print(\"   2. Attention mechanisms: similarity(query, key)\")\n",
    "print(\"   3. Word embeddings: similarity = dot product\")\n",
    "print(\"   4. Cosine similarity: normalized dot product\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 3: Vector Norms - Measuring Magnitude**\n",
    "\n",
    "### üìè **3.1 Types of Norms**\n",
    "\n",
    "Norms measure the \"length\" or \"size\" of a vector.\n",
    "\n",
    "| Norm | Formula | Use Case |\n",
    "|------|---------|----------|\n",
    "| **L1 (Manhattan)** | $\\|x\\|_1 = \\sum \\|x_i\\|$ | Sparse solutions, feature selection |\n",
    "| **L2 (Euclidean)** | $\\|x\\|_2 = \\sqrt{\\sum x_i^2}$ | Standard distance, regularization |\n",
    "| **L‚àû (Max)** | $\\|x\\|_\\infty = \\max \\|x_i\\|$ | Maximum error, robustness |\n",
    "| **L0 (Pseudo)** | Count of non-zero elements | Sparsity (actual L0 is not a norm) |\n",
    "\n",
    "### üéØ **L1 vs L2 Regularization in AI**\n",
    "```\n",
    "L1 Regularization (Lasso): Produces sparse weights (feature selection)\n",
    "L2 Regularization (Ridge): Prevents large weights (weight decay)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìè VECTOR NORMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example vector\n",
    "v = np.array([3, 4])\n",
    "print(f\"\\nVector v = {v}\")\n",
    "\n",
    "# Different norms\n",
    "l0_norm = np.count_nonzero(v)  # Pseudo-norm\n",
    "l1_norm = np.sum(np.abs(v))\n",
    "l2_norm = np.sqrt(np.sum(v**2))\n",
    "linf_norm = np.max(np.abs(v))\n",
    "\n",
    "print(f\"\\nüìê Norms of v = {v}:\")\n",
    "print(f\"   L0 (count non-zero):  {l0_norm}\")\n",
    "print(f\"   L1 (Manhattan):       {l1_norm}\")\n",
    "print(f\"   L2 (Euclidean):       {l2_norm} = ‚àö{l1_norm**2}\")\n",
    "print(f\"   L‚àû (Max):             {linf_norm}\")\n",
    "\n",
    "# Verify with NumPy\n",
    "print(f\"\\nüíª NumPy verification:\")\n",
    "print(f\"   np.linalg.norm(v, 1) = {np.linalg.norm(v, 1)}\")\n",
    "print(f\"   np.linalg.norm(v, 2) = {np.linalg.norm(v, 2)}\")\n",
    "print(f\"   np.linalg.norm(v, np.inf) = {np.linalg.norm(v, np.inf)}\")\n",
    "\n",
    "# Geometric interpretation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# L2 norm (Euclidean distance)\n",
    "ax1 = axes[0]\n",
    "ax1.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label=f'v = {v}')\n",
    "ax1.plot([0, v[0]], [0, 0], 'b--', alpha=0.5, label=f'|v‚ÇÅ| = {abs(v[0])}')\n",
    "ax1.plot([v[0], v[0]], [0, v[1]], 'g--', alpha=0.5, label=f'|v‚ÇÇ| = {abs(v[1])}')\n",
    "circle = plt.Circle((0, 0), l2_norm, fill=False, color='purple', linestyle='--', linewidth=2)\n",
    "ax1.add_patch(circle)\n",
    "ax1.text(v[0]/2, v[1]/2+0.3, f'||v||‚ÇÇ = {l2_norm}', fontsize=12, fontweight='bold', color='purple')\n",
    "ax1.set_xlim(-1, 6)\n",
    "ax1.set_ylim(-1, 6)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('L2 Norm (Euclidean)\\nPythagorean Theorem', fontweight='bold')\n",
    "\n",
    "# Unit balls for different norms\n",
    "ax2 = axes[1]\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# L2 unit ball (circle)\n",
    "x_l2 = np.cos(theta)\n",
    "y_l2 = np.sin(theta)\n",
    "\n",
    "# L1 unit ball (diamond)\n",
    "x_l1 = np.cos(theta)\n",
    "y_l1 = np.sin(theta)\n",
    "# Normalize to L1 = 1\n",
    "l1_norms = np.abs(x_l1) + np.abs(y_l1)\n",
    "x_l1 = x_l1 / l1_norms\n",
    "y_l1 = y_l1 / l1_norms\n",
    "\n",
    "# L-inf unit ball (square)\n",
    "x_linf = np.sign(np.cos(theta))\n",
    "y_linf = np.sign(np.sin(theta))\n",
    "# Normalize to L-inf = 1 (already is)\n",
    "\n",
    "ax2.plot(x_l2, y_l2, 'b-', linewidth=2, label='L2 (Circle)')\n",
    "ax2.plot(x_l1, y_l1, 'r-', linewidth=2, label='L1 (Diamond)')\n",
    "ax2.plot([-1, 1, 1, -1, -1], [-1, -1, 1, 1, -1], 'g-', linewidth=2, label='L‚àû (Square)')\n",
    "\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_title('Unit Balls: {x : ||x|| ‚â§ 1}', fontweight='bold')\n",
    "ax2.set_xlabel('x‚ÇÅ')\n",
    "ax2.set_ylabel('x‚ÇÇ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# L1 vs L2 regularization demonstration\n",
    "print(f\"\\nüéØ L1 vs L2 Regularization in Machine Learning:\")\n",
    "print(f\"   L1 encourages sparsity (many zeros) - good for feature selection\")\n",
    "print(f\"   L2 encourages small values - good for preventing overfitting\")\n",
    "\n",
    "# Demonstrate sparsity\n",
    "np.random.seed(42)\n",
    "weights_l2 = np.random.randn(20)\n",
    "weights_l1 = weights_l2.copy()\n",
    "weights_l1[np.abs(weights_l1) < 0.5] = 0  # Simulate L1 thresholding\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "x_pos = np.arange(20)\n",
    "ax.bar(x_pos - 0.2, np.abs(weights_l2), 0.4, label='L2 (Ridge)', alpha=0.8, color='blue')\n",
    "ax.bar(x_pos + 0.2, np.abs(weights_l1), 0.4, label='L1 (Lasso)', alpha=0.8, color='red')\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Weight Magnitude')\n",
    "ax.set_title('L1 vs L2 Regularization Effect on Weights', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n   L2: {np.count_nonzero(weights_l2)}/20 non-zero weights\")\n",
    "print(f\"   L1: {np.count_nonzero(weights_l1)}/20 non-zero weights (sparse!)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 4: Matrix Operations - The Engine of AI**\n",
    "\n",
    "### ‚úñÔ∏è **4.1 Matrix Multiplication**\n",
    "\n",
    "Matrix multiplication is the **fundamental operation** in neural networks.\n",
    "\n",
    "**Rule:** $(m \\times n) \\times (n \\times p) = (m \\times p)$\n",
    "\n",
    "**Element:** $C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$ (dot product of row i and column j)\n",
    "\n",
    "```\n",
    "AI Application:\n",
    "Input (batch_size √ó features) @ Weights (features √ó hidden) = \n",
    "Hidden Activations (batch_size √ó hidden)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úñÔ∏è MATRIX MULTIPLICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])  # 2√ó3\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])  # 3√ó2\n",
    "\n",
    "print(f\"\\nMatrix A (2√ó3):\\n{A}\")\n",
    "print(f\"\\nMatrix B (3√ó2):\\n{B}\")\n",
    "\n",
    "# Manual calculation of first element\n",
    "print(f\"\\nüßÆ Manual calculation of C[0,0]:\")\n",
    "print(f\"   C[0,0] = A[0,:] ¬∑ B[:,0]\")\n",
    "print(f\"          = [{A[0,0]}, {A[0,1]}, {A[0,2]}] ¬∑ [{B[0,0]}, {B[1,0]}, {B[2,0]}]\")\n",
    "print(f\"          = {A[0,0]}√ó{B[0,0]} + {A[0,1]}√ó{B[1,0]} + {A[0,2]}√ó{B[2,0]}\")\n",
    "print(f\"          = {A[0,0]*B[0,0]} + {A[0,1]*B[1,0]} + {A[0,2]*B[2,0]}\")\n",
    "print(f\"          = {A[0,0]*B[0,0] + A[0,1]*B[1,0] + A[0,2]*B[2,0]}\")\n",
    "\n",
    "# NumPy multiplication\n",
    "C = A @ B\n",
    "print(f\"\\nüíª NumPy A @ B (2√ó2 result):\\n{C}\")\n",
    "\n",
    "# Verify with dot product\n",
    "print(f\"\\n‚úÖ Verification using dot products:\")\n",
    "print(f\"   Row 0 of A ¬∑ Col 0 of B = {np.dot(A[0,:], B[:,0])}\")\n",
    "print(f\"   Row 0 of A ¬∑ Col 1 of B = {np.dot(A[0,:], B[:,1])}\")\n",
    "\n",
    "# Neural network example\n",
    "print(f\"\\nüß† Neural Network Layer Example:\")\n",
    "batch_size = 4\n",
    "input_dim = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "X = np.random.randn(batch_size, input_dim)  # Input: 4 samples, 3 features\n",
    "W = np.random.randn(input_dim, hidden_dim) * 0.1  # Weights: 3‚Üí5\n",
    "b = np.random.randn(hidden_dim) * 0.1  # Bias\n",
    "\n",
    "Z = X @ W + b  # Linear transformation\n",
    "print(f\"   Input X shape:  {X.shape} (batch_size={batch_size}, features={input_dim})\")\n",
    "print(f\"   Weights W shape: {W.shape} (input_dim={input_dim}, hidden={hidden_dim})\")\n",
    "print(f\"   Output Z shape:  {Z.shape} (batch_size={batch_size}, hidden={hidden_dim})\")\n",
    "\n",
    "# Properties demonstration\n",
    "print(f\"\\nüìê Properties of Matrix Multiplication:\")\n",
    "D = np.array([[1, 2], [3, 4]])\n",
    "E = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(f\"\\nD =\\n{D}\")\n",
    "print(f\"E =\\n{E}\")\n",
    "\n",
    "print(f\"\\n   NOT Commutative: D√óE ‚â† E√óD\")\n",
    "print(f\"   D @ E =\\n{D @ E}\")\n",
    "print(f\"   E @ D =\\n{E @ D}\")\n",
    "\n",
    "print(f\"\\n   IS Associative: (D√óE)√óF = D√ó(E√óF)\")\n",
    "\n",
    "print(f\"\\n   Transpose property: (D√óE)·µÄ = E·µÄ √ó D·µÄ\")\n",
    "print(f\"   (D @ E).T =\\n{(D @ E).T}\")\n",
    "print(f\"   E.T @ D.T =\\n{E.T @ D.T}\")\n",
    "\n",
    "# Visualization of matrix multiplication pattern\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Create a visual representation\n",
    "ax.text(0.5, 8, 'Matrix A (m√ón)', fontsize=14, ha='center', fontweight='bold')\n",
    "ax.text(4.5, 8, 'Matrix B (n√óp)', fontsize=14, ha='center', fontweight='bold')\n",
    "ax.text(2.5, 2, 'Result C (m√óp)', fontsize=14, ha='center', fontweight='bold')\n",
    "\n",
    "# Draw matrices as grids\n",
    "def draw_matrix(ax, x, y, rows, cols, color, label):\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            rect = plt.Rectangle((x+j*0.8, y-i*0.8), 0.7, 0.7, \n",
    "                               fill=True, facecolor=color, edgecolor='black', alpha=0.3)\n",
    "            ax.add_patch(rect)\n",
    "    ax.text(x + cols*0.4, y+0.5, label, fontsize=10, ha='center')\n",
    "\n",
    "draw_matrix(ax, 0, 6, 3, 4, 'lightblue', 'A (3√ó4)')\n",
    "draw_matrix(ax, 4, 6, 4, 2, 'lightgreen', 'B (4√ó2)')\n",
    "draw_matrix(ax, 2, 4, 3, 2, 'lightcoral', 'C (3√ó2)')\n",
    "\n",
    "# Show operation\n",
    "ax.annotate('', xy=(3.5, 5.5), xytext=(2.5, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax.text(3, 5.8, '@', fontsize=16, ha='center', fontweight='bold')\n",
    "ax.annotate('', xy=(3, 3.8), xytext=(3, 4.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "ax.text(3.2, 4.3, '=', fontsize=16, ha='center', fontweight='bold')\n",
    "\n",
    "# Formula\n",
    "ax.text(2.5, 0.5, r'$C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}$', \n",
    "        fontsize=16, ha='center', style='italic')\n",
    "ax.text(2.5, -0.3, 'Dot product of row i of A and column j of B', \n",
    "        fontsize=10, ha='center', color='gray')\n",
    "\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 9)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Why Matrix Multiplication is Fast in AI:\")\n",
    "print(\"   ‚Ä¢ GPUs are optimized for parallel matrix operations\")\n",
    "print(\"   ‚Ä¢ One matrix multiply = thousands of dot products in parallel\")\n",
    "print(\"   ‚Ä¢ Modern libraries (cuBLAS) achieve near-peak FLOPS\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **4.2 Special Matrix Operations**\n",
    "\n",
    "| Operation | Notation | Purpose in AI |\n",
    "|-----------|----------|---------------|\n",
    "| **Transpose** | $A^T$ | Switch rows/columns, attention mechanisms |\n",
    "| **Inverse** | $A^{-1}$ | Solving linear systems, normalization |\n",
    "| **Trace** | $tr(A)$ | Sum of diagonals, Frobenius norm |\n",
    "| **Determinant** | $det(A)$ | Volume scaling, invertibility check |\n",
    "| **Rank** | $rank(A)$ | Linear independence of rows/columns |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ SPECIAL MATRIX OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 10]], dtype=float)\n",
    "\n",
    "print(f\"\\nMatrix A:\\n{A}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nüîÄ Transpose A·µÄ:\\n{A.T}\")\n",
    "print(f\"   np.transpose(A) = A.T\")\n",
    "\n",
    "# Inverse\n",
    "print(f\"\\nüîÑ Inverse A‚Åª¬π:\")\n",
    "try:\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    print(A_inv)\n",
    "    \n",
    "    # Verify: A @ A_inv = I\n",
    "    identity_check = A @ A_inv\n",
    "    print(f\"\\n‚úÖ Verification A @ A‚Åª¬π (should be I):\\n{identity_check.round(10)}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"   Matrix is singular (not invertible)\")\n",
    "\n",
    "# Determinant\n",
    "det_A = np.linalg.det(A)\n",
    "print(f\"\\nüìê Determinant: {det_A:.4f}\")\n",
    "print(f\"   Interpretation: Volume scaling factor\")\n",
    "print(f\"   det=0: Matrix collapses space (singular)\")\n",
    "print(f\"   det<0: Reflection involved\")\n",
    "\n",
    "# Trace\n",
    "trace_A = np.trace(A)\n",
    "print(f\"\\n‚úö Trace (sum of diagonal): {trace_A}\")\n",
    "print(f\"   Trace(A) = {' + '.join([str(A[i,i]) for i in range(3)])} = {trace_A}\")\n",
    "\n",
    "# Rank\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "print(f\"\\nüèÜ Rank: {rank_A}\")\n",
    "print(f\"   Number of linearly independent rows/columns\")\n",
    "print(f\"   Full rank: rank = min(rows, cols)\")\n",
    "\n",
    "# Eigenvalues (preview of next chapter)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(f\"\\nüîÆ Eigenvalues: {eigenvalues.round(4)}\")\n",
    "print(f\"   (Will explore in detail in Chapter 6)\")\n",
    "\n",
    "# Visual demonstration of transformation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Original unit vectors\n",
    "u = np.array([1, 0])\n",
    "v = np.array([0, 1])\n",
    "origin = np.array([0, 0])\n",
    "\n",
    "# 2D transformation matrix\n",
    "T = np.array([[2, 1], \n",
    "              [1, 3]])\n",
    "\n",
    "transformed_u = T @ u\n",
    "transformed_v = T @ v\n",
    "\n",
    "# Plot 1: Original space\n",
    "ax1 = axes[0, 0]\n",
    "ax1.quiver(origin[0], origin[1], u[0], u[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='red', width=0.05, label='i (1,0)')\n",
    "ax1.quiver(origin[0], origin[1], v[0], v[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='blue', width=0.05, label='j (0,1)')\n",
    "ax1.set_xlim(-1, 4)\n",
    "ax1.set_ylim(-1, 4)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Original Space\\nBasis Vectors', fontweight='bold')\n",
    "\n",
    "# Plot 2: Transformed space\n",
    "ax2 = axes[0, 1]\n",
    "ax2.quiver(origin[0], origin[1], transformed_u[0], transformed_u[1], \n",
    "           angles='xy', scale_units='xy', scale=1, color='red', width=0.05)\n",
    "ax2.quiver(origin[0], origin[1], transformed_v[0], transformed_v[1], \n",
    "           angles='xy', scale_units='xy', scale=1, color='blue', width=0.05)\n",
    "ax2.set_xlim(-1, 4)\n",
    "ax2.set_ylim(-1, 4)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_title(f'Transformed by T\\nT = [[2,1],[1,3]]', fontweight='bold')\n",
    "\n",
    "# Plot 3: Grid transformation\n",
    "ax3 = axes[0, 2]\n",
    "for i in range(-2, 3):\n",
    "    for j in range(-2, 3):\n",
    "        point = np.array([i, j])\n",
    "        transformed = T @ point\n",
    "        ax3.plot(point[0], point[1], 'bo', alpha=0.3, markersize=4)\n",
    "        ax3.plot(transformed[0], transformed[1], 'ro', alpha=0.5, markersize=4)\n",
    "\n",
    "ax3.set_xlim(-10, 10)\n",
    "ax3.set_ylim(-10, 10)\n",
    "ax3.set_aspect('equal')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_title('Grid Transformation\\nBlue: Original, Red: Transformed', fontweight='bold')\n",
    "\n",
    "# Plot 4: Transpose visualization\n",
    "ax4 = axes[1, 0]\n",
    "M = np.array([[1, 2], [3, 4]])\n",
    "M_T = M.T\n",
    "ax4.text(0.5, 0.7, f'M =\\n{M}', fontsize=14, family='monospace', \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "ax4.text(0.5, 0.3, f'M·µÄ =\\n{M_T}', fontsize=14, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax4.text(0.5, 0.5, '‚Üì', fontsize=20, ha='center')\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Transpose: Rows‚ÜîColumns', fontweight='bold')\n",
    "\n",
    "# Plot 5: Determinant as area\n",
    "ax5 = axes[1, 1]\n",
    "det_T = np.linalg.det(T)\n",
    "parallelogram = np.array([[0,0], transformed_u, transformed_u+transformed_v, transformed_v, [0,0]])\n",
    "ax5.fill(parallelogram[:,0], parallelogram[:,1], alpha=0.3, color='purple', \n",
    "         label=f'Area = |det(T)| = {abs(det_T):.1f}')\n",
    "ax5.plot(parallelogram[:,0], parallelogram[:,1], 'k-', linewidth=2)\n",
    "ax5.set_xlim(-1, 5)\n",
    "ax5.set_ylim(-1, 5)\n",
    "ax5.set_aspect('equal')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.legend()\n",
    "ax5.set_title('Determinant = Area Scaling', fontweight='bold')\n",
    "\n",
    "# Plot 6: Rank illustration\n",
    "ax6 = axes[1, 2]\n",
    "full_rank = np.array([[1, 2], [3, 4]])\n",
    "rank_deficient = np.array([[1, 2], [2, 4]])  # Row 2 = 2 √ó Row 1\n",
    "\n",
    "ax6.text(0.5, 0.8, 'Full Rank (rank=2):', fontsize=12, ha='center', fontweight='bold')\n",
    "ax6.text(0.5, 0.65, str(full_rank), fontsize=12, ha='center', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax6.text(0.5, 0.45, 'Rank Deficient (rank=1):', fontsize=12, ha='center', fontweight='bold')\n",
    "ax6.text(0.5, 0.3, str(rank_deficient), fontsize=12, ha='center', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "ax6.text(0.5, 0.1, 'Row 2 = 2 √ó Row 1\\nLinearly dependent!', fontsize=10, ha='center', \n",
    "         style='italic', color='red')\n",
    "ax6.set_xlim(0, 1)\n",
    "ax6.set_ylim(0, 1)\n",
    "ax6.axis('off')\n",
    "ax6.set_title('Matrix Rank', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Applications:\")\n",
    "print(\"   ‚Ä¢ Transpose: Attention mechanisms (Q, K, V matrices)\")\n",
    "print(\"   ‚Ä¢ Inverse: Batch normalization, solving linear systems\")\n",
    "print(\"   ‚Ä¢ Determinant: Normalizing flows, change of variables\")\n",
    "print(\"   ‚Ä¢ Rank: Low-rank approximations, model compression\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 5: Linear Systems and Vector Spaces**\n",
    "\n",
    "### üèóÔ∏è **5.1 Linear Independence and Span**\n",
    "\n",
    "**Linear Independence:** No vector can be written as a combination of others.\n",
    "\n",
    "**Span:** All possible vectors you can reach by combining a set of vectors.\n",
    "\n",
    "**Basis:** A minimal set of linearly independent vectors that span the space.\n",
    "\n",
    "```\n",
    "In AI: Feature selection (removing redundant features)\n",
    "       Embedding spaces (word2vec basis vectors)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèóÔ∏è LINEAR INDEPENDENCE AND SPAN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Linear independence example\n",
    "print(f\"\\nüìê Linear Independence Check:\")\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "v3 = np.array([0, 0, 1])\n",
    "v4 = np.array([1, 1, 0])  # Dependent: v4 = v1 + v2\n",
    "\n",
    "print(f\"v1 = {v1}\")\n",
    "print(f\"v2 = {v2}\")\n",
    "print(f\"v3 = {v3}\")\n",
    "print(f\"v4 = {v4} = v1 + v2 (dependent!)\")\n",
    "\n",
    "# Check using matrix rank\n",
    "independent_matrix = np.vstack([v1, v2, v3])\n",
    "dependent_matrix = np.vstack([v1, v2, v4])\n",
    "\n",
    "print(f\"\\nRank of [v1;v2;v3]: {np.linalg.matrix_rank(independent_matrix)} (full rank, independent)\")\n",
    "print(f\"Rank of [v1;v2;v4]: {np.linalg.matrix_rank(dependent_matrix)} (rank deficient, dependent)\")\n",
    "\n",
    "# Span demonstration in 2D\n",
    "print(f\"\\nüéØ Span Demonstration (2D):\")\n",
    "u = np.array([2, 1])\n",
    "v = np.array([1, 2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Single vector span (line)\n",
    "ax1 = axes[0]\n",
    "t_values = np.linspace(-3, 3, 100)\n",
    "for t in t_values:\n",
    "    point = t * u\n",
    "    ax1.plot(point[0], point[1], 'bo', markersize=2, alpha=0.5)\n",
    "ax1.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label='u')\n",
    "ax1.set_xlim(-7, 7)\n",
    "ax1.set_ylim(-4, 4)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Span{u} = Line through origin\\n(1D subspace of R¬≤)', fontweight='bold')\n",
    "\n",
    "# Plot 2: Two independent vectors span (plane)\n",
    "ax2 = axes[1]\n",
    "for a in np.linspace(-2, 2, 20):\n",
    "    for b in np.linspace(-2, 2, 20):\n",
    "        point = a*u + b*v\n",
    "        ax2.plot(point[0], point[1], 'go', markersize=2, alpha=0.3)\n",
    "ax2.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label='u')\n",
    "ax2.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.01, label='v')\n",
    "ax2.set_xlim(-8, 8)\n",
    "ax2.set_ylim(-8, 8)\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_title('Span{u,v} = Entire Plane\\n(2D subspace = R¬≤)', fontweight='bold')\n",
    "\n",
    "# Plot 3: Linear combination visualization\n",
    "ax3 = axes[2]\n",
    "a, b = 2, -1\n",
    "result = a*u + b*v\n",
    "ax3.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label=f'u (√ó{a})')\n",
    "ax3.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.01, label=f'v (√ó{b})')\n",
    "ax3.quiver(0, 0, a*u[0], a*u[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.005, alpha=0.5)\n",
    "ax3.quiver(a*u[0], a*u[1], b*v[0], b*v[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.005, alpha=0.5)\n",
    "ax3.quiver(0, 0, result[0], result[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='purple', width=0.015, label=f'2u - v = {result}')\n",
    "ax3.set_xlim(-3, 6)\n",
    "ax3.set_ylim(-3, 6)\n",
    "ax3.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax3.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax3.set_aspect('equal')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "ax3.set_title('Linear Combination: 2u - v\\n(Parallelogram Law)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Applications:\")\n",
    "print(\"   ‚Ä¢ Feature extraction: Finding basis for data (PCA)\")\n",
    "print(\"   ‚Ä¢ Embeddings: Word vectors span semantic space\")\n",
    "print(\"   ‚Ä¢ Compression: Low-rank approximation reduces dimensionality\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 6: Eigenvalues and Eigenvectors - The Character of a Matrix**\n",
    "\n",
    "### üîÆ **6.1 The Fundamental Definition**\n",
    "\n",
    "**Eigenvalue Equation:** $A\\mathbf{v} = \\lambda\\mathbf{v}$\n",
    "\n",
    "```\n",
    "A: Matrix\n",
    "v: Eigenvector (direction)\n",
    "Œª: Eigenvalue (scaling factor along that direction)\n",
    "\n",
    "Interpretation: When A transforms v, v only stretches/shrinks,\n",
    "it doesn't rotate! The eigenvalue tells us by how much.\n",
    "```\n",
    "\n",
    "**In AI:**\n",
    "- **PCA:** Eigenvectors of covariance matrix = principal components\n",
    "- **PageRank:** Eigenvector of link matrix\n",
    "- **Stability:** Eigenvalues of Hessian determine convergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÆ EIGENVALUES AND EIGENVECTORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple 2x2 matrix\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]], dtype=float)\n",
    "\n",
    "print(f\"\\nMatrix A:\\n{A}\")\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(f\"\\nüîç Eigenvalues: {eigenvalues}\")\n",
    "print(f\"   Œª‚ÇÅ = {eigenvalues[0]:.4f}\")\n",
    "print(f\"   Œª‚ÇÇ = {eigenvalues[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüìê Eigenvectors (columns):\")\n",
    "print(eigenvectors)\n",
    "print(f\"   v‚ÇÅ = {eigenvectors[:, 0]}\")\n",
    "print(f\"   v‚ÇÇ = {eigenvectors[:, 1]}\")\n",
    "\n",
    "# Verification: A @ v = Œª * v\n",
    "print(f\"\\n‚úÖ Verification (A @ v should equal Œª * v):\")\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    lam = eigenvalues[i]\n",
    "    lhs = A @ v\n",
    "    rhs = lam * v\n",
    "    print(f\"\\n   For Œª = {lam:.4f}:\")\n",
    "    print(f\"   A @ v = {lhs}\")\n",
    "    print(f\"   Œª * v = {rhs}\")\n",
    "    print(f\"   Match: {np.allclose(lhs, rhs)}\")\n",
    "\n",
    "# Characteristic equation visualization\n",
    "print(f\"\\nüìù Characteristic Equation:\")\n",
    "print(f\"   det(A - ŒªI) = 0\")\n",
    "print(f\"   For our 2√ó2 matrix:\")\n",
    "print(f\"   |{A[0,0]-eigenvalues[0]:.2f}  {A[0,1]:.2f}|\")\n",
    "print(f\"   |{A[1,0]:.2f}  {A[1,1]-eigenvalues[0]:.2f}| = 0\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Transformation and eigenvectors\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Grid of points\n",
    "x = np.linspace(-2, 2, 10)\n",
    "y = np.linspace(-2, 2, 10)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "points = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "# Transform\n",
    "transformed_points = A @ points\n",
    "\n",
    "# Plot original grid\n",
    "ax1.plot(points[0, :], points[1, :], 'bo', alpha=0.3, markersize=3, label='Original')\n",
    "\n",
    "# Plot transformed grid\n",
    "ax1.plot(transformed_points[0, :], transformed_points[1, :], 'ro', alpha=0.3, markersize=3, label='Transformed')\n",
    "\n",
    "# Plot eigenvectors\n",
    "origin = np.array([[0, 0], [0, 0]])\n",
    "for i in range(2):\n",
    "    vec = eigenvectors[:, i]\n",
    "    # Original eigenvector\n",
    "    ax1.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=['green', 'purple'][i], width=0.008, \n",
    "               label=f'v{i+1} (Œª={eigenvalues[i]:.2f})')\n",
    "    # Transformed eigenvector (should be collinear)\n",
    "    t_vec = A @ vec\n",
    "    ax1.quiver(0, 0, t_vec[0], t_vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=['green', 'purple'][i], width=0.005, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax1.set_xlim(-10, 10)\n",
    "ax1.set_ylim(-10, 10)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Eigenvectors: Directions Unchanged by Transformation\\n(Solid: Original, Dashed: Transformed)', \n",
    "              fontweight='bold')\n",
    "\n",
    "# Plot 2: Eigenvalue interpretation\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Different types of matrices and their eigenvalues\n",
    "matrix_types = [\n",
    "    ('Symmetric', np.array([[2, 1], [1, 2]]), 'blue'),\n",
    "    ('Diagonal', np.array([[3, 0], [0, 1]]), 'green'),\n",
    "    ('Rotation', np.array([[0, -1], [1, 0]]), 'red'),  # Complex eigenvalues\n",
    "    ('Scaling', np.array([[2, 0], [0, 0.5]]), 'purple')\n",
    "]\n",
    "\n",
    "for name, M, color in matrix_types:\n",
    "    try:\n",
    "        eigs, _ = np.linalg.eig(M)\n",
    "        # Plot eigenvalues in complex plane\n",
    "        for ev in eigs:\n",
    "            if np.isreal(ev):\n",
    "                ax2.plot(np.real(ev), np.imag(ev), 'o', markersize=12, \n",
    "                        color=color, label=f'{name}: {ev:.2f}' if ev == eigs[0] else \"\")\n",
    "            else:\n",
    "                ax2.plot(np.real(ev), np.imag(ev), 's', markersize=12, \n",
    "                        color=color, label=f'{name}: complex')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlim(-3, 4)\n",
    "ax2.set_ylim(-2, 2)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('Real Part')\n",
    "ax2.set_ylabel('Imaginary Part')\n",
    "ax2.set_title('Eigenvalues in Complex Plane', fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Add unit circle\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(0.7, 0.7, 'Unit Circle', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîë Key Properties:\")\n",
    "print(\"   ‚Ä¢ Trace = Sum of eigenvalues\")\n",
    "print(f\"     {np.trace(A)} = {eigenvalues[0]} + {eigenvalues[1]} = {eigenvalues.sum()}\")\n",
    "print(\"   ‚Ä¢ Determinant = Product of eigenvalues\")\n",
    "print(f\"     {np.linalg.det(A):.4f} = {eigenvalues[0]} √ó {eigenvalues[1]} = {eigenvalues.prod():.4f}\")\n",
    "\n",
    "print(\"\\nüí° AI Applications:\")\n",
    "print(\"   ‚Ä¢ PCA: Eigenvectors of covariance matrix = principal components\")\n",
    "print(\"   ‚Ä¢ Spectral clustering: Eigenvectors of graph Laplacian\")\n",
    "print(\"   ‚Ä¢ Stability analysis: Eigenvalues of Hessian in optimization\")\n",
    "print(\"   ‚Ä¢ PageRank: Dominant eigenvector of transition matrix\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 7: Singular Value Decomposition (SVD) - The Universal Factorization**\n",
    "\n",
    "### üî® **7.1 The SVD Formula**\n",
    "\n",
    "**Any** matrix $A$ (m√ón) can be decomposed as:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "Where:\n",
    "- $U$: m√óm orthogonal matrix (left singular vectors)\n",
    "- $\\Sigma$: m√ón diagonal matrix (singular values $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq 0$)\n",
    "- $V^T$: n√ón orthogonal matrix (right singular vectors, transposed)\n",
    "\n",
    "```\n",
    "AI Applications:\n",
    "‚Ä¢ Dimensionality reduction (PCA is SVD on centered data)\n",
    "‚Ä¢ Recommendation systems (collaborative filtering)\n",
    "‚Ä¢ Image compression (keep top k singular values)\n",
    "‚Ä¢ Noise reduction (small singular values = noise)\n",
    "‚Ä¢ Pseudo-inverse for least squares\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî® SINGULAR VALUE DECOMPOSITION (SVD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a matrix\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(5, 4)\n",
    "print(f\"\\nMatrix A (5√ó4):\\n{A.round(3)}\")\n",
    "\n",
    "# SVD decomposition\n",
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "print(f\"\\nüîç SVD Components:\")\n",
    "print(f\"U shape: {U.shape} (left singular vectors)\")\n",
    "print(f\"Œ£ (singular values): {S.round(4)}\")\n",
    "print(f\"V·µÄ shape: {Vt.shape} (right singular vectors)\")\n",
    "\n",
    "# Reconstruct\n",
    "Sigma = np.diag(S)\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "print(f\"\\n‚úÖ Reconstruction Error: {np.linalg.norm(A - A_reconstructed):.2e}\")\n",
    "\n",
    "# Low-rank approximation\n",
    "print(f\"\\nüìâ Low-Rank Approximation (Image Compression Demo):\")\n",
    "ranks = [1, 2, 3]\n",
    "for k in ranks:\n",
    "    # Keep only top k singular values\n",
    "    U_k = U[:, :k]\n",
    "    S_k = np.diag(S[:k])\n",
    "    Vt_k = Vt[:k, :]\n",
    "    \n",
    "    A_approx = U_k @ S_k @ Vt_k\n",
    "    error = np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro')\n",
    "    \n",
    "    print(f\"\\n   Rank-{k} approximation:\")\n",
    "    print(f\"   Storage: {U_k.size + S_k.size + Vt_k.size} numbers vs {A.size} original\")\n",
    "    print(f\"   Relative error: {error:.4f}\")\n",
    "\n",
    "# Visualization: Image compression example\n",
    "from scipy import datasets\n",
    "try:\n",
    "    face = datasets.face(gray=True)\n",
    "except:\n",
    "    # Create synthetic image if scipy dataset unavailable\n",
    "    x = np.linspace(-3, 3, 256)\n",
    "    y = np.linspace(-3, 3, 256)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    face = np.sin(X**2 + Y**2) + np.cos(X*2) * np.sin(Y*2)\n",
    "\n",
    "# Perform SVD\n",
    "U_img, S_img, Vt_img = np.linalg.svd(face, full_matrices=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(face, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Singular value spectrum\n",
    "axes[1, 0].semilogy(S_img[:50], 'b-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Index')\n",
    "axes[1, 0].set_ylabel('Singular Value (log scale)')\n",
    "axes[1, 0].set_title('Singular Value Spectrum', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstructions with different ranks\n",
    "ranks = [5, 20, 50]\n",
    "for idx, k in enumerate(ranks):\n",
    "    row = idx // 2\n",
    "    col = idx % 2 + 1 if idx < 2 else (idx - 2) % 3\n",
    "    \n",
    "    U_k = U_img[:, :k]\n",
    "    S_k = np.diag(S_img[:k])\n",
    "    Vt_k = Vt_img[:k, :]\n",
    "    compressed = U_k @ S_k @ Vt_k\n",
    "    \n",
    "    axes[row, col].imshow(compressed, cmap='gray')\n",
    "    axes[row, col].set_title(f'Rank {k}\\n({100*k*(U_img.shape[0]+Vt_img.shape[1])/face.size:.1f}% storage)', \n",
    "                            fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Fill last subplot with cumulative energy\n",
    "axes[1, 2].plot(np.cumsum(S_img**2) / np.sum(S_img**2), 'r-', linewidth=2)\n",
    "axes[1, 2].axhline(y=0.95, color='g', linestyle='--', label='95% energy')\n",
    "axes[1, 2].set_xlabel('Number of Components')\n",
    "axes[1, 2].set_ylabel('Cumulative Energy')\n",
    "axes[1, 2].set_title('Energy Retention', fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° AI Applications of SVD:\")\n",
    "print(\"   1. Latent Semantic Analysis (LSA) in NLP\")\n",
    "print(\"   2. Collaborative Filtering (Netflix prize)\")\n",
    "print(\"   3. Noise reduction in signals\")\n",
    "print(\"   4. Pseudo-inverse for least squares problems\")\n",
    "print(\"   5. Model compression (low-rank factorization)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 8: Tensors - Beyond Matrices**\n",
    "\n",
    "### üé≤ **8.1 What are Tensors?**\n",
    "\n",
    "Tensors generalize scalars, vectors, and matrices to higher dimensions.\n",
    "\n",
    "| Order | Name | Shape | Example |\n",
    "|-------|------|-------|---------|\n",
    "| 0 | Scalar | () | Learning rate = 0.01 |\n",
    "| 1 | Vector | (3,) | [1, 2, 3] |\n",
    "| 2 | Matrix | (3, 4) | [[1,2,3,4], ...] |\n",
    "| 3 | 3D Tensor | (2, 3, 4) | Batch of images (B√óH√óW) |\n",
    "| 4 | 4D Tensor | (100, 3, 32, 32) | Batch of RGB images (B√óC√óH√óW) |\n",
    "| N | N-D Tensor | (d1, d2, ..., dn) | General data |\n",
    "\n",
    "**In Deep Learning:**\n",
    "- **Images:** (Batch, Channels, Height, Width) = 4D tensor\n",
    "- **Videos:** (Batch, Frames, Channels, Height, Width) = 5D tensor\n",
    "- **Transformer activations:** (Batch, Sequence, Features) = 3D tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üé≤ TENSORS - MULTI-DIMENSIONAL ARRAYS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creating tensors\n",
    "print(\"\\nüõ†Ô∏è Creating Tensors:\")\n",
    "\n",
    "# 3D Tensor (e.g., time series: samples √ó timesteps √ó features)\n",
    "tensor_3d = np.random.randn(4, 10, 5)  # 4 samples, 10 time steps, 5 features\n",
    "print(f\"\\n3D Tensor (time series data):\")\n",
    "print(f\"   Shape: {tensor_3d.shape}\")\n",
    "print(f\"   Dimensions: {tensor_3d.ndim}\")\n",
    "print(f\"   Total elements: {tensor_3d.size}\")\n",
    "print(f\"   Memory: {tensor_3d.nbytes / 1024:.2f} KB\")\n",
    "\n",
    "# 4D Tensor (e.g., batch of images: batch √ó channels √ó height √ó width)\n",
    "tensor_4d = np.random.randn(16, 3, 32, 32)  # 16 RGB images of 32√ó32\n",
    "print(f\"\\n4D Tensor (batch of images):\")\n",
    "print(f\"   Shape: {tensor_4d.shape} = (Batch, Channels, Height, Width)\")\n",
    "print(f\"   Interpretation: {tensor_4d.shape[0]} images, {tensor_4d.shape[1]} channels, \"\n",
    "      f\"{tensor_4d.shape[2]}√ó{tensor_4d.shape[3]} pixels\")\n",
    "\n",
    "# 5D Tensor (e.g., video: batch √ó frames √ó channels √ó height √ó width)\n",
    "tensor_5d = np.random.randn(2, 30, 3, 64, 64)  # 2 videos, 30 frames each\n",
    "print(f\"\\n5D Tensor (video data):\")\n",
    "print(f\"   Shape: {tensor_5d.shape}\")\n",
    "print(f\"   Interpretation: {tensor_5d.shape[0]} videos, {tensor_5d.shape[1]} frames, \"\n",
    "      f\"{tensor_5d.shape[2]} channels, {tensor_5d.shape[3]}√ó{tensor_5d.shape[4]} resolution\")\n",
    "\n",
    "# Tensor operations\n",
    "print(f\"\\nüßÆ Tensor Operations:\")\n",
    "\n",
    "# Broadcasting\n",
    "A = np.random.randn(3, 1, 4)\n",
    "B = np.random.randn(1, 5, 4)\n",
    "C = A + B  # Broadcasts to (3, 5, 4)\n",
    "print(f\"\\nBroadcasting: {A.shape} + {B.shape} = {C.shape}\")\n",
    "\n",
    "# Tensor contraction (generalization of matrix multiplication)\n",
    "# Batch matrix multiplication\n",
    "batch_A = np.random.randn(10, 3, 4)  # 10 matrices of 3√ó4\n",
    "batch_B = np.random.randn(10, 4, 5)  # 10 matrices of 4√ó5\n",
    "batch_C = np.matmul(batch_A, batch_B)  # 10 matrices of 3√ó5\n",
    "print(f\"\\nBatch matrix multiply: {batch_A.shape} @ {batch_B.shape} = {batch_C.shape}\")\n",
    "\n",
    "# Tensor reshape (view) - crucial in deep learning\n",
    "tensor = np.random.randn(2, 3, 4, 5)\n",
    "reshaped = tensor.reshape(2, -1)  # Flatten last 3 dimensions\n",
    "print(f\"\\nReshape: {tensor.shape} ‚Üí {reshaped.shape}\")\n",
    "\n",
    "# Einsum (Einstein summation) - powerful tensor operation\n",
    "# Matrix multiplication using einsum: ij,jk->ik\n",
    "A = np.random.randn(3, 4)\n",
    "B = np.random.randn(4, 5)\n",
    "C_einsum = np.einsum('ij,jk->ik', A, B)\n",
    "C_normal = A @ B\n",
    "print(f\"\\nEinsum verification: {np.allclose(C_einsum, C_normal)}\")\n",
    "\n",
    "# Visualization of tensor dimensions\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "# 1D\n",
    "ax1 = axes[0]\n",
    "ax1.plot([0, 1, 2], [0, 0, 0], 'o-', markersize=20, linewidth=5, color='blue')\n",
    "ax1.set_xlim(-0.5, 2.5)\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_title('1D: Vector\\nShape: (3,)', fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2D\n",
    "ax2 = axes[1]\n",
    "rect = plt.Rectangle((0, 0), 3, 2, fill=True, facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "ax2.add_patch(rect)\n",
    "ax2.set_xlim(-0.5, 3.5)\n",
    "ax2.set_ylim(-0.5, 2.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('2D: Matrix\\nShape: (2, 3)', fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# 3D\n",
    "ax3 = axes[2]\n",
    "# Draw 3D cube representation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax3.remove()\n",
    "ax3 = fig.add_subplot(1, 4, 3, projection='3d')\n",
    "xx, yy = np.meshgrid([0, 1], [0, 1])\n",
    "zz = np.zeros_like(xx)\n",
    "ax3.plot_surface(xx, yy, zz, alpha=0.3, color='red')\n",
    "ax3.plot_surface(xx, yy, zz+1, alpha=0.3, color='blue')\n",
    "ax3.set_title('3D: Tensor\\nShape: (2, 2, 2)', fontweight='bold')\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('Y')\n",
    "ax3.set_zlabel('Z')\n",
    "\n",
    "# 4D\n",
    "ax4 = axes[3]\n",
    "ax4.text(0.5, 0.7, '4D Tensor', fontsize=16, ha='center', fontweight='bold')\n",
    "ax4.text(0.5, 0.5, 'Shape: (B, C, H, W)', fontsize=12, ha='center', family='monospace')\n",
    "ax4.text(0.5, 0.3, 'Batch √ó Channels √ó Height √ó Width', fontsize=10, ha='center', style='italic')\n",
    "ax4.text(0.5, 0.1, '(Cannot visualize directly,\\nuse slices or projections)', fontsize=9, ha='center', color='gray')\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Deep Learning Tensor Examples:\")\n",
    "print(\"   ‚Ä¢ Input: (batch_size, seq_len, embedding_dim) - Transformer input\")\n",
    "print(\"   ‚Ä¢ Conv layer: (batch, channels_out, height, width)\")\n",
    "print(\"   ‚Ä¢ Attention weights: (batch, heads, seq_len, seq_len)\")\n",
    "print(\"   ‚Ä¢ LSTM hidden: (num_layers, batch, hidden_size)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 9: Linear Algebra in Neural Networks**\n",
    "\n",
    "### üß† **9.1 Fully Connected Layer = Matrix Multiplication**\n",
    "\n",
    "```\n",
    "Input x (batch √ó features) @ Weights W (features √ó hidden) + Bias b\n",
    "= Activations (batch √ó hidden)\n",
    "\n",
    "x ¬∑ W + b = z\n",
    "activation(z) = output\n",
    "```\n",
    "\n",
    "### üéØ **9.2 Backpropagation = Chain Rule + Jacobians**\n",
    "\n",
    "Gradients flow backward through matrix multiplications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß† LINEAR ALGEBRA IN NEURAL NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"Demonstrate linear algebra operations in a neural network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Layer 1: Linear + ReLU\n",
    "        self.z1 = X @ self.W1 + self.b1  # Matrix mult + broadcast\n",
    "        self.a1 = self.relu(self.z1)      # Element-wise activation\n",
    "        \n",
    "        # Layer 2: Linear + Sigmoid\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.output = self.sigmoid(self.z2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]  # batch size\n",
    "        \n",
    "        # Output layer gradient (chain rule)\n",
    "        dz2 = self.output - y  # dL/dz2\n",
    "        dW2 = (self.a1.T @ dz2) / m  # dL/dW2 = a1.T @ dz2\n",
    "        db2 = np.mean(dz2, axis=0)   # dL/db2\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        da1 = dz2 @ self.W2.T  # dL/da1\n",
    "        dz1 = da1 * (self.z1 > 0)  # dL/dz1 (ReLU derivative)\n",
    "        dW1 = (X.T @ dz1) / m\n",
    "        db1 = np.mean(dz1, axis=0)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        \n",
    "        return np.mean((self.output - y)**2)\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 4)  # 100 samples, 4 features\n",
    "y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(float).reshape(-1, 1)  # Simple pattern\n",
    "\n",
    "print(f\"\\nüéØ Neural Network Architecture:\")\n",
    "print(f\"   Input:  {X_train.shape[1]} features\")\n",
    "print(f\"   Hidden: 8 neurons (ReLU)\")\n",
    "print(f\"   Output: 1 neuron (Sigmoid)\")\n",
    "\n",
    "# Initialize and train\n",
    "nn = SimpleNN(4, 8, 1)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    output = nn.forward(X_train)\n",
    "    loss = nn.backward(X_train, y_train, learning_rate=0.1)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"   Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss Curve', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Weight matrices visualization\n",
    "axes[1].axis('off')\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[1].set_ylim(0, 10)\n",
    "\n",
    "# Draw network architecture\n",
    "layer_sizes = [4, 8, 1]\n",
    "layer_positions = [2, 5, 8]\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "for i, (size, pos, color) in enumerate(zip(layer_sizes, layer_positions, colors)):\n",
    "    # Draw neurons\n",
    "    for j in range(size):\n",
    "        y_pos = 5 - (size-1)/2 + j\n",
    "        circle = plt.Circle((pos, y_pos), 0.3, fill=True, facecolor=color, \n",
    "                           edgecolor='black', linewidth=2)\n",
    "        axes[1].add_patch(circle)\n",
    "        if size <= 8:\n",
    "            axes[1].text(pos, y_pos, f'n{j+1}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Layer label\n",
    "    axes[1].text(pos, 9, f'Layer {i+1}\\n({size})', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw connections\n",
    "for i in range(len(layer_sizes)-1):\n",
    "    x1, x2 = layer_positions[i] + 0.3, layer_positions[i+1] - 0.3\n",
    "    y1_start = 5 - (layer_sizes[i]-1)/2\n",
    "    y2_start = 5 - (layer_sizes[i+1]-1)/2\n",
    "    \n",
    "    for j in range(layer_sizes[i]):\n",
    "        for k in range(layer_sizes[i+1]):\n",
    "            y1 = y1_start + j\n",
    "            y2 = y2_start + k\n",
    "            axes[1].plot([x1, x2], [y1, y2], 'k-', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "# Matrix operation annotations\n",
    "axes[1].text(3.5, 0.5, 'X @ W1 + b1', fontsize=10, ha='center', style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "axes[1].text(6.5, 0.5, 'A1 @ W2 + b2', fontsize=10, ha='center', style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "axes[1].set_title('Neural Network = Matrix Operations', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Weight Matrix Shapes:\")\n",
    "print(f\"   W1: {nn.W1.shape} (input_dim √ó hidden_dim)\")\n",
    "print(f\"   W2: {nn.W2.shape} (hidden_dim √ó output_dim)\")\n",
    "\n",
    "# Calculate number of parameters\n",
    "total_params = (4 * 8 + 8) + (8 * 1 + 1)\n",
    "print(f\"\\nüìà Total Parameters: {total_params}\")\n",
    "print(f\"   W1: {4*8} + b1: {8} + W2: {8*1} + b2: {1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ **Chapter 10: Advanced Topics - Attention & Optimization**\n",
    "\n",
    "### üéØ **10.1 Attention Mechanism = Softmax(Q @ K^T / ‚àöd) @ V**\n",
    "\n",
    "The famous Transformer attention is pure linear algebra!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ ATTENTION MECHANISM - SCALED DOT-PRODUCT ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: (batch, seq_len, d_k)\n",
    "    K: (batch, seq_len, d_k)\n",
    "    V: (batch, seq_len, d_v)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Q @ K^T\n",
    "    scores = Q @ K.transpose(0, 2, 1)  # (batch, seq_len, seq_len)\n",
    "    print(f\"1. Q @ K^T shape: {scores.shape}\")\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    print(f\"2. Scaled scores shape: {scaled_scores.shape}\")\n",
    "    \n",
    "    # Step 3: Softmax\n",
    "    attention_weights = softmax(scaled_scores, axis=-1)\n",
    "    print(f\"3. Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"   Row sums (should be 1): {attention_weights[0].sum(axis=-1)}\")\n",
    "    \n",
    "    # Step 4: Apply to V\n",
    "    output = attention_weights @ V  # (batch, seq_len, d_v)\n",
    "    print(f\"4. Output shape: {output.shape}\")\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example: Translate \"hello world\"\n",
    "batch_size = 1\n",
    "seq_len = 4  # 4 tokens\n",
    "d_k = 8      # key dimension\n",
    "d_v = 8      # value dimension\n",
    "\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(batch_size, seq_len, d_k)  # Queries\n",
    "K = np.random.randn(batch_size, seq_len, d_k)  # Keys\n",
    "V = np.random.randn(batch_size, seq_len, d_v)  # Values\n",
    "\n",
    "print(f\"\\nInput shapes:\")\n",
    "print(f\"   Q (Queries): {Q.shape}\")\n",
    "print(f\"   K (Keys):    {K.shape}\")\n",
    "print(f\"   V (Values):  {V.shape}\")\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Visualize attention pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Attention heatmap\n",
    "im = axes[0].imshow(attention_weights[0], cmap='viridis', aspect='auto')\n",
    "axes[0].set_xticks(range(seq_len))\n",
    "axes[0].set_yticks(range(seq_len))\n",
    "axes[0].set_xticklabels([f'Token {i+1}' for i in range(seq_len)])\n",
    "axes[0].set_yticklabels([f'Token {i+1}' for i in range(seq_len)])\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "axes[0].set_title('Attention Weights Heatmap\\n(Which tokens attend to which)', fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Attention as graph\n",
    "ax1 = axes[1]\n",
    "G = attn_weights[0]\n",
    "threshold = 0.2\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        if G[i, j] > threshold:\n",
    "            ax1.arrow(j, seq_len-i-1, 0, 0, head_width=0.1, head_length=0.05, \n",
    "                     fc='blue', ec='blue', alpha=G[i,j], \n",
    "                     length_includes_head=True, width=0.02)\n",
    "            ax1.text((i+j)/2, (seq_len-i-1 + seq_len-j-1)/2, \n",
    "                    f'{G[i,j]:.2f}', fontsize=8, ha='center')\n",
    "\n",
    "for i in range(seq_len):\n",
    "    ax1.plot(i, seq_len-i-1, 'o', markersize=20, color='lightblue', \n",
    "            markeredgecolor='black', markeredgewidth=2)\n",
    "    ax1.text(i, seq_len-i-1, f'T{i+1}', ha='center', va='center', \n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlim(-0.5, seq_len-0.5)\n",
    "ax1.set_ylim(-0.5, seq_len-0.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Attention as Graph\\n(Arrows show information flow)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Why this matters:\")\n",
    "print(\"   ‚Ä¢ GPT, BERT, T5 all use this attention mechanism\")\n",
    "print(\"   ‚Ä¢ O(n¬≤) complexity in sequence length\")\n",
    "print(\"   ‚Ä¢ Matrix multiplication is highly optimized on GPUs\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèîÔ∏è **10.2 Optimization: Gradients and Hessians**\n",
    "\n",
    "**Gradient:** $\\nabla f$ - Direction of steepest ascent\n",
    "**Hessian:** $\\nabla^2 f$ - Curvature matrix (second derivatives)\n",
    "\n",
    "**Newton's Method:** $\\theta_{new} = \\theta_{old} - H^{-1} \\nabla f$\n",
    "\n",
    "**In AI:**\n",
    "- Gradient descent: Follow negative gradient\n",
    "- Hessian tells us about convergence (positive definite = local minimum)\n",
    "- Eigenvalues of Hessian: learning rate scaling, flat vs sharp minima\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèîÔ∏è OPTIMIZATION: GRADIENTS AND CURVATURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a 2D loss landscape\n",
    "def loss_function(w1, w2):\n",
    "    \"\"\"Bowl-shaped loss with some curvature\"\"\"\n",
    "    return 0.5 * (w1**2 + 2*w2**2) + 0.1 * w1 * w2\n",
    "\n",
    "def gradient(w1, w2):\n",
    "    \"\"\"Gradient of loss\"\"\"\n",
    "    d_w1 = w1 + 0.1 * w2\n",
    "    d_w2 = 2*w2 + 0.1 * w1\n",
    "    return np.array([d_w1, d_w2])\n",
    "\n",
    "def hessian(w1, w2):\n",
    "    \"\"\"Hessian matrix (constant for quadratic)\"\"\"\n",
    "    return np.array([[1.0, 0.1], \n",
    "                     [0.1, 2.0]])\n",
    "\n",
    "# Create grid\n",
    "w1_range = np.linspace(-3, 3, 100)\n",
    "w2_range = np.linspace(-3, 3, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Z = loss_function(W1, W2)\n",
    "\n",
    "# Optimization path\n",
    "def optimize(start_point, method='gradient_descent', lr=0.1, steps=20):\n",
    "    path = [start_point.copy()]\n",
    "    point = start_point.copy()\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        grad = gradient(point[0], point[1])\n",
    "        \n",
    "        if method == 'gradient_descent':\n",
    "            point = point - lr * grad\n",
    "        elif method == 'newton':\n",
    "            H = hessian(point[0], point[1])\n",
    "            H_inv = np.linalg.inv(H)\n",
    "            point = point - H_inv @ grad\n",
    "            \n",
    "        path.append(point.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Run optimizations\n",
    "start = np.array([2.5, 2.5])\n",
    "path_gd = optimize(start, 'gradient_descent', lr=0.3, steps=15)\n",
    "path_newton = optimize(start, 'newton', steps=5)\n",
    "\n",
    "print(f\"\\nüìâ Optimization Comparison:\")\n",
    "print(f\"   Starting point: {start}\")\n",
    "print(f\"   True minimum: [0, 0]\")\n",
    "\n",
    "print(f\"\\n   Gradient Descent ({len(path_gd)-1} steps):\")\n",
    "print(f\"   Final point: {path_gd[-1]}\")\n",
    "print(f\"   Final loss: {loss_function(path_gd[-1][0], path_gd[-1][1]):.6f}\")\n",
    "\n",
    "print(f\"\\n   Newton's Method ({len(path_newton)-1} steps):\")\n",
    "print(f\"   Final point: {path_newton[-1]}\")\n",
    "print(f\"   Final loss: {loss_function(path_newton[-1][0], path_newton[-1][1]):.6f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Contour plot with paths\n",
    "ax1 = axes[0]\n",
    "contour = ax1.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.plot(path_gd[:, 0], path_gd[:, 1], 'r.-', markersize=10, linewidth=2, \n",
    "         label=f'Gradient Descent ({len(path_gd)-1} steps)')\n",
    "ax1.plot(path_newton[:, 0], path_newton[:, 1], 'b.-', markersize=10, linewidth=2, \n",
    "         label=f'Newton ({len(path_newton)-1} steps)')\n",
    "ax1.plot(0, 0, 'g*', markersize=20, label='Global Minimum')\n",
    "ax1.set_xlabel('w‚ÇÅ')\n",
    "ax1.set_ylabel('w‚ÇÇ')\n",
    "ax1.legend()\n",
    "ax1.set_title('Optimization Paths on Loss Landscape', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Hessian eigenvalue analysis\n",
    "ax2 = axes[1]\n",
    "H = hessian(0, 0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(H)\n",
    "print(f\"\\nüîç Hessian at minimum:\")\n",
    "print(f\"   H =\\n{H}\")\n",
    "print(f\"   Eigenvalues: {eigenvalues}\")\n",
    "print(f\"   Condition number: {max(eigenvalues)/min(eigenvalues):.2f}\")\n",
    "\n",
    "# Draw Hessian ellipse\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "# Scale by inverse sqrt of eigenvalues for unit circle\n",
    "ellipse = np.array([eigenvectors @ np.diag(1/np.sqrt(eigenvalues)) @ \n",
    "                   np.array([np.cos(t), np.sin(t)]) for t in theta])\n",
    "ax2.plot(ellipse[:, 0], ellipse[:, 1], 'b-', linewidth=2, label='Hessian curvature')\n",
    "ax2.plot([0, eigenvectors[0,0]*eigenvalues[0]], \n",
    "         [0, eigenvectors[1,0]*eigenvalues[0]], \n",
    "         'r-', linewidth=3, label=f'Eigenvector 1 (Œª={eigenvalues[0]:.2f})')\n",
    "ax2.plot([0, eigenvectors[0,1]*eigenvalues[1]], \n",
    "         [0, eigenvectors[1,1]*eigenvalues[1]], \n",
    "         'g-', linewidth=3, label=f'Eigenvector 2 (Œª={eigenvalues[1]:.2f})')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_xlim(-3, 3)\n",
    "ax2.set_ylim(-3, 3)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_title('Hessian Ellipse\\n(Inverse curvature in each direction)', fontweight='bold')\n",
    "ax2.set_xlabel('Œîw‚ÇÅ')\n",
    "ax2.set_ylabel('Œîw‚ÇÇ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Why this matters for AI:\")\n",
    "print(\"   ‚Ä¢ Eigenvalues of Hessian determine learning rate bounds\")\n",
    "print(\"   ‚Ä¢ Large condition number = ill-conditioned optimization\")\n",
    "print(\"   ‚Ä¢ Sharp minima (large eigenvalues) may not generalize well\")\n",
    "print(\"   ‚Ä¢ Second-order methods use Hessian for faster convergence\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì **Summary & Cheat Sheet**\n",
    "\n",
    "### üìö **Linear Algebra for AI - Quick Reference**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  FOUNDATION CONCEPTS                                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Vector:     v ‚àà ‚Ñù‚Åø        Point in n-dimensional space        ‚îÇ\n",
    "‚îÇ  Matrix:     A ‚àà ‚Ñù^(m√ón)   Linear transformation               ‚îÇ\n",
    "‚îÇ  Tensor:     T ‚àà ‚Ñù^(d‚ÇÅ√ó...√ód‚Çô)  Multi-dimensional array        ‚îÇ\n",
    "‚îÇ  Norm:       ||v||         Length/magnitude                     ‚îÇ\n",
    "‚îÇ  Dot:        v¬∑w           Similarity measure                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  KEY OPERATIONS                                                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Matrix Mult:  C = AB where C_ij = Œ£_k A_ik B_kj               ‚îÇ\n",
    "‚îÇ  Transpose:    (A^T)_ij = A_ji                                  ‚îÇ\n",
    "‚îÇ  Inverse:      A‚Åª¬πA = I (if det(A) ‚â† 0)                        ‚îÇ\n",
    "‚îÇ  Eigen-decomp: Av = Œªv                                          ‚îÇ\n",
    "‚îÇ  SVD:          A = UŒ£V^T                                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  AI APPLICATIONS                                                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Neural Networks:  y = œÉ(Wx + b)                                ‚îÇ\n",
    "‚îÇ  Attention:        softmax(QK^T/‚àöd)V                            ‚îÇ\n",
    "‚îÇ  PCA:              Eigenvectors of covariance matrix            ‚îÇ\n",
    "‚îÇ  Embeddings:       High-dimensional vectors                     ‚îÇ\n",
    "‚îÇ  Optimization:     ‚àáf (gradient), ‚àá¬≤f (Hessian)                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üéØ **Learning Checklist**\n",
    "\n",
    "- [ ] Understand vectors as points/arrows in space\n",
    "- [ ] Master matrix multiplication (order matters!)\n",
    "- [ ] Know when to use different norms (L1 vs L2)\n",
    "- [ ] Interpret eigenvalues/eigenvectors geometrically\n",
    "- [ ] Apply SVD for dimensionality reduction\n",
    "- [ ] Think of neural networks as compositions of linear + nonlinear ops\n",
    "- [ ] Understand attention as matrix multiplication\n",
    "\n",
    "### üìñ **Next Steps**\n",
    "\n",
    "1. **Practice:** Implement these operations from scratch\n",
    "2. **Apply:** Use in PyTorch/TensorFlow and inspect shapes\n",
    "3. **Explore:** Read papers on Transformers, GANs, Normalizing Flows\n",
    "4. **Deepen:** Study differential geometry for advanced ML\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Linear Algebra is not just math‚Äîit's the language that allows computers to learn! üöÄ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Interactive shape checker for neural networks\n",
    "def check_nn_shapes():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéÆ BONUS: NEURAL NETWORK SHAPE CALCULATOR\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nEnter your layer dimensions to verify matrix operations:\")\n",
    "    \n",
    "    # Example calculation\n",
    "    batch_size = 32\n",
    "    input_dim = 784\n",
    "    hidden_dim = 256\n",
    "    output_dim = 10\n",
    "    \n",
    "    print(f\"\\nüìä Example: MLP for MNIST\")\n",
    "    print(f\"   Input:  ({batch_size}, {input_dim})\")\n",
    "    print(f\"   W1:     ({input_dim}, {hidden_dim})\")\n",
    "    print(f\"   b1:     ({hidden_dim},)\")\n",
    "    print(f\"   Hidden: ({batch_size}, {hidden_dim})  ‚Üê Input @ W1 + b1\")\n",
    "    print(f\"   W2:     ({hidden_dim}, {output_dim})\")\n",
    "    print(f\"   b2:     ({output_dim},)\")\n",
    "    print(f\"   Output: ({batch_size}, {output_dim})  ‚Üê Hidden @ W2 + b2\")\n",
    "    \n",
    "    # Parameter count\n",
    "    params_w1 = input_dim * hidden_dim\n",
    "    params_b1 = hidden_dim\n",
    "    params_w2 = hidden_dim * output_dim\n",
    "    params_b2 = output_dim\n",
    "    total = params_w1 + params_b1 + params_w2 + params_b2\n",
    "    \n",
    "    print(f\"\\nüìà Total Parameters: {total:,}\")\n",
    "    print(f\"   W1: {params_w1:,} + b1: {params_b1:,} + W2: {params_w2:,} + b2: {params_b2:,}\")\n",
    "    \n",
    "    # Memory estimation\n",
    "    bytes_per_param = 4  # float32\n",
    "    memory_mb = (total * bytes_per_param) / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Memory Required: ~{memory_mb:.2f} MB\")\n",
    "\n",
    "check_nn_shapes()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ CONGRATULATIONS! You've completed the Linear Algebra for AI course!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nRemember: The best way to learn is by doing. Try implementing\")\n",
    "print(\"these concepts in your next machine learning project!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Author:** Tassawar Abbas  \n",
    "**Email:** abbas829@gmail.com\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
