{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f7f9fa; padding: 25px; border-radius: 15px; border-left: 10px solid #228B22; box-shadow: 0 4px 8px rgba(0,0,0,0.05);\">\n",
    "    <h1 style=\"color: #2e7d32; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-weight: bold; margin-bottom: 10px;\">\n",
    "        üéì Mastering Lasso Regression (L1 Regularization): A Comprehensive Guide\n",
    "    </h1>\n",
    "    <h3 style=\"color: #555; margin-top: 0;\">Automated Feature Selection & Solving Overfitting Like a Pro</h3>\n",
    "    <hr style=\"border: 0.5px solid #ccc;\">\n",
    "    <div style=\"display: flex; align-items: center; margin-top: 15px;\">\n",
    "        <div style=\"margin-right: 20px;\">\n",
    "            <strong>Author:</strong> <span style=\"color: #1a73e8;\">Tassawar Abbas</span><br>\n",
    "            <strong>Email:</strong> <span style=\"color: #1a73e8;\">abbas829@gmail.com</span>\n",
    "        </div>\n",
    "        <div style=\"border-left: 1px solid #ddd; padding-left: 20px;\">\n",
    "            <strong>Course:</strong> Advanced Machine Learning Series<br>\n",
    "            <strong>Topic:</strong> Lasso Regression (L1)\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Executive Summary\n",
    "Welcome to this **Masterclass** on Lasso Regression! This notebook is designed as a **complete student guideline**. We don't just show you the code; we explain the *Why*, *When*, and *Where* of L1 Regularization.\n",
    "\n",
    "**In this guide, you will find:**\n",
    "1. **Mathematical Intuition**: The logic behind the penalty.\n",
    "2. **Geometric Interpretation**: Why Lasso actually selects features.\n",
    "3. **Practical Implementation**: Real-world Scikit-Learn code.\n",
    "4. **Comparative Analysis**: A ready-to-use table comparing Lasso, Ridge, and Elastic Net.\n",
    "5. **Decision Guide**: A student framework for choosing the right model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Regularization üõ°Ô∏è\n",
    "In Machine Learning, we often face the problem of **Overfitting**‚Äîwhere our model performs exceptionally well on the training data but fails to generalize to unseen data. \n",
    "\n",
    "To solve this, we use **Regularization**. It works by adding a \"penalty\" term to our cost function that discourages the model from assigning too much importance (high weights) to any single feature.\n",
    "\n",
    "### There are two main types of regularization:\n",
    "*   **Ridge Regression (L2):** Adds a squared penalty ($\\lambda \\sum \\beta^2$).\n",
    "*   **Lasso Regression (L1):** Adds an absolute value penalty ($\\lambda \\sum |\\beta|$).\n",
    "\n",
    "Today, we focus on **Lasso**, which stands for **Least Absolute Shrinkage and Selection Operator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting aesthetic parameters for plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Mathematical Foundation üßÆ\n",
    "The ordinary Linear Regression cost function (Ordinary Least Squares - OLS) is:\n",
    "$$ J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Lasso Regression modifies this by adding the **L1 Penalty**:\n",
    "$$ J(\\beta) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}))^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "### Where:\n",
    "*   $\\lambda$ (or $\\alpha$ in scikit-learn) is the **Regularization Strength**.\n",
    "*   When $\\alpha = 0$, it is just OLS Regression.\n",
    "*   As $\\alpha \\to \\infty$, all coefficients ($\\beta$) tend towards zero.\n",
    "\n",
    "> [!TIP]\n",
    "> **Key Power of Lasso:** Unlike Ridge, Lasso can shrink some coefficients exactly to **zero**. This makes it an built-in tool for **Feature Selection**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset with some irrelevant features\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "\n",
    "# Generate random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Only the first 3 features are actually relevant\n",
    "true_weights = np.zeros(n_features)\n",
    "true_weights[:3] = [10, -5, 2]\n",
    "\n",
    "# Target variable with some noise\n",
    "y = X @ true_weights + np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Feature names\n",
    "feature_names = [f'Feature_{i}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Target'] = y\n",
    "\n",
    "print(f\"Dataset generated with {n_features} features.\")\n",
    "print(f\"Information: Only Features 0, 1, and 2 are significant.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Result Explanation: Data Generation\n",
    "In the step above, we deliberately created a \"Hard Mode\" problem for our model:\n",
    "1. We gave the computer **20 different features** (columns).\n",
    "2. However, we secretly made sure **only 3 features** actually affect the result.\n",
    "3. **The Goal**: We want to see if Lasso is smart enough to find these 3 important features and ignore the other 17 random ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting and Scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training Models\n",
    "models = {\n",
    "    'Linear Regression (OLS)': LinearRegression(),\n",
    "    'Lasso (alpha=0.1)': Lasso(alpha=0.1),\n",
    "    'Lasso (alpha=1.0)': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "coefs = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results[name] = r2_score(y_test, y_pred)\n",
    "    coefs[name] = model.coef_\n",
    "\n",
    "pd.DataFrame(results, index=['R2 Score']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Result Explanation: Performance Comparison\n",
    "The table above shows the **R¬≤ Score** (Coefficient of Determination):\n",
    "- **OLS (Linear Regression)**: Often gets a high score on training but might struggle if there's noise, as it tries to use all 20 features, even the useless ones.\n",
    "- **Lasso (alpha=0.1)**: Usually performs better here because it \"simplifies\" the model by ignoring noise. It finds a balance between accuracy and simplicity.\n",
    "- **Lasso (alpha=1.0)**: If alpha is too high, the model becomes *too* thin (underfitting) and the performance might drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting coefficients to see feature selection in action\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Setup positions for bars\n",
    "x_pos = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x_pos - width, coefs['Linear Regression (OLS)'], width, label='OLS', color='gray', alpha=0.5)\n",
    "plt.bar(x_pos, coefs['Lasso (alpha=0.1)'], width, label='Lasso (0.1)', color='skyblue')\n",
    "plt.bar(x_pos + width, coefs['Lasso (alpha=1.0)'], width, label='Lasso (1.0)', color='darkblue')\n",
    "\n",
    "plt.xticks(x_pos, feature_names, rotation=45)\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('How Lasso Shrinks Irrelevant Features to Zero')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Counting non-zero coefficients\n",
    "for name, c in coefs.items():\n",
    "    print(f\"{name} has {np.sum(c != 0)} non-zero coefficients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Result Explanation: Coefficient Plot\n",
    "**Look at the bars!** \n",
    "- Notice how the **Gray bars (OLS)** are present for almost every feature. OLS is trying to learn from everything, including the \"trash\" features.\n",
    "- Notice how the **Blue bars (Lasso)** are mostly missing for Features 3 to 19. \n",
    "- **Conclusion**: Lasso successfully \"killed\" the coefficients of the irrelevant features by setting them exactly to zero. This is the **Magic of L1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Student Guideline: When, Why & Where? üìò\n",
    "\n",
    "For students, choosing between algorithms can be confusing. Use this framework to master Lasso.\n",
    "\n",
    "### üü¢ When to use Lasso?\n",
    "Use Lasso when:\n",
    "1.  **High Dimensionality**: You have a huge number of features (e.g., 100+) but you suspect only a few (e.g., 5-10) are actually important.\n",
    "2.  **Multicollinearity**: If features are highly correlated, Lasso will pick one and zero out the others, helping you find the \"representative\" variable.\n",
    "3.  **Need for Interpretability**: If you need to tell your boss *exactly* which factors affect sales, Lasso helps by removing the clutter.\n",
    "\n",
    "### üî¥ Why to use Lasso?\n",
    "1.  **Simplicity**: It creates \"Parsimonious\" models‚Äîmodels that do a lot with very little data.\n",
    "2.  **Automatic Selection**: It removes the need for manual feature selection (like checking correlation heatmaps endlessly).\n",
    "3.  **Prevents Overfitting**: By penalizing large weights, it ensures the model doesn't \"memorize\" the training data.\n",
    "\n",
    "### üó∫Ô∏è Where to use Lasso?\n",
    "1.  **Genomics**: Identifying which few genes (out of thousands) cause a specific disease.\n",
    "2.  **Marketing**: Figuring out which 2 or 3 ad campaigns (out of 50) actually drove traffic.\n",
    "3.  **Finance**: Selecting the most impactful economic indicators for stock prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparative Analysis Table üìä\n",
    "\n",
    "Here is the ultimate comparison table for your studies:\n",
    "\n",
    "| Feature | **OLS Regression** | **Ridge (L2)** | **Lasso (L1)** | **Elastic Net** |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Penalty** | None | Squared Weights ($\\beta^2$) | Absolute Weights ($|\\beta|$) | Both L1 and L2 |\n",
    "| **Shrinkage** | No | Shrinks towards zero | Shrinks to exactly zero | Combination |\n",
    "| **Feature Selection** | No | No | **Yes (Built-in)** | Yes |\n",
    "| **Correlated Features** | Becomes unstable | Keeps all of them | Picks one, drops others | Groups them together |\n",
    "| **Best Case** | Few, strong features | Many small features | Few significant features | Complex datasets |\n",
    "\n",
    "> **Student Note**: If you can't decide between Ridge and Lasso, use **Elastic Net**! It's the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Geometric Interpretation: Why Zero? üíé\n",
    "Why does Lasso set weights to zero while Ridge just makes them small?\n",
    "\n",
    "*   **Ridge (L2)** has a circular constraint ($ \\beta_1^2 + \\beta_2^2 \\le s $). The OLS solution usually hits this circle at a point where both $\\beta_i$ are non-zero.\n",
    "*   **Lasso (L1)** has a diamond-shaped constraint ($ |\\beta_1| + |\\beta_2| \\le s $). Because of the sharp corners on the axes, the OLS solution is much more likely to hit the constraint **on an axis**, forcing one coefficient to zero.\n",
    "\n",
    "![L1 vs L2](https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_regularization.png)\n",
    "*Image source: Wikipedia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of Alpha on Coefficients\n",
    "alphas = np.logspace(-4, 2, 100)\n",
    "coef_list = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    coef_list.append(lasso.coef_)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_list)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Path: How coefficients vanish as Alpha increases')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Result Explanation: Lasso Path\n",
    "This graph is called a **Lasso Path**. \n",
    "- As you move from **Left to Right**, the Alpha (penalty) increases.\n",
    "- Notice how the lines (coefficients) start dropping to the horizontal zero line one by one.\n",
    "- The features that \"die\" last are the most important ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Final Tips üöÄ\n",
    "\n",
    "Lasso Regression is a powerful tool in your ML arsenal. It ensures that your models remain simple, interpretable, and accurate.\n",
    "\n",
    "### üèÅ Final Student Checklist:\n",
    "1.  **Always Scale**: Lasso is a distance-based penalty. If one feature is 0-1 and another is 0-1,000,000, Lasso will fail unless you scale them first.\n",
    "2.  **Cross-Validation**: Don't guess $\\alpha$. Use `LassoCV` to let the data tell you the best value.\n",
    "3.  **Check for Sparsity**: If your data isn't sparse (meaning all features are important), Ridge might be a better choice.\n",
    "\n",
    "---\n",
    "### üì¨ Get in Touch\n",
    "If you found this masterclass helpful, let's connect!\n",
    "- **Kaggle Profile:** [abbas829](https://www.kaggle.com/abbas829)\n",
    "- **Email:** [abbas829@gmail.com](mailto:abbas829@gmail.com)\n",
    "\n",
    "**Keep Learning, Keep Growing!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
