{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 20px; background-color: #f8f9fa; border-radius: 10px; border-left: 5px solid #007bff; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px;\">\n",
    "    <h2 style=\"color: #007bff; margin-top: 0;\">ðŸ‘¤ Author Profile</h2>\n",
    "    <hr style=\"border: 0; height: 1px; background: #ddd; margin: 10px 0;\">\n",
    "    <p style=\"margin: 5px 0;\"><strong>Name:</strong> Tassawar Abbas</p>\n",
    "        <p style=\"margin: 5px 0;\"><strong>Contact:</strong> <a href=\"mailto:abbas829@gmail.com\" style=\"text-decoration: none; color: #007bff;\">abbas829@gmail.com</a></p>\n",
    "    <p style=\"font-style: italic; color: #6c757d; margin-top: 15px;\">\"Passionate about making complex AI concepts accessible to everyone through high-quality educational resources.\"</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for professional visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š The Complete Linear Regression Masterclass: From Theory to Production\n",
    "\n",
    "**Author:** Tassawar Abbas  \n",
    "**Date:** 2026-02-11  \n",
    "**Tags:** `linear-regression`, `machine-learning`, `tutorial`, `statistics`, `python`, `scikit-learn`, `feature-engineering` \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Executive Summary\n",
    "\n",
    "This notebook provides a **comprehensive, production-ready guide** to Linear Regressionâ€”the foundational algorithm that underpins modern machine learning. Whether you're a beginner seeking intuition or a practitioner looking for advanced techniques, this resource covers:\n",
    "\n",
    "- **Mathematical foundations** with intuitive explanations\n",
    "- **Assumption testing** with diagnostic visualizations  \n",
    "- **Implementation from scratch** and with industry-standard libraries\n",
    "- **Regularization techniques** (Ridge, Lasso, Elastic Net)\n",
    "- **Real-world case study** on California Housing dataset\n",
    "- **Production-ready code patterns**\n",
    "\n",
    "<div style=\"padding: 15px; background-color: #e7f3ff; border-left: 5px solid #2196F3; border-radius: 5px; margin-bottom: 10px;\">\n",
    "<strong style=\"color: #0c5460;\">ðŸ’¡ Why This Matters:</strong>\n",
    "Linear regression isn't just a simple algorithmâ€”it's the gateway to understanding neural networks, GLMs, and the bias-variance tradeoff. Master this, and you've mastered the DNA of supervised learning.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Table of Contents\n",
    "\n",
    "1. [Theoretical Foundations](#1-theoretical-foundations)\n",
    "2. [Mathematical Deep Dive](#2-mathematical-deep-dive)\n",
    "3. [Implementation From Scratch](#3-implementation-from-scratch)\n",
    "4. [Assumption Validation & Diagnostics](#4-assumption-validation--diagnostics)\n",
    "5. [Advanced Techniques](#5-advanced-techniques)\n",
    "6. [Real-World Case Study](#6-real-world-case-study)\n",
    "7. [Production Best Practices](#7-production-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundations\n",
    "\n",
    "### 1.1 What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between a **dependent variable** (target) and one or more **independent variables** (features) by fitting a linear equation.\n",
    "\n",
    "**Simple Linear Regression (One Feature):**\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "**Multiple Linear Regression (Multiple Features):**\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$: Target variable (what we want to predict)\n",
    "- $\\beta_0$: Intercept (baseline value when all features are zero)\n",
    "- $\\beta_j$: Coefficients (impact of each feature)\n",
    "- $x_j$: Feature values\n",
    "- $\\epsilon$: Error term (irreducible noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Geometry of Least Squares\n",
    "\n",
    "The goal is to find the line (or hyperplane) that **minimizes the sum of squared residuals**â€”the vertical distances between observed and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for visualization\n",
    "np.random.seed(42)\n",
    "X_simple = np.linspace(0, 10, 50)\n",
    "y_simple = 2.5 * X_simple + 5 + np.random.normal(0, 3, 50)\n",
    "\n",
    "# Fit simple linear regression\n",
    "beta_1 = np.sum((X_simple - np.mean(X_simple)) * (y_simple - np.mean(y_simple))) / np.sum((X_simple - np.mean(X_simple))**2)\n",
    "beta_0 = np.mean(y_simple) - beta_1 * np.mean(X_simple)\n",
    "y_pred_simple = beta_0 + beta_1 * X_simple\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Fitted line with residuals\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(X_simple, y_simple, alpha=0.6, s=60, label='Observed Data')\n",
    "ax1.plot(X_simple, y_pred_simple, 'r-', linewidth=2, label=f'Fitted Line: y = {beta_0:.2f} + {beta_1:.2f}x')\n",
    "for i in range(len(X_simple)):\n",
    "    ax1.plot([X_simple[i], X_simple[i]], [y_simple[i], y_pred_simple[i]], 'g--', alpha=0.5)\n",
    "ax1.set_xlabel('Feature (X)', fontsize=12)\n",
    "ax1.set_ylabel('Target (y)', fontsize=12)\n",
    "ax1.set_title('Simple Linear Regression: Minimizing Squared Residuals', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: 3D visualization of multiple regression plane\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "X_3d = np.random.rand(100, 2) * 10\n",
    "y_3d = 2*X_3d[:, 0] + 3*X_3d[:, 1] + 5 + np.random.normal(0, 2, 100)\n",
    "ax2.scatter(X_3d[:, 0], X_3d[:, 1], y_3d, c='b', marker='o', alpha=0.6)\n",
    "xx, yy = np.meshgrid(np.linspace(0, 10, 10), np.linspace(0, 10, 10))\n",
    "zz = 2*xx + 3*yy + 5\n",
    "ax2.plot_surface(xx, yy, zz, alpha=0.3, color='red')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X2')\n",
    "ax2.set_zlabel('y')\n",
    "ax2.set_title('Multiple Regression: Fitting a Hyperplane', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** The \"linear\" in linear regression refers to linearity in **parameters** ($\\beta$), not necessarily in features. We can model non-linear relationships using polynomial terms while keeping the model linear in parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Deep Dive\n",
    "\n",
    "### 2.1 The Cost Function: Mean Squared Error (MSE)\n",
    "\n",
    "We minimize the **Residual Sum of Squares (RSS)**:\n",
    "\n",
    "$$J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}))^2$$\n",
    "\n",
    "### 2.2 Ordinary Least Squares (OLS) - Closed Form Solution\n",
    "\n",
    "In matrix notation:\n",
    "$$\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon$$\n",
    "\n",
    "The optimal parameters:\n",
    "$$\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "This is called the **Normal Equation**. It's computationally efficient for small-to-medium datasets but becomes expensive for large $p$ (features) due to matrix inversion $O(p^3)$.\n",
    "\n",
    "### 2.3 Gradient Descent Alternative\n",
    "\n",
    "For large datasets, we use iterative optimization:\n",
    "\n",
    "$$\\beta_j := \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate and the partial derivative is:\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation From Scratch\n",
    "\n",
    "Understanding the mechanics by building it yourself is the best way to master Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using Normal Equations and Gradient Descent.\n",
    "    Educational implementation to understand the mechanics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='normal', learning_rate=0.01, n_iterations=1000):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        self.history = {'loss': []}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Add intercept term (column of ones)\n",
    "        X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "        \n",
    "        if self.method == 'normal':\n",
    "            # Normal Equation: (X^T X)^-1 X^T y\n",
    "            try:\n",
    "                theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Handle singular matrix with pseudo-inverse\n",
    "                theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "            \n",
    "            self.intercept = theta[0]\n",
    "            self.coefficients = theta[1:]\n",
    "            \n",
    "        elif self.method == 'gradient_descent':\n",
    "            # Initialize parameters\n",
    "            theta = np.random.randn(n_features + 1, 1)\n",
    "            y = y.reshape(-1, 1)\n",
    "            \n",
    "            for i in range(self.n_iterations):\n",
    "                gradients = 2/n_samples * X_b.T.dot(X_b.dot(theta) - y)\n",
    "                theta = theta - self.learning_rate * gradients\n",
    "                \n",
    "                # Track loss\n",
    "                loss = np.mean((X_b.dot(theta) - y)**2)\n",
    "                self.history['loss'].append(loss)\n",
    "            \n",
    "            self.intercept = theta[0, 0]\n",
    "            self.coefficients = theta[1:].flatten()\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.intercept + X.dot(self.coefficients)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return r2_score(y, self.predict(X))\n",
    "\n",
    "# Demonstration\n",
    "X_demo, y_demo = make_regression(n_samples=1000, n_features=3, noise=10, random_state=42)\n",
    "X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(X_demo, y_demo, test_size=0.2)\n",
    "\n",
    "# Compare implementations\n",
    "scratch_normal = LinearRegressionScratch(method='normal')\n",
    "scratch_normal.fit(X_train_demo, y_train_demo)\n",
    "\n",
    "scratch_gd = LinearRegressionScratch(method='gradient_descent', learning_rate=0.1, n_iterations=1000)\n",
    "scratch_gd.fit(X_train_demo, y_train_demo)\n",
    "\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_train_demo, y_train_demo)\n",
    "\n",
    "print(\"ðŸ“Š Implementation Comparison:\")\n",
    "print(f\"Scratch (Normal Eq) RÂ²:  {scratch_normal.score(X_test_demo, y_test_demo):.6f}\")\n",
    "print(f\"Scratch (Grad Desc) RÂ²:  {scratch_gd.score(X_test_demo, y_test_demo):.6f}\")\n",
    "print(f\"Scikit-Learn RÂ²:         {sklearn_model.score(X_test_demo, y_test_demo):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gradient descent convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scratch_gd.history['loss'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Gradient Descent Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assumption Validation & Diagnostics\n",
    "\n",
    "Linear regression relies on **LINE** assumptions plus multicollinearity. Violations lead to biased or inefficient estimates.\n",
    "\n",
    "| Assumption | Description | Diagnostic | Fix |\n",
    "|------------|-------------|------------|-----|\n",
    "| **L**inearity | Linear relationship | Residuals vs Fitted plot | Polynomial terms, transformations |\n",
    "| **I**ndependence | No autocorrelation | Durbin-Watson test | Time series models, GLS |\n",
    "| **N**ormality | Residuals ~ Normal | Q-Q plot, Shapiro-Wilk | Transformations, robust regression |\n",
    "| **E**qual Variance (Homoscedasticity) | Constant variance | Scale-Location plot | Weighted LS, transformations |\n",
    "| **No Multicollinearity** | Features not correlated | VIF < 10 | Remove features, PCA, regularization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_diagnostics(X, y, model, feature_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostic plots for linear regression assumptions.\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    standardized_residuals = (residuals - np.mean(residuals)) / np.std(residuals)\n",
    "    sqrt_standardized_residuals = np.sqrt(np.abs(standardized_residuals))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Residuals vs Fitted (Linearity & Homoscedasticity)\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(y_pred, residuals, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_xlabel('Fitted Values')\n",
    "    ax1.set_ylabel('Residuals')\n",
    "    ax1.set_title('Residuals vs Fitted\\n(Linearity & Homoscedasticity Check)', fontweight='bold')\n",
    "    \n",
    "    # 2. Q-Q Plot (Normality)\n",
    "    ax2 = axes[0, 1]\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax2)\n",
    "    ax2.set_title('Normal Q-Q Plot\\n(Normality Check)', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Scale-Location (Homoscedasticity)\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.scatter(y_pred, sqrt_standardized_residuals, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "    ax3.set_xlabel('Fitted Values')\n",
    "    ax3.set_ylabel('âˆš|Standardized Residuals|')\n",
    "    ax3.set_title('Scale-Location Plot\\n(Homoscedasticity Check)', fontweight='bold')\n",
    "    \n",
    "    # 4. Residuals vs Leverage (Influential points)\n",
    "    ax4 = axes[1, 0]\n",
    "    X_with_const = sm.add_constant(X)\n",
    "    hat_matrix = X_with_const @ np.linalg.inv(X_with_const.T @ X_with_const) @ X_with_const.T\n",
    "    leverage = np.diag(hat_matrix)\n",
    "    ax4.scatter(leverage, standardized_residuals, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "    ax4.set_xlabel('Leverage')\n",
    "    ax4.set_ylabel('Standardized Residuals')\n",
    "    ax4.set_title('Residuals vs Leverage\\n(Influential Points)', fontweight='bold')\n",
    "    \n",
    "    # 5. Distribution of Residuals\n",
    "    ax5 = axes[1, 1]\n",
    "    ax5.hist(residuals, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "    ax5.set_title('Distribution of Residuals', fontweight='bold')\n",
    "    \n",
    "    # 6. Actual vs Predicted\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.scatter(y, y_pred, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "    min_val, max_val = min(y.min(), y_pred.min()), max(y.max(), y_pred.max())\n",
    "    ax6.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax6.set_title('Actual vs Predicted', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run diagnostics on demo data\n",
    "print(\"STATISTICAL DIAGNOSTICS\")\n",
    "diag_results = comprehensive_diagnostics(X_train_demo, y_train_demo, scratch_normal, \n",
    "                                          feature_names=['Feature_1', 'Feature_2', 'Feature_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Techniques\n",
    "\n",
    "### 5.1 Regularization: Beyond Ordinary Least Squares\n",
    "\n",
    "When features are correlated or $p > n$, OLS becomes unstable. **Regularization** adds penalty terms to the cost function:\n",
    "\n",
    "- **Ridge (L2):** Shrinks coefficients toward zero. Handles multicollinearity well.\n",
    "- **Lasso (L1):** Can set coefficients exactly to zero. Performs feature selection.\n",
    "- **Elastic Net:** Combines L1 and L2 penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_comparison(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    alphas = np.logspace(-4, 4, 100)\n",
    "    models = {\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(max_iter=10000),\n",
    "        'ElasticNet': ElasticNet(max_iter=10000, l1_ratio=0.5)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for idx, (name, model) in enumerate(models.items(), 1):\n",
    "        plt.subplot(1, 3, idx)\n",
    "        coefs = []\n",
    "        for a in alphas:\n",
    "            m = Ridge(alpha=a) if name=='Ridge' else (Lasso(alpha=a) if name=='Lasso' else ElasticNet(alpha=a))\n",
    "            m.fit(X_train_scaled, y_train)\n",
    "            coefs.append(m.coef_)\n",
    "        plt.plot(alphas, coefs)\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'{name} Coefficient Paths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Polynomial Regression\n",
    "\n",
    "Linear regression can model non-linear relationships by adding polynomial terms:\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_demo():\n",
    "    np.random.seed(42)\n",
    "    X_poly = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "    y_poly = 0.5 * X_poly**3 - 2 * X_poly**2 + X_poly + 2 + np.random.normal(0, 2, 100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for degree in [1, 2, 3, 15]:\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_p = poly.fit_transform(X_poly)\n",
    "        model = LinearRegression().fit(X_p, y_poly)\n",
    "        plt.plot(X_poly, model.predict(X_p), label=f'Degree {degree}')\n",
    "    \n",
    "    plt.scatter(X_poly, y_poly, alpha=0.3, color='black')\n",
    "    plt.legend()\n",
    "    plt.title('Polynomial Regression Degree Comparison')\n",
    "    plt.show()\n",
    "\n",
    "polynomial_regression_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Case Study: California Housing\n",
    "\n",
    "Let's apply everything to the California Housing datasetâ€”a classic regression problem predicting median house values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and run baseline analysis\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Target: Median House Value (in $100,000s)\")\n",
    "\n",
    "# Feature engineering example\n",
    "X['RoomsPerHousehold'] = X['AveRooms'] / X['AveOccup']\n",
    "X['PopulationPerHousehold'] = X['Population'] / X['AveOccup']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "baseline = LinearRegression().fit(X_train, y_train)\n",
    "print(f\"Baseline RÂ²: {baseline.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Best Practices\n",
    "\n",
    "### 7.1 Production Pipeline Class\n",
    "A production-ready pipeline includes preprocessing, regularization, and robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionLinearRegression:\n",
    "    def __init__(self, model_type='ridge', poly_degree=1):\n",
    "        self.model_type = model_type\n",
    "        self.poly_degree = poly_degree\n",
    "        self.scaler = StandardScaler()\n",
    "        self.poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "        self.model = RidgeCV() if model_type=='ridge' else LinearRegression()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.poly.fit_transform(X)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.poly.transform(X)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "pipeline = ProductionLinearRegression(model_type='ridge', poly_degree=2)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Pipeline RÂ²: {r2_score(y_test, pipeline.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Key Takeaways & Checklist\n",
    "\n",
    "- [x] **Check Assumptions:** Always validate residuals for LINE criteria.\n",
    "- [x] **Standardize Features:** Essential when using regularization (Ridge/Lasso).\n",
    "- [x] **Handle Multicollinearity:** Check VIF scores and consider PCA or regularization.\n",
    "- [x] **Avoid Overfitting:** Use cross-validation and appropriate polynomial degrees.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Tassawar Abbas | abbas829@gmail.com\n",
    "\n",
    "*This notebook was created as a comprehensive educational resource. Master these fundamentals to unlock the power of modern machine learning!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
