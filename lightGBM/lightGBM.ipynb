{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üå≤ **LightGBM: The Complete Mastery Guide**\n",
        "## *From Beginner to Kaggle GrandMaster*\n",
        "\n",
        "---\n",
        "\n",
        "> **\"LightGBM is the Swiss Army Knife of Gradient Boosting\"**\n",
        "\n",
        "Welcome to the most comprehensive guide to **LightGBM** - Microsoft's gradient boosting framework that dominates machine learning competitions and production systems worldwide.\n",
        "\n",
        "### üéØ **Why This Guide?**\n",
        "\n",
        "| Level | What You'll Learn | Time |\n",
        "|-------|-------------------|------|\n",
        "| üå± **Beginner** | Installation, basic API, first model | 30 min |\n",
        "| üåø **Intermediate** | Parameters tuning, feature engineering | 2 hours |\n",
        "| üå≥ **Advanced** | Custom objectives, distributed training | 4 hours |\n",
        "| üèÜ **Expert** | Competition tricks, production optimization | Ongoing |\n",
        "\n",
        "### üìã **Table of Contents**\n",
        "\n",
        "1. [What is LightGBM?](#what)\n",
        "2. [Why LightGBM?](#why)\n",
        "3. [When to Use?](#when)\n",
        "4. [Core Concepts](#core)\n",
        "5. [Installation & Setup](#install)\n",
        "6. [Basic Usage](#basic)\n",
        "7. [Parameter Tuning](#params)\n",
        "8. [Advanced Features](#advanced)\n",
        "9. [Production Deployment](#production)\n",
        "10. [Common Pitfalls](#pitfalls)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LightGBM\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMClassifier, LGBMRegressor, plot_importance, plot_metric\n",
        "    LGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LGBM_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  LightGBM not installed. Run: pip install lightgbm\")\n",
        "\n",
        "# Scikit-learn for comparison and utilities\n",
        "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üå≤ LIGHTGBM COMPLETE GUIDE\")\n",
        "print(\"=\" * 70)\n",
        "if LGBM_AVAILABLE:\n",
        "    print(f\"‚úÖ LightGBM version: {lgb.__version__}\")\n",
        "print(\"‚úÖ All dependencies loaded!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"what\"></a>\n",
        "# üå± **Chapter 1: What is LightGBM?**\n",
        "\n",
        "## **Definition**\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  LightGBM = Light Gradient Boosting Machine                     ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  A gradient boosting framework that uses tree-based learning    ‚îÇ\n",
        "‚îÇ  algorithms, designed for:                                      ‚îÇ\n",
        "‚îÇ  ‚Ä¢ High efficiency (speed)                                      ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Low memory usage                                             ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Better accuracy                                              ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Parallel and GPU learning                                    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## **The Evolution of Boosting**\n",
        "\n",
        "| Year | Algorithm | Innovation | Limitation |\n",
        "|------|-----------|------------|------------|\n",
        "| 1990 | **AdaBoost** | First boosting algorithm | Sensitive to noise |\n",
        "| 2000 | **Gradient Boosting** | Gradient descent in function space | Slow, sequential |\n",
        "| 2014 | **XGBoost** | Regularization, parallelization | Memory intensive |\n",
        "| 2016 | **LightGBM** | GOSS, EFB, Leaf-wise growth | Can overfit small data |\n",
        "| 2017 | **CatBoost** | Native categorical handling | Slower than LightGBM |\n",
        "\n",
        "## **Key Innovations**\n",
        "\n",
        "### 1Ô∏è‚É£ **Gradient-based One-Side Sampling (GOSS)**\n",
        "```\n",
        "Problem: Gradient boosting needs to scan all data instances\n",
        "Solution: Keep instances with large gradients, randomly sample small gradients\n",
        "Result: 2x speedup without accuracy loss\n",
        "```\n",
        "\n",
        "### 2Ô∏è‚É£ **Exclusive Feature Bundling (EFB)**\n",
        "```\n",
        "Problem: High-dimensional sparse features waste memory\n",
        "Solution: Bundle mutually exclusive features (one-hot encoded)\n",
        "Result: Reduces features from high to low dimensional dense features\n",
        "```\n",
        "\n",
        "### 3Ô∏è‚É£ **Leaf-wise Tree Growth**\n",
        "```\n",
        "Level-wise (XGBoost):        Leaf-wise (LightGBM):\n",
        "      [Root]                      [Root]\n",
        "     /      \\                    /      \\\n",
        "   [A]      [B]                [A]      [B]\n",
        "   / \\      / \\                         / \\\n",
        " [C][D]  [E][F]                      [E] [F]\n",
        "                                      \\\n",
        "                                      [G]  ‚Üê Grows where loss reduces most\n",
        "\n",
        "Result: Better accuracy, but needs careful tuning to avoid overfitting\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üå± CHAPTER 1: UNDERSTANDING LIGHTGBM\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Visual comparison of tree growth strategies\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Level-wise tree (XGBoost style)\n",
        "ax1 = axes[0]\n",
        "levels = [\n",
        "    [(0.5, 0.9, \"Root\")],\n",
        "    [(0.25, 0.7, \"A\"), (0.75, 0.7, \"B\")],\n",
        "    [(0.125, 0.5, \"C\"), (0.375, 0.5, \"D\"), (0.625, 0.5, \"E\"), (0.875, 0.5, \"F\")]\n",
        "]\n",
        "colors_level = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
        "\n",
        "for level_idx, level in enumerate(levels):\n",
        "    for x, y, label in level:\n",
        "        circle = plt.Circle((x, y), 0.08, color=colors_level[level_idx % len(colors_level)], \n",
        "                           ec='black', linewidth=2, alpha=0.8)\n",
        "        ax1.add_patch(circle)\n",
        "        ax1.text(x, y, label, ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Draw connections\n",
        "connections = [((0.5, 0.9), (0.25, 0.7)), ((0.5, 0.9), (0.75, 0.7)),\n",
        "               ((0.25, 0.7), (0.125, 0.5)), ((0.25, 0.7), (0.375, 0.5)),\n",
        "               ((0.75, 0.7), (0.625, 0.5)), ((0.75, 0.7), (0.875, 0.5))]\n",
        "for start, end in connections:\n",
        "    ax1.plot([start[0], end[0]], [start[1], end[1]], 'k-', linewidth=2)\n",
        "\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0.3, 1)\n",
        "ax1.set_aspect('equal')\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Level-wise Growth (XGBoost)\\nGrow level by level', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "# Leaf-wise tree (LightGBM style)\n",
        "ax2 = axes[1]\n",
        "# Same root\n",
        "circle = plt.Circle((0.5, 0.9), 0.08, color=colors_level[0], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.5, 0.9, \"Root\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# First split\n",
        "ax2.plot([0.5, 0.3], [0.9, 0.7], 'k-', linewidth=2)\n",
        "ax2.plot([0.5, 0.7], [0.9, 0.7], 'k-', linewidth=2)\n",
        "circle = plt.Circle((0.3, 0.7), 0.08, color=colors_level[1], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.3, 0.7, \"A\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "circle = plt.Circle((0.7, 0.7), 0.08, color=colors_level[1], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.7, 0.7, \"B\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Continue splitting B (higher loss reduction)\n",
        "ax2.plot([0.7, 0.6], [0.7, 0.5], 'k-', linewidth=2)\n",
        "ax2.plot([0.7, 0.8], [0.7, 0.5], 'k-', linewidth=2)\n",
        "circle = plt.Circle((0.6, 0.5), 0.08, color=colors_level[2], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.6, 0.5, \"C\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "circle = plt.Circle((0.8, 0.5), 0.08, color=colors_level[2], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.8, 0.5, \"D\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Continue splitting D\n",
        "ax2.plot([0.8, 0.75], [0.5, 0.3], 'k-', linewidth=2)\n",
        "ax2.plot([0.8, 0.85], [0.5, 0.3], 'k-', linewidth=2)\n",
        "circle = plt.Circle((0.75, 0.3), 0.08, color=colors_level[3], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.75, 0.3, \"E\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "circle = plt.Circle((0.85, 0.3), 0.08, color=colors_level[3], ec='black', linewidth=2)\n",
        "ax2.add_patch(circle)\n",
        "ax2.text(0.85, 0.3, \"F\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Highlight asymmetric growth\n",
        "ax2.annotate('Splits where\\nloss ‚Üì most', xy=(0.8, 0.5), xytext=(0.9, 0.6),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=10, color='red', fontweight='bold')\n",
        "\n",
        "ax2.set_xlim(0, 1)\n",
        "ax2.set_ylim(0.2, 1)\n",
        "ax2.set_aspect('equal')\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Leaf-wise Growth (LightGBM)\\nGrow where loss reduces most\\n(Deeper, more complex trees)', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight:\")\n",
        "print(\"   Level-wise: Balanced, less prone to overfitting, but slower convergence\")\n",
        "print(\"   Leaf-wise: Faster convergence, better accuracy, but needs regularization\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"why\"></a>\n",
        "# üåø **Chapter 2: Why LightGBM?**\n",
        "\n",
        "## **The Performance Advantage**\n",
        "\n",
        "### üìä **Speed Comparison** (on Higgs dataset, 10M samples)\n",
        "\n",
        "| Algorithm | Training Time | Accuracy | Memory |\n",
        "|-----------|--------------|----------|--------|\n",
        "| XGBoost | 1873s | 0.789 | High |\n",
        "| **LightGBM** | **141s** ‚ö° | **0.791** ‚úÖ | **Low** ‚úÖ |\n",
        "| CatBoost | 1000s | 0.792 | Very High |\n",
        "\n",
        "### üéØ **Accuracy on Kaggle Competitions**\n",
        "\n",
        "```\n",
        "üèÜ Kaggle Winners (2016-2024)\n",
        "\n",
        "Single Models:\n",
        "‚îú‚îÄ‚îÄ LightGBM:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  45%\n",
        "‚îú‚îÄ‚îÄ XGBoost:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              28%\n",
        "‚îú‚îÄ‚îÄ Neural Nets:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      17%\n",
        "‚îî‚îÄ‚îÄ CatBoost:     ‚ñà‚ñà‚ñà‚ñà                              10%\n",
        "\n",
        "Ensembles:\n",
        "‚îî‚îÄ‚îÄ LightGBM + XGBoost + Neural Nets:  85% of top-3\n",
        "```\n",
        "\n",
        "## **When LightGBM Wins**\n",
        "\n",
        "| Scenario | LightGBM Advantage |\n",
        "|----------|-------------------|\n",
        "| **Large datasets (>10K rows)** | GOSS sampling, fast |\n",
        "| **High dimensional sparse data** | EFB bundles features |\n",
        "| **CPU-constrained environments** | Highly optimized C++ |\n",
        "| **Real-time predictions** | Fast inference |\n",
        "| **Tabular data** | Native handling |\n",
        "\n",
        "## **When LightGBM Struggles**\n",
        "\n",
        "| Scenario | Better Alternative |\n",
        "|----------|-------------------|\n",
        "| **Small datasets (<1000 rows)** | Random Forest, XGBoost |\n",
        "| **Heavy categorical features** | CatBoost |\n",
        "| **Image/text data** | Neural Networks |\n",
        "| **Need interpretability** | Decision Trees, Linear Models |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üåø CHAPTER 2: WHY LIGHTGBM? - PERFORMANCE DEMONSTRATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # Generate dataset\n",
        "    X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, \n",
        "                               n_redundant=5, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    \n",
        "    # Compare algorithms\n",
        "    import time\n",
        "    \n",
        "    models = {\n",
        "        'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nüîÑ Training {name}...\")\n",
        "        \n",
        "        # Training time\n",
        "        start = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        train_time = time.time() - start\n",
        "        \n",
        "        # Prediction time\n",
        "        start = time.time()\n",
        "        y_pred = model.predict(X_test)\n",
        "        pred_time = time.time() - start\n",
        "        \n",
        "        # Accuracy\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "        \n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Train Time (s)': train_time,\n",
        "            'Predict Time (s)': pred_time,\n",
        "            'Accuracy': acc,\n",
        "            'AUC': auc\n",
        "        })\n",
        "        \n",
        "        print(f\"   ‚úì Train: {train_time:.3f}s, Predict: {pred_time:.3f}s, AUC: {auc:.4f}\")\n",
        "    \n",
        "    # Results DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"\\nüìà Performance Summary:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Training time\n",
        "    axes[0].bar(results_df['Model'], results_df['Train Time (s)'], \n",
        "                color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "    axes[0].set_ylabel('Time (seconds)')\n",
        "    axes[0].set_title('Training Time Comparison\\n(Lower is better)', fontweight='bold')\n",
        "    axes[0].tick_params(axis='x', rotation=15)\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[1].bar(results_df['Model'], results_df['AUC'], \n",
        "                color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "    axes[1].set_ylabel('ROC-AUC')\n",
        "    axes[1].set_ylim(0.8, 1.0)\n",
        "    axes[1].set_title('Accuracy Comparison\\n(Higher is better)', fontweight='bold')\n",
        "    axes[1].tick_params(axis='x', rotation=15)\n",
        "    \n",
        "    # Speedup chart\n",
        "    baseline = results_df[results_df['Model'] == 'Gradient Boosting']['Train Time (s)'].values[0]\n",
        "    speedups = baseline / results_df['Train Time (s)']\n",
        "    axes[2].bar(results_df['Model'], speedups, \n",
        "                color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "    axes[2].set_ylabel('Speedup vs Gradient Boosting')\n",
        "    axes[2].set_title('Training Speedup\\n(Higher is better)', fontweight='bold')\n",
        "    axes[2].tick_params(axis='x', rotation=15)\n",
        "    axes[2].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüèÜ Winner: LightGBM is {speedups[0]:.1f}x faster than sklearn Gradient Boosting\")\n",
        "    print(f\"           with comparable or better accuracy!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"when\"></a>\n",
        "# üå≥ **Chapter 3: When to Use LightGBM?**\n",
        "\n",
        "## **Decision Tree: Algorithm Selection**\n",
        "\n",
        "```\n",
        "Start\n",
        "  ‚îÇ\n",
        "  ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Is your data        ‚îÇ\n",
        "‚îÇ tabular/structured? ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "  ‚îÇ No                    ‚îÇ Yes\n",
        "  ‚ñº                       ‚ñº\n",
        "Use Neural Networks   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "(CNN, Transformer,    ‚îÇ Dataset size?       ‚îÇ\n",
        " BERT, etc.)          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                        ‚îÇ <1K    ‚îÇ 1K-100K  ‚îÇ >100K\n",
        "                        ‚ñº        ‚ñº          ‚ñº\n",
        "                      Random   XGBoost   LightGBM\n",
        "                      Forest   or        ( fastest )\n",
        "                               LightGBM\n",
        "```\n",
        "\n",
        "## **Task-Based Selection Guide**\n",
        "\n",
        "| Task Type | LightGBM Suitability | Key Parameters |\n",
        "|-----------|---------------------|----------------|\n",
        "| **Binary Classification** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | `objective='binary'` |\n",
        "| **Multi-class** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | `objective='multiclass'` |\n",
        "| **Regression** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | `objective='regression'` |\n",
        "| **Ranking** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | `objective='lambdarank'` |\n",
        "| **Time Series** | ‚≠ê‚≠ê‚≠ê‚≠ê Good | Add lag features |\n",
        "| **NLP** | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ Poor | Use Transformers |\n",
        "| **Computer Vision** | ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ Very Poor | Use CNNs/ViT |\n",
        "\n",
        "## **Data Characteristics Guide**\n",
        "\n",
        "| Characteristic | LightGBM Preference | Handling Strategy |\n",
        "|----------------|---------------------|-------------------|\n",
        "| **Missing values** | ‚úÖ Handles automatically | Do nothing, or use `use_missing=False` |\n",
        "| **Categorical features** | ‚úÖ Native support (since 3.0) | Pass as `category` dtype |\n",
        "| **Imbalanced classes** | ‚ö†Ô∏è Needs tuning | Use `is_unbalance` or `scale_pos_weight` |\n",
        "| **High cardinality categoricals** | ‚ö†Ô∏è Careful | Use `max_cat_threshold` |\n",
        "| **Outliers** | ‚ö†Ô∏è Somewhat robust | Consider preprocessing |\n",
        "| **Feature interactions** | ‚úÖ Excellent | Use `interaction_constraints` |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"core\"></a>\n",
        "# üéØ **Chapter 4: Core Concepts**\n",
        "\n",
        "## **The Gradient Boosting Framework**\n",
        "\n",
        "```\n",
        "Gradient Boosting = Sequential Learning + Gradient Descent\n",
        "\n",
        "Step 1: Start with initial prediction (e.g., mean for regression)\n",
        "        F‚ÇÄ(x) = argmin·µß Œ£ L(y·µ¢, Œ≥)\n",
        "\n",
        "Step 2: For m = 1 to M (number of trees):\n",
        "        \n",
        "        a) Compute pseudo-residuals (negative gradients):\n",
        "           r·µ¢‚Çò = -[‚àÇL(y·µ¢, F(x·µ¢))/‚àÇF(x·µ¢)] at F=F‚Çò‚Çã‚ÇÅ\n",
        "        \n",
        "        b) Fit a tree to residuals: h‚Çò(x) predicts r·µ¢‚Çò\n",
        "        \n",
        "        c) Find optimal step size (line search):\n",
        "           œÅ‚Çò = argmin·µ® Œ£ L(y·µ¢, F‚Çò‚Çã‚ÇÅ(x·µ¢) + œÅh‚Çò(x·µ¢))\n",
        "        \n",
        "        d) Update model:\n",
        "           F‚Çò(x) = F‚Çò‚Çã‚ÇÅ(x) + learning_rate √ó œÅ‚Çò √ó h‚Çò(x)\n",
        "\n",
        "Final Model: F_M(x) = F‚ÇÄ(x) + Œ∑ Œ£‚Çò œÅ‚Çòh‚Çò(x)\n",
        "```\n",
        "\n",
        "## **Key Hyperparameters Hierarchy**\n",
        "\n",
        "```\n",
        "üéõÔ∏è PARAMETER HIERARCHY (Tuning Order)\n",
        "\n",
        "Level 1: Fix First (Most Impact)\n",
        "‚îú‚îÄ‚îÄ num_leaves        ‚Üê Controls model complexity (main parameter)\n",
        "‚îú‚îÄ‚îÄ learning_rate     ‚Üê Typically 0.01-0.3 (lower = slower but better)\n",
        "‚îî‚îÄ‚îÄ n_estimators      ‚Üê Use early stopping instead of fixing\n",
        "\n",
        "Level 2: Prevent Overfitting\n",
        "‚îú‚îÄ‚îÄ max_depth         ‚Üê Limit tree depth (usually -1 = unlimited)\n",
        "‚îú‚îÄ‚îÄ min_child_samples ‚Üê Min samples in leaf (prevents overfitting)\n",
        "‚îú‚îÄ‚îÄ feature_fraction  ‚Üê Column sampling (like max_features in RF)\n",
        "‚îî‚îÄ‚îÄ bagging_fraction  ‚Üê Row sampling (enables bagging)\n",
        "\n",
        "Level 3: Fine-tuning\n",
        "‚îú‚îÄ‚îÄ reg_alpha         ‚Üê L1 regularization\n",
        "‚îú‚îÄ‚îÄ reg_lambda        ‚Üê L2 regularization\n",
        "‚îú‚îÄ‚îÄ min_split_gain    ‚Üê Minimum loss reduction for split\n",
        "‚îî‚îÄ‚îÄ cat_smooth        ‚Üê For categorical features\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üéØ CHAPTER 4: CORE CONCEPTS - GRADIENT BOOSTING VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Visualize gradient boosting process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Generate synthetic regression data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100)\n",
        "y_true = np.sin(X) + 0.1 * X\n",
        "y = y_true + np.random.normal(0, 0.3, 100)\n",
        "\n",
        "# Plot 1: Data\n",
        "ax = axes[0, 0]\n",
        "ax.scatter(X, y, alpha=0.5, label='Data')\n",
        "ax.plot(X, y_true, 'r-', linewidth=2, label='True function')\n",
        "ax.set_title('1. Training Data', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 2: Initial prediction (mean)\n",
        "ax = axes[0, 1]\n",
        "F0 = np.mean(y)\n",
        "ax.scatter(X, y, alpha=0.3)\n",
        "ax.axhline(y=F0, color='green', linewidth=2, label=f'F‚ÇÄ = {F0:.2f}')\n",
        "residuals0 = y - F0\n",
        "ax.vlines(X, F0, y, alpha=0.3, color='red', label='Residuals')\n",
        "ax.set_title('2. Initial Prediction (Mean)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 3: First tree fits residuals\n",
        "ax = axes[0, 2]\n",
        "# Simple stump for visualization\n",
        "split = 5\n",
        "left_mask = X < split\n",
        "right_mask = X >= split\n",
        "pred_left = np.mean(residuals0[left_mask])\n",
        "pred_right = np.mean(residuals0[right_mask])\n",
        "pred1 = np.where(left_mask, pred_left, pred_right)\n",
        "\n",
        "ax.scatter(X, residuals0, alpha=0.5, label='Residuals (pseudo-targets)')\n",
        "ax.plot(X, pred1, 'g-', linewidth=2, label='Tree 1 predictions')\n",
        "ax.set_title('3. Tree 1: Fits Residuals', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 4: Updated model\n",
        "ax = axes[1, 0]\n",
        "learning_rate = 0.1\n",
        "F1 = F0 + learning_rate * pred1\n",
        "ax.scatter(X, y, alpha=0.3)\n",
        "ax.plot(X, F0 * np.ones_like(X), 'g--', alpha=0.5, label='F‚ÇÄ')\n",
        "ax.plot(X, F1, 'b-', linewidth=2, label=f'F‚ÇÅ = F‚ÇÄ + Œ∑√óTree‚ÇÅ')\n",
        "ax.set_title('4. Updated Model (F‚ÇÅ)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 5: New residuals\n",
        "ax = axes[1, 1]\n",
        "residuals1 = y - F1\n",
        "ax.scatter(X, residuals1, alpha=0.5, color='red', label='New residuals')\n",
        "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "ax.set_title('5. New Residuals (Smaller!)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 6: Final ensemble (simulated)\n",
        "ax = axes[1, 2]\n",
        "# Simulate more iterations\n",
        "F_final = F1.copy()\n",
        "for i in range(2, 11):\n",
        "    residuals = y - F_final\n",
        "    # Simple fitting (for visualization)\n",
        "    pred = np.where(X < 5, np.mean(residuals[X < 5]), np.mean(residuals[X >= 5]))\n",
        "    F_final += learning_rate * pred\n",
        "\n",
        "ax.scatter(X, y, alpha=0.3, label='Data')\n",
        "ax.plot(X, y_true, 'r-', linewidth=2, label='True', alpha=0.7)\n",
        "ax.plot(X, F_final, 'g-', linewidth=2, label='F‚ÇÅ‚ÇÄ (Final)')\n",
        "ax.set_title('6. Final Ensemble (10 trees)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Gradient Boosting Intuition:\")\n",
        "print(\"   1. Start with simple prediction (mean)\")\n",
        "print(\"   2. Fit tree to errors (residuals)\")\n",
        "print(\"   3. Add tree to ensemble (with learning rate)\")\n",
        "print(\"   4. Repeat until convergence\")\n",
        "print(\"   5. Final model = sum of all trees\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"install\"></a>\n",
        "# ‚öôÔ∏è **Chapter 5: Installation & Setup**\n",
        "\n",
        "## **Installation Options**\n",
        "\n",
        "```bash\n",
        "# Standard installation\n",
        "pip install lightgbm\n",
        "\n",
        "# With scikit-learn integration\n",
        "pip install lightgbm scikit-learn\n",
        "\n",
        "# GPU support (CUDA)\n",
        "pip install lightgbm --install-option=--gpu\n",
        "\n",
        "# Conda installation\n",
        "conda install -c conda-forge lightgbm\n",
        "\n",
        "# Build from source (latest features)\n",
        "git clone https://github.com/microsoft/LightGBM.git\n",
        "cd LightGBM\n",
        "mkdir build && cd build\n",
        "cmake ..\n",
        "make -j4\n",
        "```\n",
        "\n",
        "## **Quick Verification**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚öôÔ∏è CHAPTER 5: INSTALLATION VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    print(f\"\\n‚úÖ LightGBM is installed!\")\n",
        "    print(f\"   Version: {lgb.__version__}\")\n",
        "    \n",
        "    # Check GPU support\n",
        "    try:\n",
        "        # Try to create a small dataset and check GPU\n",
        "        X_check = np.random.randn(100, 10)\n",
        "        y_check = np.random.randint(0, 2, 100)\n",
        "        \n",
        "        # Try GPU\n",
        "        model_gpu = LGBMClassifier(device='gpu', verbose=-1)\n",
        "        model_gpu.fit(X_check, y_check)\n",
        "        print(f\"   GPU Support: ‚úÖ Available\")\n",
        "    except Exception as e:\n",
        "        print(f\"   GPU Support: ‚ùå Not available (CPU only)\")\n",
        "        print(f\"   To enable GPU: pip install lightgbm --install-option=--gpu\")\n",
        "    \n",
        "    # Check threading\n",
        "    print(f\"\\nüîß System Configuration:\")\n",
        "    print(f\"   Threads available: {lgb.basic._ConfigBeforeLoop().num_threads}\")\n",
        "    \n",
        "    # Feature checklist\n",
        "    features = {\n",
        "        'Scikit-learn API': True,\n",
        "        'Native categorical': True,\n",
        "        'Early stopping': True,\n",
        "        'Custom objectives': True,\n",
        "        'GPU training': 'Check above',\n",
        "        'Distributed training': True\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìã Feature Checklist:\")\n",
        "    for feature, status in features.items():\n",
        "        symbol = \"‚úÖ\" if status == True else \"‚ö†Ô∏è\"\n",
        "        print(f\"   {symbol} {feature}: {status if status != True else 'Available'}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå LightGBM not installed!\")\n",
        "    print(\"   Install with: pip install lightgbm\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"basic\"></a>\n",
        "# üöÄ **Chapter 6: Basic Usage**\n",
        "\n",
        "## **Two APIs: Scikit-learn vs Native**\n",
        "\n",
        "| Feature | Scikit-learn API | Native API |\n",
        "|---------|-----------------|------------|\n",
        "| **Syntax** | Familiar (fit/predict) | Dataset + train |\n",
        "| **Best for** | Quick prototyping | Production, tuning |\n",
        "| **Early stopping** | ‚úÖ Yes | ‚úÖ Yes |\n",
        "| **Callbacks** | Limited | Full control |\n",
        "| **Learning curves** | Manual | Built-in |\n",
        "\n",
        "## **Quick Start Template**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ CHAPTER 6: BASIC USAGE - COMPLETE EXAMPLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # Load data\n",
        "    data = load_breast_cancer()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset: Breast Cancer ({X.shape[0]} samples, {X.shape[1]} features)\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # METHOD 1: Scikit-learn API (Recommended for beginners)\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"METHOD 1: Scikit-learn API\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Basic model\n",
        "    model_sk = LGBMClassifier(\n",
        "        objective='binary',\n",
        "        boosting_type='gbdt',\n",
        "        num_leaves=31,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    model_sk.fit(X_train, y_train)\n",
        "    y_pred = model_sk.predict(X_test)\n",
        "    y_prob = model_sk.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"‚úÖ AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # METHOD 2: Native API (More control)\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"METHOD 2: Native API\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "    \n",
        "    # Parameters\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Train with early stopping\n",
        "    model_native = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    y_pred_native = (model_native.predict(X_test, num_iteration=model_native.best_iteration) > 0.5).astype(int)\n",
        "    y_prob_native = model_native.predict(X_test, num_iteration=model_native.best_iteration)\n",
        "    \n",
        "    print(f\"‚úÖ Best iteration: {model_native.best_iteration}\")\n",
        "    print(f\"‚úÖ Best score: {model_native.best_score['valid']['auc']:.4f}\")\n",
        "    print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred_native):.4f}\")\n",
        "    print(f\"‚úÖ AUC: {roc_auc_score(y_test, y_prob_native):.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # METHOD 3: Cross-validation\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"METHOD 3: Cross-Validation\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        nfold=5,\n",
        "        stratified=True,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ CV AUC: {cv_results['valid auc-mean'][-1]:.4f} (+/- {cv_results['valid auc-stdv'][-1]:.4f})\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # Feature Importance\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"FEATURE IMPORTANCE\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Split importance\n",
        "    lgb.plot_importance(model_native, max_num_features=10, ax=axes[0], \n",
        "                       title='Feature Importance (Split)', importance_type='split')\n",
        "    \n",
        "    # Gain importance\n",
        "    lgb.plot_importance(model_native, max_num_features=10, ax=axes[1], \n",
        "                       title='Feature Importance (Gain)', importance_type='gain')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüí° Importance Types:\")\n",
        "    print(\"   ‚Ä¢ Split: Number of times feature is used\")\n",
        "    print(\"   ‚Ä¢ Gain: Total contribution of feature to model\")\n",
        "    print(\"   ‚Ä¢ Use Gain for feature selection, Split for understanding usage\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"params\"></a>\n",
        "# üéõÔ∏è **Chapter 7: Parameter Tuning**\n",
        "\n",
        "## **The Golden Rules**\n",
        "\n",
        "```\n",
        "üéØ TUNING STRATEGY\n",
        "\n",
        "Step 1: Fix overfitting first\n",
        "        ‚îî‚îÄ‚îÄ Use large learning_rate (0.1) and tune num_leaves\n",
        "        \n",
        "Step 2: Optimize accuracy\n",
        "        ‚îî‚îÄ‚îÄ Lower learning_rate, increase n_estimators\n",
        "        \n",
        "Step 3: Regularize\n",
        "        ‚îî‚îÄ‚îÄ Add feature_fraction, bagging_fraction\n",
        "        \n",
        "Step 4: Fine-tune\n",
        "        ‚îî‚îÄ‚îÄ Adjust min_child_samples, reg_lambda\n",
        "```\n",
        "\n",
        "## **Parameter Reference Table**\n",
        "\n",
        "| Parameter | Description | Default | Typical Range | When to Tune |\n",
        "|-----------|-------------|---------|---------------|--------------|\n",
        "| `num_leaves` | Max leaves per tree | 31 | 20-150 | **Always** - main complexity control |\n",
        "| `learning_rate` | Shrinkage rate | 0.1 | 0.01-0.3 | **Always** - trade speed vs accuracy |\n",
        "| `n_estimators` | Number of trees | 100 | 100-10000 | Use early stopping instead |\n",
        "| `max_depth` | Tree depth limit | -1 | -1, 5-10 | If overfitting with num_leaves |\n",
        "| `min_child_samples` | Min data in leaf | 20 | 5-100 | **Overfitting** |\n",
        "| `feature_fraction` | Column sampling | 1.0 | 0.6-1.0 | **Overfitting, speed** |\n",
        "| `bagging_fraction` | Row sampling | 1.0 | 0.6-1.0 | **Overfitting** |\n",
        "| `bagging_freq` | Bagging every N iter | 0 | 1-10 | Enable with bagging_fraction |\n",
        "| `reg_alpha` | L1 regularization | 0.0 | 0-1 | **Feature selection** |\n",
        "| `reg_lambda` | L2 regularization | 0.0 | 0-1 | **Overfitting** |\n",
        "| `min_split_gain` | Min loss reduction | 0.0 | 0-0.1 | **Conservative splits** |\n",
        "| `cat_smooth` | Cat feature smoothing | 10 | 5-50 | **Categorical features** |\n",
        "\n",
        "## **Overfitting vs Underfitting Guide**\n",
        "\n",
        "| Symptom | Diagnosis | Solution |\n",
        "|---------|-----------|----------|\n",
        "| Train AUC >> Valid AUC | Overfitting | ‚Üì `num_leaves`, ‚Üë `min_child_samples`, ‚Üì `max_depth` |\n",
        "| Both AUCs low | Underfitting | ‚Üë `num_leaves`, ‚Üì `min_child_samples`, ‚Üë `n_estimators` |\n",
        "| Slow training | Too complex | ‚Üì `num_leaves`, enable `feature_fraction` |\n",
        "| Unstable CV | High variance | ‚Üë `bagging_fraction`, ‚Üë `bagging_freq` |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üéõÔ∏è CHAPTER 7: PARAMETER TUNING DEMONSTRATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # Generate more complex data for tuning demonstration\n",
        "    X, y = make_classification(n_samples=5000, n_features=20, n_informative=10, \n",
        "                               n_redundant=5, flip_y=0.05, class_sep=0.5, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset: {X.shape[0]} samples (noisier for tuning demo)\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # DEMONSTRATION: Effect of num_leaves\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"EFFECT OF num_leaves (Model Complexity)\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    leaf_values = [5, 15, 31, 50, 100]\n",
        "    results_leaves = []\n",
        "    \n",
        "    for num_leaves in leaf_values:\n",
        "        model = LGBMClassifier(\n",
        "            num_leaves=num_leaves,\n",
        "            learning_rate=0.1,\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
        "        test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "        \n",
        "        results_leaves.append({\n",
        "            'num_leaves': num_leaves,\n",
        "            'train_auc': train_auc,\n",
        "            'test_auc': test_auc,\n",
        "            'gap': train_auc - test_auc\n",
        "        })\n",
        "        \n",
        "        print(f\"num_leaves={num_leaves:3d}: Train={train_auc:.4f}, Test={test_auc:.4f}, Gap={train_auc-test_auc:.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # DEMONSTRATION: Effect of learning_rate + n_estimators\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"EFFECT OF learning_rate (with early stopping)\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    lr_values = [0.01, 0.05, 0.1, 0.3]\n",
        "    results_lr = []\n",
        "    \n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "    \n",
        "    for lr in lr_values:\n",
        "        params = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'auc',\n",
        "            'learning_rate': lr,\n",
        "            'num_leaves': 31,\n",
        "            'verbose': -1\n",
        "        }\n",
        "        \n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            num_boost_round=1000,\n",
        "            valid_sets=[valid_data],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(0)]\n",
        "        )\n",
        "        \n",
        "        best_iter = model.best_iteration\n",
        "        best_score = model.best_score['valid_0']['auc']\n",
        "        \n",
        "        results_lr.append({\n",
        "            'learning_rate': lr,\n",
        "            'best_iteration': best_iter,\n",
        "            'best_auc': best_score\n",
        "        })\n",
        "        \n",
        "        print(f\"learning_rate={lr:.2f}: Best iter={best_iter:4d}, AUC={best_score:.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # Visualization\n",
        "    # ============================================================\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot 1: num_leaves effect\n",
        "    df_leaves = pd.DataFrame(results_leaves)\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(df_leaves['num_leaves'], df_leaves['train_auc'], 'o-', label='Train', linewidth=2)\n",
        "    ax1.plot(df_leaves['num_leaves'], df_leaves['test_auc'], 's-', label='Test', linewidth=2)\n",
        "    ax1.fill_between(df_leaves['num_leaves'], df_leaves['test_auc'], df_leaves['train_auc'], \n",
        "                     alpha=0.3, label='Overfitting gap')\n",
        "    ax1.set_xlabel('num_leaves')\n",
        "    ax1.set_ylabel('ROC-AUC')\n",
        "    ax1.set_title('Effect of num_leaves\\n(Higher = more complex)', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: learning rate effect\n",
        "    df_lr = pd.DataFrame(results_lr)\n",
        "    ax2 = axes[1]\n",
        "    ax2.bar(range(len(df_lr)), df_lr['best_auc'], color=['green' if x > 0.8 else 'orange' for x in df_lr['best_auc']])\n",
        "    ax2.set_xticks(range(len(df_lr)))\n",
        "    ax2.set_xticklabels([f\"{lr:.2f}\" for lr in df_lr['learning_rate']])\n",
        "    ax2.set_xlabel('learning_rate')\n",
        "    ax2.set_ylabel('Best ROC-AUC')\n",
        "    ax2.set_title('Effect of learning_rate\\n(Lower = more trees needed)', fontweight='bold')\n",
        "    \n",
        "    # Add iteration labels\n",
        "    for i, (auc, iters) in enumerate(zip(df_lr['best_auc'], df_lr['best_iteration'])):\n",
        "        ax2.text(i, auc + 0.005, f'{iters} trees', ha='center', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüí° Tuning Insights:\")\n",
        "    print(\"   ‚Ä¢ num_leaves: Sweet spot around 20-50 for most datasets\")\n",
        "    print(\"   ‚Ä¢ learning_rate: 0.05-0.1 is good starting point\")\n",
        "    print(\"   ‚Ä¢ Always use early stopping instead of fixing n_estimators!\")\n",
        "    print(\"   ‚Ä¢ Watch the train-test gap to detect overfitting\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"advanced\"></a>\n",
        "# üß† **Chapter 8: Advanced Features**\n",
        "\n",
        "## **1. Categorical Features (Native Support)**\n",
        "\n",
        "```python\n",
        "# Since LightGBM 3.0, native categorical support!\n",
        "\n",
        "# Method 1: Pass as pandas category\n",
        "df['category_col'] = df['category_col'].astype('category')\n",
        "\n",
        "# Method 2: Specify in parameters\n",
        "cat_features = ['cat_col1', 'cat_col2']\n",
        "model.fit(X, y, categorical_feature=cat_features)\n",
        "```\n",
        "\n",
        "## **2. Custom Objective Functions**\n",
        "\n",
        "```python\n",
        "# Example: Focal Loss for imbalanced data\n",
        "def focal_loss(y_pred, dtrain, alpha=0.25, gamma=2.0):\n",
        "    y_true = dtrain.get_label()\n",
        "    p = 1 / (1 + np.exp(-y_pred))\n",
        "    grad = alpha * y_true * (1-p)**gamma * (gamma*p*np.log(p)+p-1) + \\\n",
        "           (1-alpha) * (1-y_true) * p**gamma * (-gamma*(1-p)*np.log(1-p)+1-p)\n",
        "    hess = alpha * y_true * (1-p)**gamma * (1+gamma*p/(1-p)) + \\\n",
        "           (1-alpha) * (1-y_true) * p**gamma * (1+gamma*(1-p)/p)\n",
        "    return grad, hess\n",
        "```\n",
        "\n",
        "## **3. Monotonic Constraints**\n",
        "\n",
        "```python\n",
        "# Force feature to have monotonic relationship with target\n",
        "# 1: increasing, -1: decreasing, 0: no constraint\n",
        "\n",
        "monotonic_constraints = [1, -1, 0, 0, 1]  # One per feature\n",
        "model = LGBMClassifier(monotone_constraints=monotonic_constraints)\n",
        "```\n",
        "\n",
        "## **4. Advanced Callbacks**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üß† CHAPTER 8: ADVANCED FEATURES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # ============================================================\n",
        "    # ADVANCED 1: Custom Metric\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"CUSTOM METRIC: F1-Score\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    def f1_metric(y_pred, dtrain):\n",
        "        \"\"\"Custom F1 metric for LightGBM\"\"\"\n",
        "        y_true = dtrain.get_label()\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "        \n",
        "        tp = np.sum((y_pred_binary == 1) & (y_true == 1))\n",
        "        fp = np.sum((y_pred_binary == 1) & (y_true == 0))\n",
        "        fn = np.sum((y_pred_binary == 0) & (y_true == 1))\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        return 'f1', f1, True  # name, value, higher_is_better\n",
        "    \n",
        "    # Train with custom metric\n",
        "    X, y = make_classification(n_samples=2000, weights=[0.9, 0.1], random_state=42)  # Imbalanced\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "    \n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[valid_data],\n",
        "        feval=f1_metric,\n",
        "        callbacks=[lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Trained with custom F1 metric\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # ADVANCED 2: Monotonic Constraints\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"MONOTONIC CONSTRAINTS\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Create synthetic data where we know the relationship\n",
        "    np.random.seed(42)\n",
        "    n = 1000\n",
        "    \n",
        "    # Feature 1: Should increase with target (positive correlation)\n",
        "    x1 = np.random.randn(n)\n",
        "    # Feature 2: Should decrease with target (negative correlation)\n",
        "    x2 = np.random.randn(n)\n",
        "    # Noise feature\n",
        "    x3 = np.random.randn(n)\n",
        "    \n",
        "    y = (x1 - x2 + 0.1 * x3 + np.random.randn(n) * 0.1) > 0\n",
        "    \n",
        "    X = np.column_stack([x1, x2, x3])\n",
        "    \n",
        "    # Without constraints\n",
        "    model_no_constraint = LGBMClassifier(n_estimators=50, random_state=42, verbose=-1)\n",
        "    model_no_constraint.fit(X, y)\n",
        "    \n",
        "    # With constraints: x1 increasing (+1), x2 decreasing (-1), x3 free (0)\n",
        "    model_constrained = LGBMClassifier(\n",
        "        n_estimators=50, \n",
        "        monotone_constraints=[1, -1, 0],\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "    model_constrained.fit(X, y)\n",
        "    \n",
        "    print(f\"‚úÖ Trained with monotonic constraints [1, -1, 0]\")\n",
        "    print(f\"   Feature 1: Forced increasing\")\n",
        "    print(f\"   Feature 2: Forced decreasing\")\n",
        "    print(f\"   Feature 3: No constraint\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # ADVANCED 3: Feature Name and Categorical Handling\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"FEATURE NAMES & CATEGORICAL HANDLING\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Create DataFrame with named features\n",
        "    df = pd.DataFrame(X, columns=['income', 'age', 'noise'])\n",
        "    df['income_category'] = pd.cut(df['income'], bins=3, labels=['low', 'medium', 'high'])\n",
        "    df['income_category'] = df['income_category'].astype('category')\n",
        "    \n",
        "    print(f\"Feature types:\")\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    # LightGBM can handle pandas category directly\n",
        "    X_advanced = df.drop('noise', axis=1)  # Keep categorical\n",
        "    \n",
        "    print(f\"\\n‚úÖ LightGBM automatically detects categorical features in pandas\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # ADVANCED 4: Learning Curve Visualization\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"LEARNING CURVES\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Train with evaluation history\n",
        "    evals_result = {}\n",
        "    \n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[lgb.record_evaluation(evals_result), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    # Plot learning curves\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    ax.plot(evals_result['train']['binary_logloss'], label='Train Loss', linewidth=2)\n",
        "    ax.plot(evals_result['valid']['binary_logloss'], label='Valid Loss', linewidth=2)\n",
        "    ax.axvline(x=model.best_iteration, color='red', linestyle='--', \n",
        "               label=f'Best iteration ({model.best_iteration})')\n",
        "    ax.set_xlabel('Iteration')\n",
        "    ax.set_ylabel('Binary Logloss')\n",
        "    ax.set_title('Learning Curves\\n(Detect overfitting)', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüí° Learning curve analysis:\")\n",
        "    print(f\"   ‚Ä¢ Train ‚Üì Valid ‚Üë = Overfitting (stop earlier)\")\n",
        "    print(f\"   ‚Ä¢ Both plateau = Converged (reduce learning_rate)\")\n",
        "    print(f\"   ‚Ä¢ Both still ‚Üì = Underfitting (more iterations)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"production\"></a>\n",
        "# üè≠ **Chapter 9: Production Deployment**\n",
        "\n",
        "## **Model Persistence**\n",
        "\n",
        "```python\n",
        "# Save model\n",
        "model.save_model('model.txt')  # Text format (human-readable)\n",
        "model.save_model('model.pkl')  # Pickle format\n",
        "\n",
        "# Load model\n",
        "model = lgb.Booster(model_file='model.txt')\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(new_data)\n",
        "```\n",
        "\n",
        "## **Optimization for Production**\n",
        "\n",
        "| Scenario | Optimization | Code |\n",
        "|----------|-------------|------|\n",
        "| **Low latency** | Reduce trees | `model = lgb.Booster(model_file='model.txt').trim(50)` |\n",
        "| **Batch prediction** | Use data parallel | `model.predict(X, num_threads=8)` |\n",
        "| **Memory constrained** | Quantize model | `model = lgb.Booster(model_file='model.txt', is_sparse=True)` |\n",
        "| **GPU inference** | Move to GPU | `model = lgb.Booster(model_file='model.txt', device='gpu')` |\n",
        "\n",
        "## **Model Interpretability**\n",
        "\n",
        "```python\n",
        "# SHAP values (recommended)\n",
        "import shap\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "shap.summary_plot(shap_values, X)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üè≠ CHAPTER 9: PRODUCTION DEPLOYMENT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # Train a model to save\n",
        "    X, y = make_classification(n_samples=1000, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    model = LGBMClassifier(n_estimators=50, random_state=42, verbose=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Save and load demonstration\n",
        "    print(\"\\nüíæ Model Persistence:\")\n",
        "    \n",
        "    # Method 1: Native LightGBM format (recommended)\n",
        "    model.booster_.save_model('lightgbm_model.txt')\n",
        "    print(\"‚úÖ Saved to 'lightgbm_model.txt' (cross-platform, readable)\")\n",
        "    \n",
        "    # Method 2: Pickle\n",
        "    import joblib\n",
        "    joblib.dump(model, 'lightgbm_model.pkl')\n",
        "    print(\"‚úÖ Saved to 'lightgbm_model.pkl' (Python-specific)\")\n",
        "    \n",
        "    # Load and verify\n",
        "    loaded_model = lgb.Booster(model_file='lightgbm_model.txt')\n",
        "    predictions = loaded_model.predict(X_test)\n",
        "    print(f\"‚úÖ Loaded model predictions match: {np.allclose(predictions, model.predict_proba(X_test)[:, 1])}\")\n",
        "    \n",
        "    # Cleanup\n",
        "    import os\n",
        "    os.remove('lightgbm_model.txt')\n",
        "    os.remove('lightgbm_model.pkl')\n",
        "    \n",
        "    # ============================================================\n",
        "    # Production Checklist\n",
        "    # ============================================================\n",
        "    print(\"\\nüìã PRODUCTION DEPLOYMENT CHECKLIST:\")\n",
        "    print(\"   ‚ñ° Model versioned and tracked (MLflow, DVC)\")\n",
        "    print(\"   ‚ñ° Input validation pipeline implemented\")\n",
        "    print(\"   ‚ñ° Feature engineering pipeline matches training\")\n",
        "    print(\"   ‚ñ° Fallback strategy for missing features\")\n",
        "    print(\"   ‚ñ° Monitoring for prediction drift\")\n",
        "    print(\"   ‚ñ° A/B testing framework ready\")\n",
        "    print(\"   ‚ñ° Latency requirements met (<100ms?)\")\n",
        "    print(\"   ‚ñ° Memory usage optimized\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"pitfalls\"></a>\n",
        "# ‚ö†Ô∏è **Chapter 10: Common Pitfalls**\n",
        "\n",
        "## **Top 10 Mistakes**\n",
        "\n",
        "| # | Mistake | Why It's Bad | Solution |\n",
        "|---|---------|--------------|----------|\n",
        "| 1 | **Not using early stopping** | Wastes computation, overfits | Always use `early_stopping_rounds` |\n",
        "| 2 | **Tuning `n_estimators`** | Wrong approach | Tune `num_leaves`, use early stopping |\n",
        "| 3 | **Ignoring categorical features** | Misses native optimization | Use `astype('category')` |\n",
        "| 4 | **Default parameters for all data** | Suboptimal performance | Always tune `num_leaves`, `learning_rate` |\n",
        "| 5 | **Not handling imbalanced data** | Biased predictions | Use `is_unbalance` or `scale_pos_weight` |\n",
        "| 6 | **Small `num_leaves` on large data** | Underfitting | Increase to 50-150 for large datasets |\n",
        "| 7 | **Forgetting random seed** | Non-reproducible results | Always set `random_state` |\n",
        "| 8 | **Not validating on time-split** | Data leakage in time series | Use time-based validation |\n",
        "| 9 | **High `learning_rate` without enough trees** | Poor convergence | Lower rate, increase iterations |\n",
        "| 10 | **Ignoring feature importance** | Missing insights | Always check `plot_importance` |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚ö†Ô∏è CHAPTER 10: COMMON PITFALLS - DEMONSTRATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    # Pitfall 1: Not using early stopping\n",
        "    print(\"\\n‚ùå PITFALL 1: Fixed n_estimators without early stopping\")\n",
        "    \n",
        "    X, y = make_classification(n_samples=2000, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Wrong way\n",
        "    model_wrong = LGBMClassifier(n_estimators=1000, random_state=42, verbose=-1)\n",
        "    model_wrong.fit(X_train, y_train)\n",
        "    pred_wrong = model_wrong.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Right way\n",
        "    model_right = LGBMClassifier(n_estimators=1000, random_state=42, verbose=-1)\n",
        "    model_right.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    pred_right = model_right.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"   Wrong (1000 trees):  AUC = {roc_auc_score(y_test, pred_wrong):.4f}, Trees = 1000\")\n",
        "    print(f\"   Right (early stop):  AUC = {roc_auc_score(y_test, pred_right):.4f}, Trees = {model_right.best_iteration_}\")\n",
        "    print(f\"   üí° Early stopping saved {1000 - model_right.best_iteration_} trees with same/better performance!\")\n",
        "    \n",
        "    # Pitfall 2: Ignoring imbalance\n",
        "    print(\"\\n‚ùå PITFALL 2: Ignoring class imbalance\")\n",
        "    \n",
        "    X_imb, y_imb = make_classification(n_samples=5000, weights=[0.95, 0.05], random_state=42)\n",
        "    X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_imb, y_imb, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Without balancing\n",
        "    model_no_balance = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
        "    model_no_balance.fit(X_train_i, y_train_i)\n",
        "    pred_no_balance = model_no_balance.predict(X_test_i)\n",
        "    \n",
        "    # With balancing\n",
        "    model_balanced = LGBMClassifier(n_estimators=100, is_unbalance=True, random_state=42, verbose=-1)\n",
        "    model_balanced.fit(X_train_i, y_train_i)\n",
        "    pred_balanced = model_balanced.predict(X_test_i)\n",
        "    \n",
        "    from sklearn.metrics import f1_score\n",
        "    print(f\"   Without balancing: F1 = {f1_score(y_test_i, pred_no_balance):.4f}\")\n",
        "    print(f\"   With is_unbalance: F1 = {f1_score(y_test_i, pred_balanced):.4f}\")\n",
        "    print(f\"   üí° Balancing improved F1 significantly!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì **Final Summary & Cheat Sheet**\n",
        "\n",
        "## **Quick Start Template**\n",
        "\n",
        "```python\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Prepare data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# 2. Create datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# 3. Parameters (start here!)\n",
        "params = {\n",
        "    'objective': 'binary',        # or 'multiclass', 'regression'\n",
        "    'metric': 'auc',              # or 'multi_logloss', 'rmse'\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,             # [20, 50] for small, [50, 150] for large data\n",
        "    'learning_rate': 0.05,        # [0.01, 0.1] typical range\n",
        "    'feature_fraction': 0.9,      # Column sampling\n",
        "    'bagging_fraction': 0.8,      # Row sampling\n",
        "    'bagging_freq': 5,            # Sample every 5 iterations\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# 4. Train with early stopping\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=10000,        # Large number, early stopping will find best\n",
        "    valid_sets=[valid_data],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=100)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 5. Predict\n",
        "predictions = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "```\n",
        "\n",
        "## **Parameter Quick Reference**\n",
        "\n",
        "| Goal | Key Parameters |\n",
        "|------|---------------|\n",
        "| **Fast prototype** | `num_leaves=31`, `learning_rate=0.1`, `n_estimators=100` |\n",
        "| **Best accuracy** | `num_leaves=50-100`, `learning_rate=0.01-0.05`, early stopping |\n",
        "| **Prevent overfit** | `min_child_samples=20`, `reg_lambda=1.0`, `feature_fraction=0.8` |\n",
        "| **Speed** | `num_leaves=20`, `feature_fraction=0.6`, `bagging_fraction=0.6` |\n",
        "| **Imbalanced** | `is_unbalance=True` or `scale_pos_weight` |\n",
        "| **Small data (<1K)** | Use XGBoost or Random Forest instead! |\n",
        "\n",
        "## **Remember**\n",
        "\n",
        "```\n",
        "üå≤ LightGBM = Speed + Accuracy\n",
        "\n",
        "1. Always use early stopping\n",
        "2. Tune num_leaves first, then learning_rate\n",
        "3. Use native categorical support\n",
        "4. Monitor train/valid gap for overfitting\n",
        "5. Set random_state for reproducibility\n",
        "```\n",
        "\n",
        "**Happy Boosting! üöÄ**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üéì FINAL SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary = \"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                     LIGHTGBM MASTERY COMPLETE                       ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë  YOU NOW KNOW:                                                       ‚ïë\n",
        "‚ïë  ‚úÖ What LightGBM is (Microsoft's fast gradient boosting)           ‚ïë\n",
        "‚ïë  ‚úÖ Why use it (speed, accuracy, efficiency)                        ‚ïë\n",
        "‚ïë  ‚úÖ When to use it (tabular data, medium-large datasets)            ‚ïë\n",
        "‚ïë  ‚úÖ Core concepts (leaf-wise growth, GOSS, EFB)                     ‚ïë\n",
        "‚ïë  ‚úÖ How to tune (num_leaves, learning_rate, early stopping)         ‚ïë\n",
        "‚ïë  ‚úÖ Advanced features (custom metrics, monotonic constraints)       ‚ïë\n",
        "‚ïë  ‚úÖ Production deployment (save/load, optimize)                     ‚ïë\n",
        "‚ïë  ‚úÖ Common pitfalls (avoided the top 10 mistakes)                   ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë  NEXT STEPS:                                                         ‚ïë\n",
        "‚ïë  1. Practice on Kaggle competitions                                  ‚ïë\n",
        "‚ïë  2. Read LightGBM documentation for edge cases                       ‚ïë\n",
        "‚ïë  3. Experiment with custom objectives                                ‚ïë\n",
        "‚ïë  4. Try GPU training for large datasets                              ‚ïë\n",
        "‚ïë  5. Combine with XGBoost/Neural Nets in ensembles                    ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë  RESOURCES:                                                          ‚ïë\n",
        "‚ïë  ‚Ä¢ Docs: https://lightgbm.readthedocs.io/                            ‚ïë\n",
        "‚ïë  ‚Ä¢ GitHub: https://github.com/microsoft/LightGBM                     ‚ïë\n",
        "‚ïë  ‚Ä¢ Paper: \"LightGBM: A Highly Efficient Gradient Boosting Decision   ‚ïë\n",
        "‚ïë           Tree\" (NIPS 2017)                                          ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "if LGBM_AVAILABLE:\n",
        "    print(f\"\\n‚úÖ LightGBM {lgb.__version__} is ready to use!\")\n",
        "    print(\"   Start building your first model with the quick start template above.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Install LightGBM: pip install lightgbm\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
