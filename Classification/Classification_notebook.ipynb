{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üö¢ Binary Classification Masterclass: Predicting Titanic Survival\n",
        "\n",
        "## A Complete Guide for Beginners\n",
        "\n",
        "**Welcome!** This notebook will take you from zero to hero in binary classification. We'll use the famous Titanic dataset to predict whether a passenger survived or not.\n",
        "\n",
        "### üìö What You'll Learn:\n",
        "1. **Data Exploration** - Understanding your dataset\n",
        "2. **Statistical Testing** - Verifying ML assumptions\n",
        "3. **Data Preprocessing** - Cleaning and preparing data\n",
        "4. **Multiple Models** - Logistic Regression, Random Forest, SVM, XGBoost\n",
        "5. **Model Evaluation** - Accuracy, Precision, Recall, F1, ROC-AUC\n",
        "6. **Model Improvement** - Hyperparameter tuning and feature engineering\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, ttest_ind, shapiro, levene\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                             accuracy_score, precision_score, recall_score, \n",
        "                             f1_score, roc_auc_score, roc_curve, \n",
        "                             precision_recall_curve)\n",
        "\n",
        "# For advanced models (install if needed: pip install xgboost)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Data Loading and Initial Exploration\n",
        "\n",
        "The Titanic dataset is built into Seaborn. It contains information about passengers including:\n",
        "- **survived**: 0 = No, 1 = Yes (our target variable)\n",
        "- **pclass**: Ticket class (1st, 2nd, 3rd)\n",
        "- **sex**: Male/Female\n",
        "- **age**: Age in years\n",
        "- **sibsp**: Number of siblings/spouses aboard\n",
        "- **parch**: Number of parents/children aboard\n",
        "- **fare**: Passenger fare\n",
        "- **embarked**: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
        "- **class**: Same as pclass but categorical\n",
        "- **who**: man/woman/child\n",
        "- **adult_male**: True/False\n",
        "- **deck**: Deck level (many missing values)\n",
        "- **embark_town**: Full name of embarkation port\n",
        "- **alive**: yes/no (same as survived)\n",
        "- **alone**: True/False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö¢ Load the Titanic dataset from Seaborn\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FIRST 5 ROWS:\")\n",
        "print(\"=\"*50)\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET INFO:\")\n",
        "print(\"=\"*50)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Basic statistics\n",
        "print(\"=\"*50)\n",
        "print(\"STATISTICAL SUMMARY:\")\n",
        "print(\"=\"*50)\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MISSING VALUES:\")\n",
        "print(\"=\"*50)\n",
        "missing = df.isnull().sum()\n",
        "missing_percent = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Count': missing, 'Percentage': missing_percent})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Percentage', ascending=False)\n",
        "display(missing_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION:\")\n",
        "print(\"=\"*50)\n",
        "survival_rate = df['survived'].value_counts(normalize=True) * 100\n",
        "print(f\"Did not survive (0): {survival_rate[0]:.2f}%\")\n",
        "print(f\"Survived (1): {survival_rate[1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's visualize the data to understand patterns before building models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Create comprehensive visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Survival distribution\n",
        "ax1 = axes[0, 0]\n",
        "survival_counts = df['survived'].value_counts()\n",
        "colors = ['#ff6b6b', '#4ecdc4']\n",
        "ax1.pie(survival_counts, labels=['Did not survive', 'Survived'], \n",
        "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "ax1.set_title('Survival Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 2. Survival by Gender\n",
        "ax2 = axes[0, 1]\n",
        "sns.countplot(data=df, x='sex', hue='survived', ax=ax2, palette=colors)\n",
        "ax2.set_title('Survival by Gender', fontsize=14, fontweight='bold')\n",
        "ax2.legend(['Did not survive', 'Survived'])\n",
        "\n",
        "# 3. Survival by Class\n",
        "ax3 = axes[0, 2]\n",
        "sns.countplot(data=df, x='pclass', hue='survived', ax=ax3, palette=colors)\n",
        "ax3.set_title('Survival by Passenger Class', fontsize=14, fontweight='bold')\n",
        "ax3.legend(['Did not survive', 'Survived'])\n",
        "\n",
        "# 4. Age distribution by survival\n",
        "ax4 = axes[1, 0]\n",
        "sns.histplot(data=df, x='age', hue='survived', bins=30, kde=True, ax=ax4, palette=colors)\n",
        "ax4.set_title('Age Distribution by Survival', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 5. Fare distribution by survival\n",
        "ax5 = axes[1, 1]\n",
        "sns.boxplot(data=df, x='survived', y='fare', ax=ax5, palette=colors)\n",
        "ax5.set_title('Fare Distribution by Survival', fontsize=14, fontweight='bold')\n",
        "ax5.set_xticklabels(['Did not survive', 'Survived'])\n",
        "\n",
        "# 6. Correlation heatmap\n",
        "ax6 = axes[1, 2]\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax6)\n",
        "ax6.set_title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# üìà Print key insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîç KEY INSIGHTS FROM EDA:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"1. Overall survival rate: {df['survived'].mean():.2%}\")\n",
        "print(f\"2. Female survival rate: {df[df['sex']=='female']['survived'].mean():.2%}\")\n",
        "print(f\"3. Male survival rate: {df[df['sex']=='male']['survived'].mean():.2%}\")\n",
        "print(f\"4. 1st class survival rate: {df[df['pclass']==1]['survived'].mean():.2%}\")\n",
        "print(f\"5. 3rd class survival rate: {df[df['pclass']==3]['survived'].mean():.2%}\")\n",
        "print(f\"6. Average age of survivors: {df[df['survived']==1]['age'].mean():.1f}\")\n",
        "print(f\"7. Average age of non-survivors: {df[df['survived']==0]['age'].mean():.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Statistical Testing: Verifying ML Assumptions üî¨\n",
        "\n",
        "Before applying machine learning algorithms, we should verify statistical assumptions. This is crucial for understanding whether our data is suitable for classification.\n",
        "\n",
        "### Key Tests We'll Perform:\n",
        "\n",
        "1. **Chi-Square Test** - Tests independence between categorical variables and survival\n",
        "   - *Null Hypothesis (H0)*: The categorical variable is independent of survival\n",
        "   - *Alternative Hypothesis (H1)*: The categorical variable is associated with survival\n",
        "\n",
        "2. **T-Test** - Compares means of continuous variables between survived/did not survive groups\n",
        "   - *Null Hypothesis (H0)*: There is no difference in means between groups\n",
        "   - *Alternative Hypothesis (H1)*: There is a significant difference in means\n",
        "\n",
        "3. **Shapiro-Wilk Test** - Tests for normality (important for parametric tests)\n",
        "   - *Null Hypothesis (H0)*: Data is normally distributed\n",
        "   - *Alternative Hypothesis (H1)*: Data is not normally distributed\n",
        "\n",
        "4. **Levene's Test** - Tests for equal variances (homoscedasticity)\n",
        "   - *Null Hypothesis (H0)*: Groups have equal variances\n",
        "   - *Alternative Hypothesis (H1)*: Groups have unequal variances\n",
        "\n",
        "**Significance Level (Œ±)**: 0.05 (5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ STATISTICAL TESTING SECTION\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL TESTS FOR CLASSIFICATION ASSUMPTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare data for testing\n",
        "df_test = df.copy()\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 1: CHI-SQUARE TESTS (Categorical vs Survival)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 1: CHI-SQUARE TESTS (Categorical Variables vs Survival)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "categorical_vars = ['sex', 'pclass', 'embarked', 'who', 'adult_male', 'alone']\n",
        "\n",
        "chi2_results = []\n",
        "\n",
        "for var in categorical_vars:\n",
        "    if var in df_test.columns:\n",
        "        # Create contingency table\n",
        "        contingency_table = pd.crosstab(df_test[var], df_test['survived'])\n",
        "        \n",
        "        # Perform Chi-square test\n",
        "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "        \n",
        "        # Determine significance\n",
        "        significant = \"YES ‚úÖ\" if p_value < 0.05 else \"NO ‚ùå\"\n",
        "        \n",
        "        chi2_results.append({\n",
        "            'Variable': var,\n",
        "            'Chi2': chi2,\n",
        "            'p-value': p_value,\n",
        "            'Significant (Œ±=0.05)': significant\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{var.upper()}:\")\n",
        "        print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
        "        print(f\"  p-value: {p_value:.2e}\")\n",
        "        print(f\"  Degrees of freedom: {dof}\")\n",
        "        print(f\"  Significant association with survival: {significant}\")\n",
        "\n",
        "chi2_df = pd.DataFrame(chi2_results)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"CHI-SQUARE SUMMARY TABLE:\")\n",
        "print(\"-\"*70)\n",
        "display(chi2_df)\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "print(\"   Variables with p < 0.05 are significantly associated with survival\")\n",
        "print(\"   and are good predictors for our classification model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST 2: T-TESTS (Continuous Variables vs Survival Groups)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 2: INDEPENDENT T-TESTS (Continuous Variables by Survival)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Separate groups\n",
        "survived = df_test[df_test['survived'] == 1]\n",
        "not_survived = df_test[df_test['survived'] == 0]\n",
        "\n",
        "continuous_vars = ['age', 'fare']\n",
        "\n",
        "ttest_results = []\n",
        "\n",
        "for var in continuous_vars:\n",
        "    if var in df_test.columns:\n",
        "        # Remove NaN values\n",
        "        group1 = survived[var].dropna()\n",
        "        group2 = not_survived[var].dropna()\n",
        "        \n",
        "        # Perform independent t-test\n",
        "        t_stat, p_value = ttest_ind(group1, group2)\n",
        "        \n",
        "        # Calculate means\n",
        "        mean_survived = group1.mean()\n",
        "        mean_not_survived = group2.mean()\n",
        "        \n",
        "        significant = \"YES ‚úÖ\" if p_value < 0.05 else \"NO ‚ùå\"\n",
        "        \n",
        "        ttest_results.append({\n",
        "            'Variable': var,\n",
        "            'Mean (Survived)': mean_survived,\n",
        "            'Mean (Not Survived)': mean_not_survived,\n",
        "            't-statistic': t_stat,\n",
        "            'p-value': p_value,\n",
        "            'Significant': significant\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{var.upper()}:\")\n",
        "        print(f\"  Mean (Survived): {mean_survived:.2f}\")\n",
        "        print(f\"  Mean (Not Survived): {mean_not_survived:.2f}\")\n",
        "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "        print(f\"  p-value: {p_value:.4f}\")\n",
        "        print(f\"  Significant difference: {significant}\")\n",
        "\n",
        "ttest_df = pd.DataFrame(ttest_results)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"T-TEST SUMMARY TABLE:\")\n",
        "print(\"-\"*70)\n",
        "display(ttest_df)\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "print(\"   Significant p-values indicate the variable differs between\")\n",
        "print(\"   survivors and non-survivors, making it useful for prediction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST 3: NORMALITY TESTS (Shapiro-Wilk)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 3: SHAPIRO-WILK NORMALITY TESTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"H0: Data is normally distributed\")\n",
        "print(\"H1: Data is NOT normally distributed\\n\")\n",
        "\n",
        "# Sample size for Shapiro-Wilk (max 5000 recommended)\n",
        "sample_size = min(5000, len(df_test))\n",
        "\n",
        "normality_results = []\n",
        "\n",
        "for var in continuous_vars:\n",
        "    if var in df_test.columns:\n",
        "        data = df_test[var].dropna()\n",
        "        \n",
        "        # Sample if too large\n",
        "        if len(data) > 5000:\n",
        "            data = data.sample(5000, random_state=42)\n",
        "        \n",
        "        # Shapiro-Wilk test\n",
        "        stat, p_value = shapiro(data)\n",
        "        \n",
        "        normal = \"YES ‚úÖ\" if p_value > 0.05 else \"NO ‚ùå\"\n",
        "        \n",
        "        normality_results.append({\n",
        "            'Variable': var,\n",
        "            'W-statistic': stat,\n",
        "            'p-value': p_value,\n",
        "            'Normal Distribution': normal\n",
        "        })\n",
        "        \n",
        "        print(f\"{var.upper()}:\")\n",
        "        print(f\"  W-statistic: {stat:.4f}\")\n",
        "        print(f\"  p-value: {p_value:.2e}\")\n",
        "        print(f\"  Normally distributed: {normal}\\n\")\n",
        "\n",
        "normality_df = pd.DataFrame(normality_results)\n",
        "print(\"-\"*70)\n",
        "print(\"NORMALITY TEST SUMMARY:\")\n",
        "print(\"-\"*70)\n",
        "display(normality_df)\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "print(\"   If p < 0.05, data is NOT normal. Many ML algorithms (like Logistic\")\n",
        "print(\"   Regression) assume normality, but tree-based methods don't.\")\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 4: LEVENE'S TEST (Equal Variances)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 4: LEVENE'S TEST FOR EQUAL VARIANCES\")\n",
        "print(\"=\"*70)\n",
        "print(\"H0: Groups have equal variances (homoscedasticity)\")\n",
        "print(\"H1: Groups have unequal variances (heteroscedasticity)\\n\")\n",
        "\n",
        "levene_results = []\n",
        "\n",
        "for var in continuous_vars:\n",
        "    if var in df_test.columns:\n",
        "        group1 = survived[var].dropna()\n",
        "        group2 = not_survived[var].dropna()\n",
        "        \n",
        "        # Levene's test\n",
        "        stat, p_value = levene(group1, group2)\n",
        "        \n",
        "        equal_var = \"YES ‚úÖ\" if p_value > 0.05 else \"NO ‚ùå\"\n",
        "        \n",
        "        levene_results.append({\n",
        "            'Variable': var,\n",
        "            'Statistic': stat,\n",
        "            'p-value': p_value,\n",
        "            'Equal Variances': equal_var\n",
        "        })\n",
        "        \n",
        "        print(f\"{var.upper()}:\")\n",
        "        print(f\"  Statistic: {stat:.4f}\")\n",
        "        print(f\"  p-value: {p_value:.4f}\")\n",
        "        print(f\"  Equal variances: {equal_var}\\n\")\n",
        "\n",
        "levene_df = pd.DataFrame(levene_results)\n",
        "print(\"-\"*70)\n",
        "print(\"LEVENE'S TEST SUMMARY:\")\n",
        "print(\"-\"*70)\n",
        "display(levene_df)\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "print(\"   Equal variances are assumed by many statistical tests.\")\n",
        "print(\"   Violations may require data transformation or robust methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Visualize Statistical Test Results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Chi-square results\n",
        "ax1 = axes[0, 0]\n",
        "colors_chi = ['#2ecc71' if 'YES' in str(x) else '#e74c3c' for x in chi2_df['Significant (Œ±=0.05)']]\n",
        "bars1 = ax1.bar(chi2_df['Variable'], chi2_df['Chi2'], color=colors_chi)\n",
        "ax1.set_title('Chi-Square Statistics\\n(Green = Significant Association)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Chi-Square Value')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. T-test results\n",
        "ax2 = axes[0, 1]\n",
        "x_pos = np.arange(len(ttest_df))\n",
        "width = 0.35\n",
        "bars2 = ax2.bar(x_pos - width/2, ttest_df['Mean (Survived)'], width, label='Survived', color='#2ecc71')\n",
        "bars3 = ax2.bar(x_pos + width/2, ttest_df['Mean (Not Survived)'], width, label='Not Survived', color='#e74c3c')\n",
        "ax2.set_title('Mean Comparison by Survival Status', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Mean Value')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(ttest_df['Variable'])\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Distribution plots for normality check\n",
        "ax3 = axes[1, 0]\n",
        "df_test['age'].dropna().hist(bins=30, ax=ax3, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax3.set_title('Age Distribution\\n(Checking Normality)', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('Age')\n",
        "ax3.set_ylabel('Frequency')\n",
        "ax3.axvline(df_test['age'].mean(), color='red', linestyle='--', label=f\"Mean: {df_test['age'].mean():.1f}\")\n",
        "ax3.legend()\n",
        "\n",
        "# 4. Q-Q plot for normality\n",
        "ax4 = axes[1, 1]\n",
        "from scipy.stats import probplot\n",
        "probplot(df_test['fare'].dropna(), dist=\"norm\", plot=ax4)\n",
        "ax4.set_title('Q-Q Plot: Fare vs Normal Distribution', fontsize=12, fontweight='bold')\n",
        "ax4.get_lines()[0].set_markerfacecolor('skyblue')\n",
        "ax4.get_lines()[0].set_markersize(5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ STATISTICAL TESTING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nCONCLUSIONS:\")\n",
        "print(\"1. Categorical variables (sex, pclass, etc.) show significant association with survival\")\n",
        "print(\"2. Continuous variables (fare) show significant mean differences\")\n",
        "print(\"3. Data may not be perfectly normal - consider this for model selection\")\n",
        "print(\"4. Proceed with classification modeling with confidence!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Data Preprocessing üßπ\n",
        "\n",
        "Now we prepare the data for machine learning. This includes:\n",
        "- Handling missing values\n",
        "- Encoding categorical variables\n",
        "- Feature engineering\n",
        "- Scaling features\n",
        "- Splitting into train/test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üßπ DATA PREPROCESSING\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: HANDLING MISSING VALUES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create a copy for processing\n",
        "df_processed = df.copy()\n",
        "\n",
        "print(\"Missing values before processing:\")\n",
        "print(df_processed.isnull().sum()[df_processed.isnull().sum() > 0])\n",
        "\n",
        "# 1. Drop 'deck' column (too many missing values - 77%)\n",
        "df_processed = df_processed.drop('deck', axis=1)\n",
        "\n",
        "# 2. Fill missing 'age' with median (robust to outliers)\n",
        "df_processed['age'].fillna(df_processed['age'].median(), inplace=True)\n",
        "\n",
        "# 3. Fill missing 'embarked' and 'embark_town' with mode (most frequent)\n",
        "df_processed['embarked'].fillna(df_processed['embarked'].mode()[0], inplace=True)\n",
        "df_processed['embark_town'].fillna(df_processed['embark_town'].mode()[0], inplace=True)\n",
        "\n",
        "# 4. Fill missing 'fare' with median\n",
        "df_processed['fare'].fillna(df_processed['fare'].median(), inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after processing:\")\n",
        "print(df_processed.isnull().sum()[df_processed.isnull().sum() > 0])\n",
        "print(\"\\n‚úÖ Missing values handled!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create new features that might improve prediction\n",
        "\n",
        "# 1. Family Size (sibsp + parch + 1)\n",
        "df_processed['family_size'] = df_processed['sibsp'] + df_processed['parch'] + 1\n",
        "\n",
        "# 2. Is Alone (1 if family_size == 1, else 0)\n",
        "df_processed['is_alone'] = (df_processed['family_size'] == 1).astype(int)\n",
        "\n",
        "# 3. Age Group (categorize age)\n",
        "def categorize_age(age):\n",
        "    if age < 13:\n",
        "        return 'Child'\n",
        "    elif age < 20:\n",
        "        return 'Teenager'\n",
        "    elif age < 60:\n",
        "        return 'Adult'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "df_processed['age_group'] = df_processed['age'].apply(categorize_age)\n",
        "\n",
        "# 4. Fare per person\n",
        "df_processed['fare_per_person'] = df_processed['fare'] / df_processed['family_size']\n",
        "\n",
        "# 5. Title extraction from name (if we had name column, we'll use 'who' instead)\n",
        "# Since we don't have 'name', we'll use the 'who' column which is already processed\n",
        "\n",
        "print(\"New features created:\")\n",
        "print(\"  - family_size: Total family members on board\")\n",
        "print(\"  - is_alone: Binary flag for solo travelers\")\n",
        "print(\"  - age_group: Categorical age groups\")\n",
        "print(\"  - fare_per_person: Fare divided by family size\")\n",
        "\n",
        "# Check survival rates by new features\n",
        "print(\"\\nSurvival rate by family size:\")\n",
        "print(df_processed.groupby('family_size')['survived'].mean().round(3))\n",
        "\n",
        "print(\"\\nSurvival rate by age group:\")\n",
        "print(df_processed.groupby('age_group')['survived'].mean().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Select features for modeling\n",
        "# Drop redundant or non-predictive columns\n",
        "columns_to_drop = ['alive', 'class', 'embark_town', 'adult_male']  # Redundant with other columns\n",
        "df_model = df_processed.drop(columns_to_drop, axis=1)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = df_model.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"Categorical columns to encode: {categorical_columns}\")\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "df_encoded = pd.get_dummies(df_model, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "print(f\"\\nDataset shape after encoding: {df_encoded.shape}\")\n",
        "print(f\"Columns: {list(df_encoded.columns)}\")\n",
        "\n",
        "# Display first few rows of processed data\n",
        "print(\"\\nProcessed data sample:\")\n",
        "display(df_encoded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_encoded.drop('survived', axis=1)\n",
        "y = df_encoded['survived']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set survival rate: {y_train.mean():.2%}\")\n",
        "print(f\"Test set survival rate: {y_test.mean():.2%}\")\n",
        "\n",
        "# Feature Scaling (important for Logistic Regression and SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
        "\n",
        "print(\"\\n‚úÖ Data split and scaled successfully!\")\n",
        "print(\"\\nNote: We use StandardScaler to normalize features to mean=0, std=1\")\n",
        "print(\"This is crucial for algorithms like Logistic Regression and SVM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Model Training: Building Multiple Classifiers ü§ñ\n",
        "\n",
        "We'll train and compare 5 different classification algorithms:\n",
        "\n",
        "1. **Logistic Regression** - Linear model, good baseline, interpretable\n",
        "2. **Random Forest** - Ensemble of decision trees, handles non-linearity well\n",
        "3. **Support Vector Machine (SVM)** - Effective in high-dimensional spaces\n",
        "4. **K-Nearest Neighbors (KNN)** - Instance-based learning\n",
        "5. **Naive Bayes** - Probabilistic classifier, fast and simple\n",
        "6. **XGBoost** (if available) - Gradient boosting, often top performer\n",
        "\n",
        "Each model has different strengths and assumptions. Let's see which works best for Titanic!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ü§ñ INITIALIZE MODELS\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INITIALIZING CLASSIFICATION MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dictionary to store models\n",
        "models = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "models['Logistic Regression'] = LogisticRegression(\n",
        "    random_state=42, \n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'  # Handle class imbalance\n",
        ")\n",
        "\n",
        "# 2. Random Forest\n",
        "models['Random Forest'] = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 3. Support Vector Machine\n",
        "models['SVM'] = SVC(\n",
        "    probability=True,  # Enable probability estimates\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 4. K-Nearest Neighbors\n",
        "models['KNN'] = KNeighborsClassifier(\n",
        "    n_neighbors=5\n",
        ")\n",
        "\n",
        "# 5. Naive Bayes\n",
        "models['Naive Bayes'] = GaussianNB()\n",
        "\n",
        "# 6. XGBoost (if available)\n",
        "if XGBOOST_AVAILABLE:\n",
        "    models['XGBoost'] = xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "print(f\"Models initialized: {list(models.keys())}\")\n",
        "print(\"\\nModel descriptions:\")\n",
        "for name in models.keys():\n",
        "    print(f\"  ‚úÖ {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèãÔ∏è TRAIN MODELS AND EVALUATE WITH CROSS-VALIDATION\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING MODELS WITH 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Store results\n",
        "cv_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Use StratifiedKFold to maintain class distribution\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Use scaled data for models that need it\n",
        "    if name in ['Logistic Regression', 'SVM', 'KNN']:\n",
        "        X_train_use = X_train_scaled\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_train\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='accuracy')\n",
        "    \n",
        "    # Train on full training set\n",
        "    model.fit(X_train_use, y_train)\n",
        "    \n",
        "    # Store trained model\n",
        "    trained_models[name] = model\n",
        "    \n",
        "    # Store CV results\n",
        "    cv_results[name] = {\n",
        "        'CV Mean Accuracy': cv_scores.mean(),\n",
        "        'CV Std Accuracy': cv_scores.std(),\n",
        "        'CV Scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CROSS-VALIDATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cv_summary = pd.DataFrame({\n",
        "    name: {\n",
        "        'Mean Accuracy': f\"{res['CV Mean Accuracy']:.4f}\",\n",
        "        'Std Dev': f\"{res['CV Std Accuracy']:.4f}\"\n",
        "    } for name, res in cv_results.items()\n",
        "}).T\n",
        "\n",
        "cv_summary = cv_summary.sort_values('Mean Accuracy', ascending=False)\n",
        "display(cv_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä DETAILED EVALUATION ON TEST SET\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DETAILED EVALUATION ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Store all predictions and metrics\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "all_metrics = []\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    # Use appropriate data (scaled or unscaled)\n",
        "    if name in ['Logistic Regression', 'SVM', 'KNN']:\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_use)\n",
        "    predictions[name] = y_pred\n",
        "    \n",
        "    # Probabilities (for ROC curve)\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_prob = model.predict_proba(X_test_use)[:, 1]\n",
        "        probabilities[name] = y_prob\n",
        "    else:\n",
        "        y_prob = None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1-Score': f1_score(y_test, y_pred),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else 'N/A'\n",
        "    }\n",
        "    \n",
        "    all_metrics.append(metrics)\n",
        "\n",
        "# Create metrics DataFrame\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "metrics_df = metrics_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nMODEL PERFORMANCE COMPARISON:\")\n",
        "print(\"-\"*70)\n",
        "display(metrics_df.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METRIC EXPLANATIONS:\")\n",
        "print(\"=\"*70)\n",
        "print(\"‚Ä¢ Accuracy: Overall correctness (TP + TN) / Total\")\n",
        "print(\"‚Ä¢ Precision: Of predicted survivors, how many actually survived (TP / (TP + FP))\")\n",
        "print(\"‚Ä¢ Recall: Of actual survivors, how many did we predict (TP / (TP + FN))\")\n",
        "print(\"‚Ä¢ F1-Score: Harmonic mean of Precision and Recall\")\n",
        "print(\"‚Ä¢ ROC-AUC: Area under ROC curve (1.0 = perfect, 0.5 = random)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà VISUALIZE CONFUSION MATRICES\n",
        "\n",
        "n_models = len(trained_models)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
        "    if idx < len(axes):\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        # Create heatmap\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                   xticklabels=['Did not survive', 'Survived'],\n",
        "                   yticklabels=['Did not survive', 'Survived'])\n",
        "        axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "\n",
        "# Remove empty subplot\n",
        "if n_models < 6:\n",
        "    fig.delaxes(axes[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONFUSION MATRIX INTERPRETATION:\")\n",
        "print(\"=\"*70)\n",
        "print(\"‚Ä¢ True Negatives (Top-Left): Correctly predicted non-survivors\")\n",
        "print(\"‚Ä¢ False Positives (Top-Right): Predicted survival, but didn't survive\")\n",
        "print(\"‚Ä¢ False Negatives (Bottom-Left): Predicted death, but survived\")\n",
        "print(\"‚Ä¢ True Positives (Bottom-Right): Correctly predicted survivors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìâ PLOT ROC CURVES\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot ROC curve for each model\n",
        "for name, y_prob in probabilities.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "\n",
        "# Plot random guess line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess (AUC = 0.500)', linewidth=1)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
        "plt.title('ROC Curves Comparison\\n(Higher curve = Better model)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ROC CURVE INTERPRETATION:\")\n",
        "print(\"=\"*70)\n",
        "print(\"‚Ä¢ X-axis: False Positive Rate (FPR) - how many non-survivors we wrongly label as survivors\")\n",
        "print(\"‚Ä¢ Y-axis: True Positive Rate (TPR) - how many actual survivors we correctly identify\")\n",
        "print(\"‚Ä¢ Ideal model: Goes straight up to (0,1) then right to (1,1)\")\n",
        "print(\"‚Ä¢ Random guess: Diagonal line from (0,0) to (1,1)\")\n",
        "print(\"‚Ä¢ AUC = 1.0: Perfect classifier\")\n",
        "print(\"‚Ä¢ AUC = 0.5: No better than random guessing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç FEATURE IMPORTANCE ANALYSIS\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Random Forest Feature Importance\n",
        "rf_model = trained_models['Random Forest']\n",
        "rf_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False).head(10)\n",
        "\n",
        "axes[0].barh(rf_importance['feature'], rf_importance['importance'], color='forestgreen')\n",
        "axes[0].set_title('Random Forest\\nTop 10 Feature Importances', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Logistic Regression Coefficients\n",
        "lr_model = trained_models['Logistic Regression']\n",
        "lr_coef = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'coefficient': lr_model.coef_[0]\n",
        "})\n",
        "lr_coef['abs_coef'] = np.abs(lr_coef['coefficient'])\n",
        "lr_coef = lr_coef.sort_values('abs_coef', ascending=False).head(10)\n",
        "\n",
        "colors = ['red' if x < 0 else 'blue' for x in lr_coef['coefficient']]\n",
        "axes[1].barh(lr_coef['feature'], lr_coef['coefficient'], color=colors)\n",
        "axes[1].set_title('Logistic Regression\\nTop 10 Coefficients (Blue=Positive, Red=Negative)', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Coefficient Value')\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 Most Important Features (Random Forest):\")\n",
        "for idx, row in rf_importance.head(5).iterrows():\n",
        "    print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\nTop 5 Most Influential Features (Logistic Regression):\")\n",
        "for idx, row in lr_coef.head(5).iterrows():\n",
        "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
        "    print(f\"  {idx+1}. {row['feature']}: {row['coefficient']:.4f} ({direction} survival probability)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Model Improvement: Hyperparameter Tuning üéØ\n",
        "\n",
        "Now let's improve our best models by tuning their hyperparameters. We'll use **GridSearchCV** to find the optimal combination of parameters.\n",
        "\n",
        "### What is Hyperparameter Tuning?\n",
        "\n",
        "Hyperparameters are settings that control the learning process. Unlike model parameters (which are learned from data), hyperparameters must be set before training. Examples include:\n",
        "- **Random Forest**: Number of trees, max depth, min samples per leaf\n",
        "- **Logistic Regression**: Regularization strength (C), penalty type\n",
        "- **SVM**: Kernel type, C parameter, gamma\n",
        "\n",
        "**Grid Search** systematically tries all combinations of specified parameters to find the best performing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"This may take 1-2 minutes...\\n\")\n",
        "\n",
        "# Dictionary to store best models\n",
        "best_models = {}\n",
        "\n",
        "# =============================================================================\n",
        "# 1. TUNE RANDOM FOREST\n",
        "# =============================================================================\n",
        "print(\"Tuning Random Forest...\")\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    rf_params,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_models['Random Forest'] = rf_grid.best_estimator_\n",
        "\n",
        "print(f\"  Best Parameters: {rf_grid.best_params_}\")\n",
        "print(f\"  Best CV Score: {rf_grid.best_score_:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. TUNE LOGISTIC REGRESSION\n",
        "# =============================================================================\n",
        "print(\"\\nTuning Logistic Regression...\")\n",
        "\n",
        "lr_params = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # liblinear supports both l1 and l2\n",
        "}\n",
        "\n",
        "lr_grid = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    lr_params,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lr_grid.fit(X_train_scaled, y_train)\n",
        "best_models['Logistic Regression'] = lr_grid.best_estimator_\n",
        "\n",
        "print(f\"  Best Parameters: {lr_grid.best_params_}\")\n",
        "print(f\"  Best CV Score: {lr_grid.best_score_:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. TUNE SVM\n",
        "# =============================================================================\n",
        "print(\"\\nTuning SVM...\")\n",
        "\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['rbf', 'linear'],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01]\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(\n",
        "    SVC(probability=True, random_state=42, class_weight='balanced'),\n",
        "    svm_params,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "svm_grid.fit(X_train_scaled, y_train)\n",
        "best_models['SVM'] = svm_grid.best_estimator_\n",
        "\n",
        "print(f\"  Best Parameters: {svm_grid.best_params_}\")\n",
        "print(f\"  Best CV Score: {svm_grid.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter tuning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä COMPARE TUNED MODELS VS BASELINE\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: TUNED MODELS VS BASELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for name in ['Random Forest', 'Logistic Regression', 'SVM']:\n",
        "    # Baseline model\n",
        "    baseline = trained_models[name]\n",
        "    \n",
        "    # Tuned model\n",
        "    tuned = best_models[name]\n",
        "    \n",
        "    # Use appropriate data\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Baseline predictions\n",
        "    baseline_pred = baseline.predict(X_test_use)\n",
        "    baseline_acc = accuracy_score(y_test, baseline_pred)\n",
        "    \n",
        "    # Tuned predictions\n",
        "    tuned_pred = tuned.predict(X_test_use)\n",
        "    tuned_acc = accuracy_score(y_test, tuned_pred)\n",
        "    \n",
        "    # Improvement\n",
        "    improvement = tuned_acc - baseline_acc\n",
        "    \n",
        "    comparison_results.append({\n",
        "        'Model': name,\n",
        "        'Baseline Accuracy': baseline_acc,\n",
        "        'Tuned Accuracy': tuned_acc,\n",
        "        'Improvement': improvement\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "comparison_df = comparison_df.sort_values('Tuned Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n\")\n",
        "display(comparison_df.round(4))\n",
        "\n",
        "# Visualize improvement\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, comparison_df['Baseline Accuracy'], width, \n",
        "               label='Baseline', color='lightcoral', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, comparison_df['Tuned Accuracy'], width, \n",
        "               label='Tuned', color='lightgreen', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Model Performance: Baseline vs Tuned', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df['Model'])\n",
        "ax.legend()\n",
        "ax.set_ylim([0.7, 0.9])\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_accuracy = comparison_df.iloc[0]['Tuned Accuracy']\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   Test Accuracy: {best_accuracy:.4f} ({best_accuracy:.2%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèÜ FINAL EVALUATION OF BEST MODEL\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get the best model\n",
        "final_model = best_models[best_model_name]\n",
        "\n",
        "# Use appropriate test data\n",
        "if best_model_name in ['Logistic Regression', 'SVM']:\n",
        "    X_test_final = X_test_scaled\n",
        "else:\n",
        "    X_test_final = X_test\n",
        "\n",
        "# Final predictions\n",
        "y_pred_final = final_model.predict(X_test_final)\n",
        "y_prob_final = final_model.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "# Comprehensive metrics\n",
        "print(f\"\\nModel: {best_model_name}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_final):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred_final):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test, y_pred_final):.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_prob_final):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(y_test, y_pred_final, \n",
        "                           target_names=['Did not survive', 'Survived']))\n",
        "\n",
        "# Confusion Matrix for final model\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=['Did not survive', 'Survived'],\n",
        "           yticklabels=['Did not survive', 'Survived'])\n",
        "plt.title(f'Final Model: {best_model_name}\\nConfusion Matrix', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Calculate specific metrics from confusion matrix\n",
        "tn, fp, fn, tp = cm_final.ravel()\n",
        "print(f\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"  True Negatives:  {tn} (correctly predicted non-survivors)\")\n",
        "print(f\"  False Positives: {fp} (predicted survival, actually died)\")\n",
        "print(f\"  False Negatives: {fn} (predicted death, actually survived)\")\n",
        "print(f\"  True Positives:  {tp} (correctly predicted survivors)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà LEARNING CURVES (Diagnosing Bias vs Variance)\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LEARNING CURVES ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"Learning curves show how model performance changes with training set size.\")\n",
        "print(\"This helps diagnose:\")\n",
        "print(\"  ‚Ä¢ High Bias (Underfitting): Both curves converge at low score\")\n",
        "print(\"  ‚Ä¢ High Variance (Overfitting): Large gap between curves\")\n",
        "print(\"  ‚Ä¢ Good Fit: Curves converge at high score\\n\")\n",
        "\n",
        "def plot_learning_curve(model, X, y, title, ax, cv=5):\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model, X, y, cv=cv, n_jobs=-1, \n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    \n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    train_std = train_scores.std(axis=1)\n",
        "    val_mean = val_scores.mean(axis=1)\n",
        "    val_std = val_scores.std(axis=1)\n",
        "    \n",
        "    ax.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
        "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    ax.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
        "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "    \n",
        "    ax.set_xlabel('Training Set Size')\n",
        "    ax.set_ylabel('Accuracy Score')\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot learning curves for top 3 models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "models_to_plot = ['Random Forest', 'Logistic Regression', 'SVM']\n",
        "\n",
        "for idx, name in enumerate(models_to_plot):\n",
        "    model = best_models[name]\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        X_plot = X_train_scaled\n",
        "    else:\n",
        "        X_plot = X_train\n",
        "    \n",
        "    plot_learning_curve(model, X_plot, y_train, f'{name}', axes[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "print(\"   If validation score is much lower than training score = Overfitting\")\n",
        "print(\"   If both scores are low and close together = Underfitting\")\n",
        "print(\"   If both scores are high and close together = Good fit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Summary and Key Takeaways üéì\n",
        "\n",
        "Congratulations! You've completed a comprehensive binary classification project. Here's what we accomplished:\n",
        "\n",
        "### üìä What We Did:\n",
        "\n",
        "1. **Data Exploration**\n",
        "   - Loaded and explored the Titanic dataset\n",
        "   - Identified key patterns (women and 1st class passengers had higher survival rates)\n",
        "   - Visualized distributions and correlations\n",
        "\n",
        "2. **Statistical Testing**\n",
        "   - ‚úÖ Chi-Square tests confirmed categorical variables are associated with survival\n",
        "   - ‚úÖ T-tests showed significant mean differences in age and fare\n",
        "   - ‚úÖ Verified (or identified violations of) ML assumptions\n",
        "\n",
        "3. **Data Preprocessing**\n",
        "   - Handled missing values strategically\n",
        "   - Created new features (family_size, is_alone, age_group)\n",
        "   - Encoded categorical variables\n",
        "   - Scaled features for algorithms that need it\n",
        "\n",
        "4. **Model Building**\n",
        "   - Trained 6 different classification algorithms\n",
        "   - Used cross-validation for robust evaluation\n",
        "   - Compared multiple performance metrics\n",
        "\n",
        "5. **Model Improvement**\n",
        "   - Performed hyperparameter tuning with GridSearchCV\n",
        "   - Improved model performance\n",
        "   - Analyzed learning curves to diagnose overfitting/underfitting\n",
        "\n",
        "### üèÜ Best Practices Learned:\n",
        "\n",
        "- **Always split data** into train/test sets to avoid data leakage\n",
        "- **Use cross-validation** for more reliable performance estimates\n",
        "- **Scale features** for distance-based algorithms (SVM, KNN, Logistic Regression)\n",
        "- **Tune hyperparameters** to optimize model performance\n",
        "- **Evaluate multiple metrics** (not just accuracy) - especially important for imbalanced data\n",
        "- **Check statistical assumptions** before applying certain algorithms\n",
        "\n",
        "### üöÄ Next Steps to Explore:\n",
        "\n",
        "1. **Feature Engineering**: Try creating more complex features\n",
        "2. **Ensemble Methods**: Combine multiple models (VotingClassifier, Stacking)\n",
        "3. **Advanced Techniques**: Try Neural Networks or Gradient Boosting (XGBoost, LightGBM)\n",
        "4. **Imbalanced Data**: Explore SMOTE, class weights, or threshold tuning\n",
        "5. **Model Interpretation**: Use SHAP or LIME for explainable AI\n",
        "\n",
        "---\n",
        "\n",
        "**Remember**: Machine learning is an iterative process. Start simple, evaluate thoroughly, and gradually improve! üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ BONUS: MAKE PREDICTIONS ON NEW DATA\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MAKING PREDICTIONS WITH THE TRAINED MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Example: Create a passenger profile\n",
        "new_passenger = pd.DataFrame({\n",
        "    'pclass': [1],\n",
        "    'age': [25],\n",
        "    'sibsp': [0],\n",
        "    'parch': [0],\n",
        "    'fare': [100],\n",
        "    'sex_male': [0],  # Female = 0, Male = 1\n",
        "    'embarked_Q': [0],\n",
        "    'embarked_S': [1],\n",
        "    'who_man': [0],\n",
        "    'who_woman': [1],\n",
        "    'alone_True': [1],\n",
        "    'family_size': [1],\n",
        "    'is_alone': [1],\n",
        "    'age_group_Child': [0],\n",
        "    'age_group_Senior': [0],\n",
        "    'age_group_Teenager': [0],\n",
        "    'fare_per_person': [100]\n",
        "})\n",
        "\n",
        "# Ensure column order matches training data\n",
        "new_passenger = new_passenger[X.columns]\n",
        "\n",
        "# Scale if needed\n",
        "if best_model_name in ['Logistic Regression', 'SVM']:\n",
        "    new_passenger_scaled = scaler.transform(new_passenger)\n",
        "    new_passenger_scaled = pd.DataFrame(new_passenger_scaled, columns=X.columns)\n",
        "    prediction_input = new_passenger_scaled\n",
        "else:\n",
        "    prediction_input = new_passenger\n",
        "\n",
        "# Make prediction\n",
        "prediction = final_model.predict(prediction_input)[0]\n",
        "probability = final_model.predict_proba(prediction_input)[0]\n",
        "\n",
        "print(\"\\nNew Passenger Profile:\")\n",
        "print(f\"  Class: 1st, Age: 25, Female, Alone, Fare: $100\")\n",
        "print(f\"\\nPrediction: {'SURVIVED' if prediction == 1 else 'DID NOT SURVIVE'}\")\n",
        "print(f\"Survival Probability: {probability[1]:.2%}\")\n",
        "print(f\"Death Probability: {probability[0]:.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"END OF NOTEBOOK - HAPPY LEARNING! üéâ\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
