{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231e93d8",
   "metadata": {},
   "source": [
    "# ğŸŒ² **Random Forest: The Complete Mastery Guide**\n",
    "## *From Intuition to Implementation - The Ultimate Ensemble Method*\n",
    "\n",
    "---\n",
    "\n",
    "> **\"Random Forest is the perfect introduction to ensemble learning - powerful yet intuitive\"**\n",
    "\n",
    "Welcome to the definitive guide to **Random Forest** - the algorithm that combines simplicity with exceptional performance, making it a staple in every data scientist's toolkit.\n",
    "\n",
    "### ğŸ¯ **Why This Guide?**\n",
    "\n",
    "| Level | What You'll Master | Time |\n",
    "|-------|-------------------|------|\n",
    "| ğŸŒ± **Beginner** | Intuition, basic implementation, first predictions | 45 min |\n",
    "| ğŸŒ¿ **Intermediate** | Tuning, feature importance, OOB scores | 2 hours |\n",
    "| ğŸŒ³ **Advanced** | Bias-variance tradeoff, parallelization, extensions | 3 hours |\n",
    "| ğŸ† **Expert** | Optimizations, production deployment, comparisons | Ongoing |\n",
    "\n",
    "### ğŸ“‹ **Complete Table of Contents**\n",
    "\n",
    "1. [What is Random Forest?](#what)\n",
    "2. [The Core Intuition: Wisdom of Crowds](#intuition)\n",
    "3. [Why Random Forest?](#why)\n",
    "4. [When to Use Random Forest?](#when)\n",
    "5. [The Algorithm Deep Dive](#algorithm)\n",
    "6. [Feature Importance & Interpretability](#importance)\n",
    "7. [Out-of-Bag (OOB) Scoring](#oob)\n",
    "8. [Hyperparameter Tuning](#tuning)\n",
    "9. [Comparison with Other Algorithms](#comparison)\n",
    "10. [Production & Real-World Usage](#production)\n",
    "11. [Common Pitfalls & Solutions](#pitfalls)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf16206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸŒ² RANDOM FOREST: THE COMPLETE GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸ“Š Ready to explore the power of ensemble learning!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0894551",
   "metadata": {},
   "source": [
    "<a name=\"what\"></a>\n",
    "# ğŸŒ± **Chapter 1: What is Random Forest?**\n",
    "\n",
    "## **The Definition**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  RANDOM FOREST = An ensemble of Decision Trees                         â”‚\n",
    "â”‚                                                                         \n",
    "â”‚  Key Idea: Build multiple decision trees and aggregate their predictionsâ”‚\n",
    "â”‚                                                                         \n",
    "â”‚  Two Sources of Randomness:                                             â”‚\n",
    "â”‚  1. BAGGING (Bootstrap Aggregating): Random sampling of data with       â”‚\n",
    "â”‚     replacement - each tree sees different subset of training data      â”‚\n",
    "â”‚  2. FEATURE RANDOMNESS: At each split, only consider random subset      â”‚\n",
    "â”‚     of features - decorrelates trees                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## **The Analogy: The Expert Panel**\n",
    "\n",
    "Imagine you're diagnosing a medical condition:\n",
    "\n",
    "| Scenario | Approach | Problem |\n",
    "|----------|----------|---------|\n",
    "| **Single Doctor** | One specialist's opinion | Might miss something, biased by experience |\n",
    "| **Panel of Doctors** | Multiple specialists vote | Better coverage, reduces individual bias |\n",
    "| **Random Forest** | Panel where each doctor: | Optimal balance |\n",
    "| | â€¢ Sees different patient records (Bagging) | Diversity in training |\n",
    "| | â€¢ Focuses on different symptoms (Feature randomness) | Decorrelated opinions |\n",
    "| | â€¢ Votes on final diagnosis (Aggregation) | Robust prediction |\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "For **Classification**:\n",
    "$$\\hat{y} = \\text{mode}\\{h_1(x), h_2(x), ..., h_B(x)\\}$$\n",
    "\n",
    "For **Regression**:\n",
    "$$\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} h_b(x)$$\n",
    "\n",
    "Where:\n",
    "- $B$ = number of trees (estimators)\n",
    "- $h_b(x)$ = prediction of $b$-th tree\n",
    "- mode = most frequent class (majority vote)\n",
    "\n",
    "## **The Complete Architecture**\n",
    "\n",
    "```\n",
    "Training Phase:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  For b = 1 to B (number of trees):                         â”‚\n",
    "â”‚    1. Bootstrap sample: Draw n samples with replacement     â”‚\n",
    "â”‚       from training data (creates D_b)                     â”‚\n",
    "â”‚    2. Grow tree T_b on D_b:                                â”‚\n",
    "â”‚       At each node:                                        â”‚\n",
    "â”‚         - Select m features randomly from p features       â”‚\n",
    "â”‚         - Find best split using only these m features      â”‚\n",
    "â”‚         - Split node, continue until stopping criteria     â”‚\n",
    "â”‚    3. Store trained tree T_b                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Prediction Phase:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  For new sample x:                                         â”‚\n",
    "â”‚    Classification: Å· = majority_vote{T_1(x), ..., T_B(x)}  â”‚\n",
    "â”‚    Regression:     Å· = average{T_1(x), ..., T_B(x)}        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b77520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸŒ± CHAPTER 1: UNDERSTANDING RANDOM FOREST ARCHITECTURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize the Random Forest architecture\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Create grid for complex layout\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Title\n",
    "fig.suptitle('Random Forest Architecture: From Data to Prediction', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1. Original Data\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "np.random.seed(42)\n",
    "X_viz = np.random.randn(50, 2)\n",
    "y_viz = (X_viz[:, 0] + X_viz[:, 1] > 0).astype(int)\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_viz]\n",
    "ax1.scatter(X_viz[:, 0], X_viz[:, 1], c=colors, alpha=0.6, s=50)\n",
    "ax1.set_title('Original Dataset\\n(n samples, p features)', fontsize=10, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# 2. Bootstrap Sample 1\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "# Sample with replacement\n",
    "idx1 = np.random.choice(50, 50, replace=True)\n",
    "ax2.scatter(X_viz[idx1, 0], X_viz[idx1, 1], c=[colors[i] for i in idx1], alpha=0.6, s=50)\n",
    "# Mark duplicates\n",
    "unique, counts = np.unique(idx1, return_counts=True)\n",
    "duplicates = unique[counts > 1]\n",
    "for dup in duplicates[:3]:  # Show first 3 duplicates\n",
    "    ax2.scatter(X_viz[dup, 0], X_viz[dup, 1], facecolors='none', edgecolors='green', s=150, linewidths=2)\n",
    "ax2.set_title('Bootstrap Sample 1\\n(~63% unique samples)', fontsize=10, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "# 3. Bootstrap Sample 2\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "idx2 = np.random.choice(50, 50, replace=True)\n",
    "ax3.scatter(X_viz[idx2, 0], X_viz[idx2, 1], c=[colors[i] for i in idx2], alpha=0.6, s=50)\n",
    "ax3.set_title('Bootstrap Sample 2\\n(Different samples)', fontsize=10, fontweight='bold')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "\n",
    "# 4. Tree 1 (simplified visualization)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "# Draw simple decision tree structure\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "# Root\n",
    "circle = plt.Circle((0.5, 0.9), 0.08, color='lightgreen', ec='black', linewidth=2)\n",
    "ax4.add_patch(circle)\n",
    "ax4.text(0.5, 0.9, 'Xâ‚<0?', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "# Branches\n",
    "ax4.plot([0.5, 0.3], [0.82, 0.6], 'k-', linewidth=1.5)\n",
    "ax4.plot([0.5, 0.7], [0.82, 0.6], 'k-', linewidth=1.5)\n",
    "# Leaves\n",
    "circle1 = plt.Circle((0.3, 0.5), 0.08, color='lightcoral', ec='black', linewidth=2)\n",
    "circle2 = plt.Circle((0.7, 0.5), 0.08, color='lightblue', ec='black', linewidth=2)\n",
    "ax4.add_patch(circle1)\n",
    "ax4.add_patch(circle2)\n",
    "ax4.text(0.3, 0.5, 'Class 0', ha='center', va='center', fontsize=8)\n",
    "ax4.text(0.7, 0.5, 'Class 1', ha='center', va='center', fontsize=8)\n",
    "ax4.text(0.5, 0.15, 'Tree 1\\n(uses features 1,3)', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 5. Tree 2 (different structure)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.set_xlim(0, 1)\n",
    "ax5.set_ylim(0, 1)\n",
    "ax5.axis('off')\n",
    "circle = plt.Circle((0.5, 0.9), 0.08, color='lightgreen', ec='black', linewidth=2)\n",
    "ax5.add_patch(circle)\n",
    "ax5.text(0.5, 0.9, 'Xâ‚‚>0?', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "ax5.plot([0.5, 0.3], [0.82, 0.6], 'k-', linewidth=1.5)\n",
    "ax5.plot([0.5, 0.7], [0.82, 0.6], 'k-', linewidth=1.5)\n",
    "circle1 = plt.Circle((0.3, 0.5), 0.08, color='lightblue', ec='black', linewidth=2)\n",
    "circle2 = plt.Circle((0.7, 0.5), 0.08, color='lightcoral', ec='black', linewidth=2)\n",
    "ax5.add_patch(circle1)\n",
    "ax5.add_patch(circle2)\n",
    "ax5.text(0.3, 0.5, 'Class 1', ha='center', va='center', fontsize=8)\n",
    "ax5.text(0.7, 0.5, 'Class 0', ha='center', va='center', fontsize=8)\n",
    "ax5.text(0.5, 0.15, 'Tree 2\\n(uses features 2,4)', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 6. Tree B (ellipsis)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.set_xlim(0, 1)\n",
    "ax6.set_ylim(0, 1)\n",
    "ax6.axis('off')\n",
    "ax6.text(0.5, 0.7, '...', fontsize=40, ha='center', va='center')\n",
    "ax6.text(0.5, 0.4, 'Tree B\\n(100s or 1000s)', ha='center', fontsize=9, fontweight='bold', style='italic')\n",
    "\n",
    "# 7. Aggregation/Voting\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "ax7.set_xlim(0, 10)\n",
    "ax7.set_ylim(0, 3)\n",
    "ax7.axis('off')\n",
    "\n",
    "# Draw voting boxes\n",
    "tree_positions = np.linspace(1, 9, 5)\n",
    "votes = [1, 1, 0, 1, 1]  # 4 votes for class 1, 1 for class 0\n",
    "vote_colors = ['lightblue' if v == 1 else 'lightcoral' for v in votes]\n",
    "\n",
    "for i, (pos, vote, color) in enumerate(zip(tree_positions, votes, vote_colors)):\n",
    "    rect = plt.Rectangle((pos-0.3, 1.5), 0.6, 0.8, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax7.add_patch(rect)\n",
    "    ax7.text(pos, 1.9, f'Tree {i+1}\\nVote: {vote}', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    # Arrows to final\n",
    "    ax7.arrow(pos, 1.5, 0, -0.3, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Final prediction\n",
    "final_rect = plt.Rectangle((3.5, 0.2), 3, 0.8, facecolor='gold', edgecolor='black', linewidth=3)\n",
    "ax7.add_patch(final_rect)\n",
    "ax7.text(5, 0.6, 'FINAL PREDICTION: Class 1\\n(Majority Vote: 4/5)', \n",
    "         ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax7.text(5, 2.8, 'AGGREGATION PHASE: Majority Voting', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Architecture Points:\")\n",
    "print(\"   â€¢ Each tree is trained on ~63% unique samples (bootstrap)\")\n",
    "print(\"   â€¢ Each tree sees different feature subsets at splits\")\n",
    "print(\"   â€¢ Final prediction aggregates all tree predictions\")\n",
    "print(\"   â€¢ More trees = better performance (with diminishing returns)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345abc0",
   "metadata": {},
   "source": [
    "<a name=\"intuition\"></a>\n",
    "# ğŸ§  **Chapter 2: The Core Intuition - Why It Works**\n",
    "\n",
    "## **The Bias-Variance Tradeoff**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ERROR = BIASÂ² + VARIANCE + IRREDUCIBLE ERROR                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Single Decision Tree:                                          â”‚\n",
    "â”‚  â€¢ Low Bias (fits training data well)                          â”‚\n",
    "â”‚  â€¢ High Variance (overfits, unstable)                          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Random Forest:                                                 â”‚\n",
    "â”‚  â€¢ Low Bias (maintained from trees)                            â”‚\n",
    "â”‚  â€¢ Low Variance (averaging reduces variance by factor of B)    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Magic: Ensemble reduces variance WITHOUT increasing bias!     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## **Why Averaging Reduces Variance**\n",
    "\n",
    "Assume we have $B$ independent trees, each with variance $\\sigma^2$:\n",
    "\n",
    "$$\\text{Var}(\\text{average}) = \\text{Var}\\left(\\frac{1}{B}\\sum_{i=1}^{B} X_i\\right) = \\frac{1}{B^2}\\sum_{i=1}^{B}\\text{Var}(X_i) = \\frac{\\sigma^2}{B}$$\n",
    "\n",
    "**With 100 trees, variance reduces by 100x!**\n",
    "\n",
    "## **The Decorrelation Effect**\n",
    "\n",
    "| Method | Tree Correlation | Variance Reduction |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Bagging only** | High (0.9) | Limited (10x) |\n",
    "| **Random Forest** | Low (0.4) | Strong (25x) |\n",
    "| **Extra Trees** | Very Low (0.2) | Very Strong (50x) |\n",
    "\n",
    "## **Visual Intuition: Decision Boundaries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ§  CHAPTER 2: THE INTUITION - WHY RANDOM FOREST WORKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demonstrate variance reduction through visualization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create dataset with noise\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, \n",
    "                           class_sep=0.8, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Add noise to make overfitting visible\n",
    "X_train_noisy = X_train + np.random.normal(0, 0.1, X_train.shape)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Single Decision Tree (deep - overfitting)\n",
    "ax1 = axes[0, 0]\n",
    "tree_deep = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "tree_deep.fit(X_train_noisy, y_train)\n",
    "\n",
    "# Create mesh\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = tree_deep.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax1.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "ax1.scatter(X_train_noisy[:, 0], X_train_noisy[:, 1], c=y_train, cmap='RdYlBu', edgecolors='k')\n",
    "ax1.set_title('Single Decision Tree\\n(No depth limit)\\nHigh Variance, Low Bias', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 2: Single Decision Tree (shallow - underfitting)\n",
    "ax2 = axes[0, 1]\n",
    "tree_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_shallow.fit(X_train_noisy, y_train)\n",
    "Z = tree_shallow.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax2.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "ax2.scatter(X_train_noisy[:, 0], X_train_noisy[:, 1], c=y_train, cmap='RdYlBu', edgecolors='k')\n",
    "ax2.set_title('Single Decision Tree\\n(Max depth=3)\\nHigh Bias, Low Variance', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 3: Random Forest (10 trees)\n",
    "ax3 = axes[0, 2]\n",
    "rf_small = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_small.fit(X_train_noisy, y_train)\n",
    "Z = rf_small.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax3.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "ax3.scatter(X_train_noisy[:, 0], X_train_noisy[:, 1], c=y_train, cmap='RdYlBu', edgecolors='k')\n",
    "ax3.set_title('Random Forest\\n(10 trees)\\nBalanced', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 4: Random Forest (100 trees)\n",
    "ax4 = axes[1, 0]\n",
    "rf_large = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_large.fit(X_train_noisy, y_train)\n",
    "Z = rf_large.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "ax4.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "ax4.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', edgecolors='k', marker='s', s=60)\n",
    "ax4.set_title('Random Forest\\n(100 trees)\\nSmooth, Robust', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax4.set_xlabel('Feature 1')\n",
    "ax4.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 5: Error vs Number of Trees\n",
    "ax5 = axes[1, 1]\n",
    "n_trees_range = [1, 5, 10, 25, 50, 100, 200]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_noisy, y_train)\n",
    "    train_errors.append(1 - accuracy_score(y_train, rf.predict(X_train_noisy)))\n",
    "    test_errors.append(1 - accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "ax5.plot(n_trees_range, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\n",
    "ax5.plot(n_trees_range, test_errors, 's-', label='Test Error', linewidth=2, markersize=8)\n",
    "ax5.axhline(y=test_errors[-1], color='red', linestyle='--', alpha=0.5, label='Asymptotic Error')\n",
    "ax5.set_xlabel('Number of Trees')\n",
    "ax5.set_ylabel('Error Rate')\n",
    "ax5.set_title('Error vs Ensemble Size\\n(More trees = better generalization)', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.set_xscale('log')\n",
    "\n",
    "# Plot 6: Variance demonstration\n",
    "ax6 = axes[1, 2]\n",
    "# Train multiple RF with different seeds to show stability\n",
    "n_experiments = 20\n",
    "predictions = []\n",
    "\n",
    "for seed in range(n_experiments):\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=seed)\n",
    "    rf.fit(X_train_noisy, y_train)\n",
    "    predictions.append(rf.predict(X_test))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "# Calculate variance in predictions for each test point\n",
    "prediction_variance = np.var(predictions, axis=0)\n",
    "\n",
    "# Color test points by prediction variance\n",
    "scatter = ax6.scatter(X_test[:, 0], X_test[:, 1], c=prediction_variance, \n",
    "                     cmap='Reds', s=100, edgecolors='k', vmin=0, vmax=0.25)\n",
    "plt.colorbar(scatter, ax=ax6, label='Prediction Variance')\n",
    "ax6.set_title('Prediction Stability\\n(Lower variance = more confident)', \n",
    "              fontsize=11, fontweight='bold')\n",
    "ax6.set_xlabel('Feature 1')\n",
    "ax6.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nğŸ“Š Bias-Variance Analysis:\")\n",
    "print(f\"Single Tree (deep) - Train Acc: {accuracy_score(y_train, tree_deep.predict(X_train_noisy)):.3f}, \"\n",
    "      f\"Test Acc: {accuracy_score(y_test, tree_deep.predict(X_test)):.3f}\")\n",
    "print(f\"Single Tree (shallow) - Train Acc: {accuracy_score(y_train, tree_shallow.predict(X_train_noisy)):.3f}, \"\n",
    "      f\"Test Acc: {accuracy_score(y_test, tree_shallow.predict(X_test)):.3f}\")\n",
    "print(f\"Random Forest (100) - Train Acc: {accuracy_score(y_train, rf_large.predict(X_train_noisy)):.3f}, \"\n",
    "      f\"Test Acc: {accuracy_score(y_test, rf_large.predict(X_test)):.3f}\")\n",
    "print(f\"\\nğŸ’¡ Random Forest achieves best test accuracy by balancing bias and variance!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c439d",
   "metadata": {},
   "source": [
    "<a name=\"why\"></a>\n",
    "# ğŸŒ¿ **Chapter 3: Why Random Forest?**\n",
    "\n",
    "## **The Competitive Advantage**\n",
    "\n",
    "### ğŸ“Š **Performance Comparison**\n",
    "\n",
    "| Algorithm | Accuracy | Training Speed | Prediction Speed | Robustness | Tuning Effort |\n",
    "|-----------|----------|----------------|------------------|------------|---------------|\n",
    "| **Decision Tree** | â­â­â˜†â˜†â˜† | â­â­â­â­â­ | â­â­â­â­â­ | â­â˜†â˜†â˜†â˜† | â­â­â­â˜†â˜† |\n",
    "| **Logistic Regression** | â­â­â­â˜†â˜† | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â˜†â˜† | â­â­â˜†â˜†â˜† |\n",
    "| **SVM** | â­â­â­â­â˜† | â­â­â˜†â˜†â˜† | â­â­â˜†â˜†â˜† | â­â­â­â˜†â˜† | â­â­â­â­â˜† |\n",
    "| **Gradient Boosting** | â­â­â­â­â­ | â­â­â˜†â˜†â˜† | â­â­â­â˜†â˜† | â­â­â­â­â˜† | â­â­â­â­â­ |\n",
    "| **Random Forest** | â­â­â­â­â˜† | â­â­â­â˜†â˜† | â­â­â­â­â˜† | â­â­â­â­â­ | â­â­â˜†â˜†â˜† |\n",
    "| **Neural Network** | â­â­â­â­â­ | â­â˜†â˜†â˜†â˜† | â­â­â­â­â˜† | â­â­â­â˜†â˜† | â­â­â­â­â­ |\n",
    "\n",
    "### ğŸ¯ **When Random Forest Shines**\n",
    "\n",
    "```\n",
    "âœ… EXCELLENT FOR:\n",
    "   â€¢ Tabular data with mixed feature types\n",
    "   â€¢ Datasets with missing values (handles natively)\n",
    "   â€¢ When you need feature importance\n",
    "   â€¢ Baseline modeling (hard to beat with default params)\n",
    "   â€¢ Production systems (robust, low maintenance)\n",
    "   â€¢ Parallel processing (embarrassingly parallel)\n",
    "\n",
    "âš ï¸  AVOID WHEN:\n",
    "   â€¢ Very high dimensional sparse data (text, images)\n",
    "   â€¢ Need probabilistic outputs well-calibrated\n",
    "   â€¢ Real-time prediction with microsecond latency\n",
    "   â€¢ Dataset size > 10GB (consider gradient boosting)\n",
    "   â€¢ Linear relationships only (linear models better)\n",
    "```\n",
    "\n",
    "## **Unique Advantages**\n",
    "\n",
    "| Feature | Benefit | How RF Delivers |\n",
    "|---------|---------|-----------------|\n",
    "| **No Overfitting** (theoretically) | As trees â†’ âˆ, generalization error converges | Law of Large Numbers |\n",
    "| **Built-in Validation** | No need for separate validation set | Out-of-Bag (OOB) scoring |\n",
    "| **Feature Selection** | Know which features matter | Gini importance, permutation importance |\n",
    "| **Handles Missing Data** | No imputation needed | Proximity matrix, surrogate splits |\n",
    "| **Outlier Robustness** | Extreme values don't break model | Median aggregation, tree structure |\n",
    "| **Parallel Training** | Fast training on multi-core | Trees are independent |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸŒ¿ CHAPTER 3: WHY RANDOM FOREST? - PERFORMANCE DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Comprehensive comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "# Load complex dataset\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_informative=10, \n",
    "                           n_redundant=5, n_clusters_per_class=2, \n",
    "                           class_sep=0.8, flip_y=0.05, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset: {X.shape[0]} samples, {X.shape[1]} features (complex, noisy)\")\n",
    "\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM (RBF)': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ”„ Training {name}...\")\n",
    "    \n",
    "    # Training time\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Prediction time\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_time = time.time() - start\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    # Cross-validation for stability\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Test AUC': auc,\n",
    "        'Test Acc': acc,\n",
    "        'CV AUC Mean': cv_scores.mean(),\n",
    "        'CV AUC Std': cv_scores.std(),\n",
    "        'Train Time (s)': train_time,\n",
    "        'Predict Time (s)': pred_time\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nğŸ“ˆ Comprehensive Comparison:\")\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Test AUC\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['#e74c3c', '#3498db', '#9b59b6', '#2ecc71', '#f39c12']\n",
    "bars1 = ax1.bar(range(len(results_df)), results_df['Test AUC'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_xticks(range(len(results_df)))\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('ROC-AUC')\n",
    "ax1.set_title('Test Set Performance\\n(Higher is better)', fontweight='bold')\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "for i, (bar, val) in enumerate(zip(bars1, results_df['Test AUC'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# CV Stability (mean Â± std)\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax2.bar(x_pos, results_df['CV AUC Mean'], yerr=results_df['CV AUC Std'], \n",
    "        color=colors, alpha=0.8, capsize=5, edgecolor='black')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('CV ROC-AUC')\n",
    "ax2.set_title('Cross-Validation Stability\\n(Lower std = more robust)', fontweight='bold')\n",
    "ax2.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Training time\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(range(len(results_df)), results_df['Train Time (s)'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_xticks(range(len(results_df)))\n",
    "ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Time (seconds)')\n",
    "ax3.set_title('Training Time\\n(Lower is better)', fontweight='bold')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Speedup relative to slowest\n",
    "ax4 = axes[1, 1]\n",
    "max_time = results_df['Train Time (s)'].max()\n",
    "speedups = max_time / results_df['Train Time (s)']\n",
    "bars4 = ax4.bar(range(len(results_df)), speedups, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax4.set_xticks(range(len(results_df)))\n",
    "ax4.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax4.set_ylabel('Speedup Factor')\n",
    "ax4.set_title('Relative Training Speed\\n(Higher is better)', fontweight='bold')\n",
    "ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ† Random Forest Strengths Demonstrated:\")\n",
    "print(f\"   â€¢ Competitive accuracy (AUC: {results_df[results_df['Model']=='Random Forest']['Test AUC'].values[0]:.3f})\")\n",
    "print(f\"   â€¢ Excellent stability (CV std: {results_df[results_df['Model']=='Random Forest']['CV AUC Std'].values[0]:.4f})\")\n",
    "print(f\"   â€¢ Fast training with parallelization\")\n",
    "print(f\"   â€¢ No hyperparameter tuning needed for good results!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68248df",
   "metadata": {},
   "source": [
    "<a name=\"when\"></a>\n",
    "# ğŸŒ³ **Chapter 4: When to Use Random Forest?**\n",
    "\n",
    "## **Decision Flowchart**\n",
    "\n",
    "```\n",
    "START: New Machine Learning Project\n",
    "        â”‚\n",
    "        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Is your data tabular        â”‚\n",
    "â”‚ (rows and columns)?         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚ No                           â”‚ Yes\n",
    "     â–¼                              â–¼\n",
    "Use Neural Networks          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "(CNN, RNN, Transformer)      â”‚ Dataset size?               â”‚\n",
    "                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                  â”‚ <1K      â”‚ 1K-100K    â”‚ >100K\n",
    "                                  â–¼          â–¼            â–¼\n",
    "                             Logistic    Random      Gradient\n",
    "                             Regression  Forest      Boosting\n",
    "                                         â­â­â­        (XGBoost,\n",
    "                                         Start here!   LightGBM)\n",
    "\n",
    "                             Need fast\n",
    "                             baseline?\n",
    "                                  â”‚\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                   â–¼                             â–¼\n",
    "             Random Forest                  Tune RF or\n",
    "             (5 min setup)                  switch to GB\n",
    "```\n",
    "\n",
    "## **Task-Specific Recommendations**\n",
    "\n",
    "| Task | Random Forest Fit | Notes |\n",
    "|------|-------------------|-------|\n",
    "| **Binary Classification** | â­â­â­â­â­ Excellent | Default choice for tabular |\n",
    "| **Multi-class** | â­â­â­â­â­ Excellent | Handles naturally |\n",
    "| **Regression** | â­â­â­â­â­ Excellent | Use RandomForestRegressor |\n",
    "| **Feature Selection** | â­â­â­â­â­ Excellent | Built-in importance |\n",
    "| **Time Series** | â­â­â­â˜†â˜† Fair | Need feature engineering |\n",
    "| **Anomaly Detection** | â­â­â­â˜†â˜† Fair | Use isolation forest variant |\n",
    "| **Recommendation** | â­â­â˜†â˜†â˜† Poor | Use collaborative filtering |\n",
    "| **NLP/Images** | â­â˜†â˜†â˜†â˜† Very Poor | Use deep learning |\n",
    "\n",
    "## **Data Characteristics Guide**\n",
    "\n",
    "| Characteristic | Impact on RF | Recommendation |\n",
    "|----------------|--------------|----------------|\n",
    "| **Missing values (<30%)** | âœ… Handles well | Use `missing_values` strategy |\n",
    "| **Categorical features** | âš ï¸ Needs encoding | One-hot or ordinal encoding |\n",
    "| **High cardinality (>100 categories)** | âš ï¸ Be careful | Target encoding may help |\n",
    "| **Imbalanced classes** | âš ï¸ Needs tuning | Use `class_weight` or SMOTE |\n",
    "| **Outliers** | âœ… Robust | No preprocessing needed |\n",
    "| **Correlated features** | âœ… Handles well | Feature selection optional |\n",
    "| **Non-linear relationships** | âœ… Excellent | RF's strength |\n",
    "| **Linear only** | âš ï¸ Overkill | Use linear models |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d75e96",
   "metadata": {},
   "source": [
    "<a name=\"algorithm\"></a>\n",
    "# ğŸ¯ **Chapter 5: The Algorithm Deep Dive**\n",
    "\n",
    "## **Step-by-Step Algorithm**\n",
    "\n",
    "```\n",
    "RANDOM FOREST TRAINING ALGORITHM\n",
    "\n",
    "Input: Training set D = {(xâ‚,yâ‚), ..., (xâ‚™,yâ‚™)}, \n",
    "       Number of trees B,\n",
    "       Number of features to consider m\n",
    "\n",
    "For b = 1 to B:\n",
    "    1. BOOTSTRAP SAMPLING:\n",
    "       D_b = BootstrapSample(D, n)  // Sample n items with replacement\n",
    "       \n",
    "    2. TREE CONSTRUCTION:\n",
    "       T_b = BuildTree(D_b, m)\n",
    "       \n",
    "       Procedure BuildTree(Node, m):\n",
    "           If stopping criteria met:\n",
    "               Return Leaf(predict majority/average)\n",
    "           \n",
    "           // FEATURE RANDOMNESS\n",
    "           Select m features randomly from p available\n",
    "           \n",
    "           // FIND BEST SPLIT\n",
    "           For each of m features:\n",
    "               Find best split point using Gini/MSE\n",
    "           \n",
    "           Select best feature and split point\n",
    "           Split node into left and right children\n",
    "           \n",
    "           BuildTree(Left Child, m)\n",
    "           BuildTree(Right Child, m)\n",
    "\n",
    "Return: Forest F = {Tâ‚, Tâ‚‚, ..., T_B}\n",
    "\n",
    "PREDICTION:\n",
    "Classification: Å· = MajorityVote(F(x))\n",
    "Regression:     Å· = Average(F(x))\n",
    "```\n",
    "\n",
    "## **Key Hyperparameters**\n",
    "\n",
    "```\n",
    "ğŸ›ï¸ PARAMETER HIERARCHY\n",
    "\n",
    "Level 1: Most Important (Tune These First)\n",
    "â”œâ”€â”€ n_estimators     â† Number of trees (100-500 typical)\n",
    "â”‚                      More = better, but diminishing returns\n",
    "â”œâ”€â”€ max_features     â† Features considered at each split\n",
    "â”‚                      'sqrt' for classification (default)\n",
    "â”‚                      'log2' or None for regression\n",
    "â””â”€â”€ max_depth        â† Tree depth limit\n",
    "                       None = grow until pure (risk overfit)\n",
    "                       10-30 = good starting point\n",
    "\n",
    "Level 2: Control Overfitting\n",
    "â”œâ”€â”€ min_samples_split  â† Min samples to split node (default: 2)\n",
    "â”œâ”€â”€ min_samples_leaf   â† Min samples in leaf (default: 1)\n",
    "â”‚                        Increase to prevent overfitting\n",
    "â”œâ”€â”€ min_impurity_decrease â† Min impurity reduction to split\n",
    "â””â”€â”€ bootstrap          â† Use bootstrap sampling? (default: True)\n",
    "                         False = use whole dataset (less random)\n",
    "\n",
    "Level 3: Optimization\n",
    "â”œâ”€â”€ criterion          â† 'gini' or 'entropy' (classification)\n",
    "â”‚                      â† 'squared_error' or 'absolute_error' (regression)\n",
    "â”œâ”€â”€ max_leaf_nodes     â† Alternative to max_depth\n",
    "â””â”€â”€ class_weight       â† Handle imbalanced classes\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ CHAPTER 5: ALGORITHM DEEP DIVE - BOOTSTRAP & TREE GROWTH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demonstrate bootstrap sampling\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Original data indices\n",
    "original_indices = np.arange(n_samples)\n",
    "\n",
    "# Perform multiple bootstrap samples\n",
    "unique_counts = []\n",
    "for _ in range(n_bootstrap):\n",
    "    bootstrap_sample = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    unique_counts.append(len(np.unique(bootstrap_sample)))\n",
    "\n",
    "avg_unique = np.mean(unique_counts)\n",
    "theoretical_unique = n_samples * (1 - (1 - 1/n_samples)**n_samples)\n",
    "\n",
    "print(f\"\\nğŸ“Š Bootstrap Sampling Analysis:\")\n",
    "print(f\"Original samples: {n_samples}\")\n",
    "print(f\"Bootstrap samples: {n_samples} (with replacement)\")\n",
    "print(f\"Average unique samples in bootstrap: {avg_unique:.1f} (theory: {theoretical_unique:.1f})\")\n",
    "print(f\"Percentage of unique samples: {avg_unique/n_samples*100:.1f}%\")\n",
    "print(f\"Out-of-Bag samples per tree: ~{n_samples - avg_unique:.0f} ({(n_samples-avg_unique)/n_samples*100:.1f}%)\")\n",
    "\n",
    "# Visualize bootstrap and tree diversity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Bootstrap distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(unique_counts, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax1.axvline(x=avg_unique, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_unique:.1f}')\n",
    "ax1.set_xlabel('Number of Unique Samples in Bootstrap')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Bootstrap Sampling Distribution\\n(~63% unique samples expected)', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Effect of n_estimators\n",
    "ax2 = axes[0, 1]\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 200]\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, oob_score=True, \n",
    "                                random_state=42, n_jobs=-1, max_features='sqrt')\n",
    "    rf.fit(X_train, y_train)\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "    test_scores.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "ax2.plot(n_estimators_range, oob_scores, 'o-', label='OOB Score', linewidth=2, markersize=8)\n",
    "ax2.plot(n_estimators_range, test_scores, 's-', label='Test Score', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=max(test_scores), color='gray', linestyle='--', alpha=0.5, label='Best Test Score')\n",
    "ax2.set_xlabel('Number of Trees (n_estimators)')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Performance vs Ensemble Size\\n(Diminishing returns after 100-200 trees)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "# Plot 3: Effect of max_features\n",
    "ax3 = axes[1, 0]\n",
    "max_features_options = [1, 2, 3, 5, 7, 10]  # out of 10 features\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_f in max_features_options:\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_features=max_f, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, rf.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "ax3.plot(max_features_options, train_scores, 'o-', label='Train', linewidth=2)\n",
    "ax3.plot(max_features_options, test_scores, 's-', label='Test', linewidth=2)\n",
    "ax3.axvline(x=np.sqrt(10), color='red', linestyle='--', alpha=0.7, label='sqrt(p) default')\n",
    "ax3.set_xlabel('max_features')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Effect of Feature Subsampling\\n(Lower = more randomness, less overfit)', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Tree depth effect\n",
    "ax4 = axes[1, 1]\n",
    "max_depth_options = [1, 2, 3, 5, 7, 10, None]\n",
    "train_scores_d = []\n",
    "test_scores_d = []\n",
    "\n",
    "for depth in max_depth_options:\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_depth=depth, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores_d.append(accuracy_score(y_train, rf.predict(X_train)))\n",
    "    test_scores_d.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "x_labels = [str(d) if d is not None else 'None' for d in max_depth_options]\n",
    "x_pos = range(len(x_labels))\n",
    "ax4.plot(x_pos, train_scores_d, 'o-', label='Train', linewidth=2, markersize=8)\n",
    "ax4.plot(x_pos, test_scores_d, 's-', label='Test', linewidth=2, markersize=8)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(x_labels)\n",
    "ax4.set_xlabel('max_depth')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Effect of Tree Depth\\n(None = grow until pure)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Algorithm Insights:\")\n",
    "print(f\"   â€¢ Bootstrap creates ~63% unique samples per tree\")\n",
    "print(f\"   â€¢ ~37% OOB samples enable built-in validation\")\n",
    "print(f\"   â€¢ Performance plateaus around 100-200 trees\")\n",
    "print(f\"   â€¢ max_features='sqrt' (default) balances bias-variance well\")\n",
    "print(f\"   â€¢ Unlimited depth can overfit, but ensemble mitigates this\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01703a58",
   "metadata": {},
   "source": [
    "<a name=\"importance\"></a>\n",
    "# ğŸ“Š **Chapter 6: Feature Importance & Interpretability**\n",
    "\n",
    "## **Types of Feature Importance**\n",
    "\n",
    "| Type | Description | Best For | Caution |\n",
    "|------|-------------|----------|---------|\n",
    "| **Gini Importance** (MDI) | Total impurity reduction | Quick screening | Biased toward high cardinality |\n",
    "| **Permutation Importance** | Drop in performance when shuffled | Reliable ranking | Computationally expensive |\n",
    "| **SHAP Values** | Game-theoretic contribution | Individual predictions | Complex interpretation |\n",
    "\n",
    "## **Mathematical Definition: Gini Importance**\n",
    "\n",
    "For feature $X_j$ in tree $T$:\n",
    "\n",
    "$$\\text{Importance}(X_j) = \\sum_{t \\in T: v(t)=j} p(t) \\Delta i(t)$$\n",
    "\n",
    "Where:\n",
    "- $v(t)$ = feature used at node $t$\n",
    "- $p(t)$ = proportion of samples reaching node $t$\n",
    "- $\\Delta i(t)$ = impurity reduction at node $t$\n",
    "\n",
    "**Random Forest aggregates across all trees:**\n",
    "\n",
    "$$\\text{RF_Importance}(X_j) = \\frac{1}{B}\\sum_{b=1}^{B} \\text{Importance}_b(X_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97cb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š CHAPTER 6: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load dataset with meaningful feature names\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset: Breast Cancer ({X.shape[1]} features)\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get different types of importance\n",
    "print(f\"\\nğŸ” Computing Feature Importances...\")\n",
    "\n",
    "# 1. Gini Importance (MDI)\n",
    "gini_importance = rf.feature_importances_\n",
    "\n",
    "# 2. Permutation Importance\n",
    "perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "perm_mean = perm_importance.importances_mean\n",
    "perm_std = perm_importance.importances_std\n",
    "\n",
    "# Create comparison DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Gini_Importance': gini_importance,\n",
    "    'Permutation_Mean': perm_mean,\n",
    "    'Permutation_Std': perm_std\n",
    "}).sort_values('Gini_Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Top 10 Features (Gini Importance):\")\n",
    "print(importance_df.head(10)[['Feature', 'Gini_Importance']].to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Plot 1: Gini Importance\n",
    "ax1 = axes[0, 0]\n",
    "top_features = importance_df.head(15)\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax1.barh(y_pos, top_features['Gini_Importance'], color='skyblue', edgecolor='black')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels([f[:30] for f in top_features['Feature']], fontsize=9)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Gini Importance')\n",
    "ax1.set_title('Feature Importance (Gini/MDI)\\n(Biased toward high cardinality)', fontweight='bold')\n",
    "\n",
    "# Plot 2: Permutation Importance\n",
    "ax2 = axes[0, 1]\n",
    "sorted_perm = importance_df.sort_values('Permutation_Mean', ascending=False).head(15)\n",
    "y_pos = np.arange(len(sorted_perm))\n",
    "ax2.barh(y_pos, sorted_perm['Permutation_Mean'], xerr=sorted_perm['Permutation_Std'],\n",
    "         color='lightcoral', edgecolor='black', capsize=3)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels([f[:30] for f in sorted_perm['Feature']], fontsize=9)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Feature Importance (Permutation)\\n(More reliable, unbiased)', fontweight='bold')\n",
    "\n",
    "# Plot 3: Comparison scatter\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(importance_df['Gini_Importance'], importance_df['Permutation_Mean'], \n",
    "           s=100, alpha=0.6, edgecolors='black')\n",
    "ax3.plot([0, max(importance_df['Gini_Importance'])], \n",
    "         [0, max(importance_df['Gini_Importance'])], 'r--', alpha=0.5)\n",
    "ax3.set_xlabel('Gini Importance')\n",
    "ax3.set_ylabel('Permutation Importance')\n",
    "ax3.set_title('Gini vs Permutation Importance\\n(Correlation shows agreement)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate outliers\n",
    "for idx, row in importance_df.iterrows():\n",
    "    if abs(row['Gini_Importance'] - row['Permutation_Mean']) > 0.05:\n",
    "        ax3.annotate(row['Feature'][:15], \n",
    "                    (row['Gini_Importance'], row['Permutation_Mean']),\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# Plot 4: Cumulative importance\n",
    "ax4 = axes[1, 1]\n",
    "sorted_gini = np.sort(gini_importance)[::-1]\n",
    "cumulative = np.cumsum(sorted_gini)\n",
    "ax4.plot(range(1, len(cumulative)+1), cumulative, 'o-', linewidth=2, markersize=4)\n",
    "ax4.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax4.set_xlabel('Number of Features')\n",
    "ax4.set_ylabel('Cumulative Importance')\n",
    "ax4.set_title('Cumulative Feature Importance\\n(How many features needed for 90%?)', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "# Find how many features for 90%\n",
    "n_90 = np.argmax(cumulative >= 0.9) + 1\n",
    "ax4.axvline(x=n_90, color='green', linestyle='--', alpha=0.7)\n",
    "ax4.text(n_90+2, 0.5, f'{n_90} features\\nfor 90%', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Feature Selection Insights:\")\n",
    "print(f\"   â€¢ Top 5 features account for {cumulative[4]:.1%} of total importance\")\n",
    "print(f\"   â€¢ Only {n_90} features needed for 90% of predictive power\")\n",
    "print(f\"   â€¢ Consider removing low-importance features to reduce overfitting\")\n",
    "\n",
    "# Feature selection demonstration\n",
    "print(f\"\\nğŸ¯ Feature Selection Impact:\")\n",
    "for n_features in [5, 10, 15, 20, 30]:\n",
    "    top_idx = np.argsort(gini_importance)[-n_features:]\n",
    "    X_train_sel = X_train[:, top_idx]\n",
    "    X_test_sel = X_test[:, top_idx]\n",
    "    \n",
    "    rf_sel = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    rf_sel.fit(X_train_sel, y_train)\n",
    "    acc = accuracy_score(y_test, rf_sel.predict(X_test_sel))\n",
    "    print(f\"   Top {n_features:2d} features: Test Accuracy = {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a5f95",
   "metadata": {},
   "source": [
    "<a name=\"oob\"></a>\n",
    "# ğŸ¯ **Chapter 7: Out-of-Bag (OOB) Scoring**\n",
    "\n",
    "## **What is OOB?**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  OUT-OF-BAG (OOB) SCORE                                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Each tree sees ~63% of data (bootstrap sample)                â”‚\n",
    "â”‚  Remaining ~37% = \"Out-of-Bag\" samples                         â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  For each training sample xáµ¢:                                  â”‚\n",
    "â”‚    â€¢ Predict using only trees where xáµ¢ was OOB                 â”‚\n",
    "â”‚    â€¢ Average those predictions                                 â”‚\n",
    "â”‚    â€¢ Compare to true label                                     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Result: Unbiased estimate of test error WITHOUT validation setâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## **Why OOB is Amazing**\n",
    "\n",
    "| Benefit | Explanation |\n",
    "|---------|-------------|\n",
    "| **No validation set needed** | Use all data for training |\n",
    "| **Built-in cross-validation** | Every sample validated multiple times |\n",
    "| **Training time diagnostic** | OOB score vs iteration shows convergence |\n",
    "| **Feature selection** | OOB importance more reliable than Gini |\n",
    "\n",
    "## **Mathematical Justification**\n",
    "\n",
    "Probability a sample is **NOT** selected in bootstrap:\n",
    "\n",
    "$$P(\\text{not selected}) = \\left(1 - \\frac{1}{n}\\right)^n \\approx \\frac{1}{e} \\approx 0.368$$\n",
    "\n",
    "So ~36.8% of samples are OOB for each tree!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ CHAPTER 7: OUT-OF-BAG (OOB) SCORING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demonstrate OOB scoring\n",
    "X, y = make_classification(n_samples=2000, n_features=20, n_informative=10, \n",
    "                           random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset: {len(X_train)} training samples\")\n",
    "\n",
    "# Train with OOB scoring\n",
    "rf_oob = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    oob_score=True,  # Enable OOB scoring\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    warm_start=True  # Allow incremental training for curve\n",
    ")\n",
    "\n",
    "# Train incrementally to show OOB convergence\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "n_estimators_range = list(range(1, 201, 5))\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf_oob.set_params(n_estimators=n_est)\n",
    "    rf_oob.fit(X_train, y_train)\n",
    "    oob_scores.append(rf_oob.oob_score_)\n",
    "    test_scores.append(accuracy_score(y_test, rf_oob.predict(X_test)))\n",
    "\n",
    "print(f\"Final OOB Score: {oob_scores[-1]:.4f}\")\n",
    "print(f\"Final Test Score: {test_scores[-1]:.4f}\")\n",
    "print(f\"Difference: {abs(oob_scores[-1] - test_scores[-1]):.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: OOB vs Test convergence\n",
    "ax1 = axes[0]\n",
    "ax1.plot(n_estimators_range, oob_scores, 'o-', label='OOB Score', linewidth=2, markersize=4)\n",
    "ax1.plot(n_estimators_range, test_scores, 's-', label='Test Score', linewidth=2, markersize=4)\n",
    "ax1.axhline(y=test_scores[-1], color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Number of Trees')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('OOB Score vs Test Score\\n(OOB â‰ˆ Test: Built-in validation!)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: OOB error bars (simulated)\n",
    "ax2 = axes[1]\n",
    "# Show OOB predictions for individual samples\n",
    "n_samples_show = 50\n",
    "sample_indices = np.random.choice(len(X_train), n_samples_show, replace=False)\n",
    "\n",
    "oob_predictions = np.zeros((len(sample_indices), len(n_estimators_range)))\n",
    "for idx, n_est in enumerate(n_estimators_range):\n",
    "    rf_temp = RandomForestClassifier(n_estimators=n_est, oob_score=True, random_state=42)\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "    # Get OOB decision function for samples\n",
    "    oob_pred = rf_temp.oob_decision_function_[sample_indices]\n",
    "    oob_predictions[:, idx] = oob_pred[:, 1] if oob_pred.ndim > 1 else oob_pred\n",
    "\n",
    "# Plot convergence of individual sample predictions\n",
    "for i in range(min(10, n_samples_show)):  # Show first 10\n",
    "    ax2.plot(n_estimators_range, oob_predictions[i], alpha=0.3, color='blue')\n",
    "\n",
    "ax2.set_xlabel('Number of Trees')\n",
    "ax2.set_ylabel('OOB Prediction Probability')\n",
    "ax2.set_title('Individual Sample OOB Predictions\\n(Convergence with more trees)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ OOB Scoring Benefits:\")\n",
    "print(f\"   â€¢ OOB score ({oob_scores[-1]:.4f}) â‰ˆ Test score ({test_scores[-1]:.4f})\")\n",
    "print(f\"   â€¢ No need for train/validation split - use all data!\")\n",
    "print(f\"   â€¢ Can detect overfitting by comparing OOB vs train score\")\n",
    "print(f\"   â€¢ Early stopping possible with OOB (though not built-in)\")\n",
    "\n",
    "# Compare with cross-validation\n",
    "print(f\"\\nâ±ï¸  Time Comparison:\")\n",
    "from time import time\n",
    "\n",
    "# OOB approach (no CV needed)\n",
    "start = time()\n",
    "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42, n_jobs=-1)\n",
    "rf_oob.fit(X_train, y_train)\n",
    "oob_time = time() - start\n",
    "\n",
    "# CV approach\n",
    "start = time()\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                            X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_time = time() - start\n",
    "\n",
    "print(f\"   OOB scoring:  {oob_time:.3f}s (single fit)\")\n",
    "print(f\"   5-Fold CV:    {cv_time:.3f}s (5 fits)\")\n",
    "print(f\"   Speedup:      {cv_time/oob_time:.1f}x faster!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5763528",
   "metadata": {},
   "source": [
    "<a name=\"tuning\"></a>\n",
    "# ğŸ›ï¸ **Chapter 8: Hyperparameter Tuning**\n",
    "\n",
    "## **The Tuning Strategy**\n",
    "\n",
    "```\n",
    "ğŸ¯ SYSTEMATIC APPROACH TO TUNING\n",
    "\n",
    "Phase 1: Establish Baseline\n",
    "â”œâ”€â”€ Use default parameters (surprisingly good!)\n",
    "â””â”€â”€ Evaluate with cross-validation or OOB\n",
    "\n",
    "Phase 2: Tune n_estimators\n",
    "â”œâ”€â”€ Start with 100, check convergence curve\n",
    "â”œâ”€â”€ Increase until OOB score plateaus\n",
    "â””â”€â”€ Typical range: 100-500 for most problems\n",
    "\n",
    "Phase 3: Tune max_features\n",
    "â”œâ”€â”€ Classification: try 'sqrt' (default), 'log2', 0.3-0.7\n",
    "â”œâ”€â”€ Regression: try None (all features), 0.5-0.8\n",
    "â””â”€â”€ Lower = less overfitting, more diversity\n",
    "\n",
    "Phase 4: Control Overfitting\n",
    "â”œâ”€â”€ Increase min_samples_leaf (2-10)\n",
    "â”œâ”€â”€ Decrease max_depth (10-30)\n",
    "â””â”€â”€ Increase min_samples_split (5-20)\n",
    "\n",
    "Phase 5: Fine-tuning\n",
    "â”œâ”€â”€ bootstrap: True (default) or False\n",
    "â”œâ”€â”€ criterion: 'gini' vs 'entropy'\n",
    "â””â”€â”€ class_weight for imbalanced data\n",
    "```\n",
    "\n",
    "## **Grid Search vs Random Search**\n",
    "\n",
    "| Method | Best For | Trade-off |\n",
    "|--------|----------|-----------|\n",
    "| **Grid Search** | Few parameters, small ranges | Exhaustive but slow |\n",
    "| **Random Search** | Many parameters, wide ranges | Faster, good coverage |\n",
    "| **Bayesian Optimization** | Expensive evaluations | Smart exploration |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ›ï¸ CHAPTER 8: HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Practical tuning demonstration\n",
    "X, y = make_classification(n_samples=2000, n_features=15, n_informative=8, \n",
    "                           n_redundant=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Baseline\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 1: BASELINE (Default Parameters)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "rf_baseline = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "baseline_score = accuracy_score(y_test, rf_baseline.predict(X_test))\n",
    "\n",
    "print(f\"Default Parameters:\")\n",
    "print(f\"  n_estimators={rf_baseline.n_estimators}, max_features={rf_baseline.max_features}\")\n",
    "print(f\"  max_depth={rf_baseline.max_depth}, min_samples_split={rf_baseline.min_samples_split}\")\n",
    "print(f\"Test Accuracy: {baseline_score:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Tune n_estimators\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 2: TUNE N_ESTIMATORS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "n_est_range = [10, 25, 50, 100, 200, 500]\n",
    "scores_n_est = []\n",
    "\n",
    "for n_est in n_est_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    scores_n_est.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "    print(f\"n_estimators={n_est:3d}: Accuracy={scores_n_est[-1]:.4f}\")\n",
    "\n",
    "best_n_est = n_est_range[np.argmax(scores_n_est)]\n",
    "print(f\"Best: n_estimators={best_n_est}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Tune max_features\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 3: TUNE MAX_FEATURES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "max_features_options = ['sqrt', 'log2', None, 0.3, 0.5, 0.7, 0.9]\n",
    "scores_max_feat = []\n",
    "\n",
    "for max_feat in max_features_options:\n",
    "    rf = RandomForestClassifier(n_estimators=best_n_est, max_features=max_feat, \n",
    "                                random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    scores_max_feat.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "    print(f\"max_features={str(max_feat):6s}: Accuracy={scores_max_feat[-1]:.4f}\")\n",
    "\n",
    "best_max_feat = max_features_options[np.argmax(scores_max_feat)]\n",
    "print(f\"Best: max_features={best_max_feat}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Control Overfitting\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 4: CONTROL OVERFITTING\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check if overfitting\n",
    "rf_current = RandomForestClassifier(n_estimators=best_n_est, max_features=best_max_feat,\n",
    "                                    random_state=42, n_jobs=-1)\n",
    "rf_current.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_train, rf_current.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, rf_current.predict(X_test))\n",
    "\n",
    "print(f\"Current - Train: {train_acc:.4f}, Test: {test_acc:.4f}, Gap: {train_acc-test_acc:.4f}\")\n",
    "\n",
    "if train_acc - test_acc > 0.05:  # Overfitting\n",
    "    print(\"Overfitting detected! Tuning regularization...\")\n",
    "    \n",
    "    # Try min_samples_leaf\n",
    "    min_leaf_options = [1, 2, 5, 10]\n",
    "    best_gap = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    for min_leaf in min_leaf_options:\n",
    "        rf = RandomForestClassifier(n_estimators=best_n_est, max_features=best_max_feat,\n",
    "                                    min_samples_leaf=min_leaf, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        gap = accuracy_score(y_train, rf.predict(X_train)) - accuracy_score(y_test, rf.predict(X_test))\n",
    "        print(f\"min_samples_leaf={min_leaf:2d}: Gap={gap:.4f}\")\n",
    "        if gap < best_gap:\n",
    "            best_gap = gap\n",
    "            best_params['min_samples_leaf'] = min_leaf\n",
    "    \n",
    "    print(f\"Best regularization: min_samples_leaf={best_params['min_samples_leaf']}\")\n",
    "else:\n",
    "    print(\"No significant overfitting detected!\")\n",
    "    best_params = {'min_samples_leaf': 1}\n",
    "\n",
    "# ============================================================\n",
    "# Final Comparison\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=best_n_est,\n",
    "    max_features=best_max_feat,\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_train, y_train)\n",
    "final_score = accuracy_score(y_test, rf_final.predict(X_test))\n",
    "\n",
    "print(f\"Baseline Accuracy:  {baseline_score:.4f}\")\n",
    "print(f\"Tuned Accuracy:     {final_score:.4f}\")\n",
    "print(f\"Improvement:        +{final_score-baseline_score:.4f} ({(final_score/baseline_score-1)*100:.1f}%)\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "print(f\"  n_estimators:     {best_n_est}\")\n",
    "print(f\"  max_features:     {best_max_feat}\")\n",
    "print(f\"  min_samples_leaf: {best_params['min_samples_leaf']}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: n_estimators tuning\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(n_est_range, scores_n_est, 'o-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=best_n_est, color='red', linestyle='--', label=f'Best: {best_n_est}')\n",
    "ax1.set_xlabel('n_estimators')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Tuning n_estimators', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Plot 2: max_features tuning\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = range(len(max_features_options))\n",
    "ax2.bar(x_pos, scores_max_feat, color='skyblue', edgecolor='black')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([str(m) for m in max_features_options], rotation=45)\n",
    "ax2.axhline(y=max(scores_max_feat), color='red', linestyle='--', alpha=0.7)\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Tuning max_features', fontweight='bold')\n",
    "\n",
    "# Plot 3: Before vs After\n",
    "ax3 = axes[1, 0]\n",
    "models = ['Baseline\\n(Default)', 'Tuned\\n(Optimized)']\n",
    "scores = [baseline_score, final_score]\n",
    "bars = ax3.bar(models, scores, color=['lightcoral', 'lightgreen'], edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Performance Improvement', fontweight='bold')\n",
    "ax3.set_ylim(0.8, 1.0)\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Learning curves with best params\n",
    "ax4 = axes[1, 1]\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "train_scores_curve = []\n",
    "test_scores_curve = []\n",
    "\n",
    "for frac in train_sizes:\n",
    "    n_samples = int(len(X_train) * frac)\n",
    "    X_frac = X_train[:n_samples]\n",
    "    y_frac = y_train[:n_samples]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=best_n_est, max_features=best_max_feat,\n",
    "                                min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                random_state=42)\n",
    "    rf.fit(X_frac, y_frac)\n",
    "    train_scores_curve.append(accuracy_score(y_frac, rf.predict(X_frac)))\n",
    "    test_scores_curve.append(accuracy_score(X_test, y_test))  # Fixed test set\n",
    "\n",
    "ax4.plot(train_sizes * 100, train_scores_curve, 'o-', label='Train', linewidth=2)\n",
    "ax4.plot(train_sizes * 100, test_scores_curve, 's-', label='Test', linewidth=2)\n",
    "ax4.set_xlabel('Training Set Size (%)')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Learning Curves (Tuned Model)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2181448",
   "metadata": {},
   "source": [
    "<a name=\"comparison\"></a>\n",
    "# âš–ï¸ **Chapter 9: Comparison with Other Algorithms**\n",
    "\n",
    "## **Random Forest vs The World**\n",
    "\n",
    "| Aspect | Random Forest | XGBoost/LightGBM | Neural Networks | SVM |\n",
    "|--------|---------------|------------------|-----------------|-----|\n",
    "| **Training Speed** | Fast (parallel) | Medium (sequential) | Slow | Slow |\n",
    "| **Prediction Speed** | Fast | Fast | Fast/Medium | Slow |\n",
    "| **Accuracy** | Very Good | Excellent | Excellent (with data) | Good |\n",
    "| **Tuning Effort** | Low | High | Very High | Medium |\n",
    "| **Interpretability** | Good (feature importance) | Medium | Poor | Poor |\n",
    "| **Handles Missing Data** | Yes | Yes | No | No |\n",
    "| **Data Size** | Medium-Large | Large | Very Large | Small-Medium |\n",
    "| **Feature Types** | Numeric, Categorical | Numeric | Numeric | Numeric |\n",
    "| **Overfitting Risk** | Low | Medium (needs tuning) | High | Medium |\n",
    "\n",
    "## **When to Choose What**\n",
    "\n",
    "```\n",
    "Decision Tree:\n",
    "\n",
    "Need interpretable rules? â”€â”€â–º Single Decision Tree\n",
    "    â”‚\n",
    "    No\n",
    "    â–¼\n",
    "Tabular data size? â”€â”€â–º <10K samples: Try Logistic Regression first\n",
    "    â”‚                  10K-100K: Random Forest (fast baseline)\n",
    "    â”‚                  >100K: Gradient Boosting (XGBoost/LightGBM)\n",
    "    â”‚\n",
    "    Need best accuracy? â”€â”€â–º Gradient Boosting + tuning\n",
    "    â”‚\n",
    "    Need fast results? â”€â”€â–º Random Forest (default params work well)\n",
    "    â”‚\n",
    "    Need uncertainty? â”€â”€â–º Random Forest (probability from vote fraction)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
